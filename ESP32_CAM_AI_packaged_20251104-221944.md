## Prompt for Copilot
```
I want to update this app add some controls to the camera window. CTL-mousewheel zoom, fit window to stream, zoom stream at 100% and pan by mouse movement on right mouse button. Do not alter unrelated code.
```
# Packaged Project (Python)
- **Project root**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI
- **Generated**: 2025-11-04 22:19:44 +0000
- **Tool**: Copilot Python Packager v1.6
## Table of contents
- `cam discovery.py`
- `cam_ai.py`
- `gallery.py`
- `mdi_app.py`
- `refactor/ai_viewer/__init__.py`
- `refactor/ai_viewer/ai/__init__.py`
- `refactor/ai_viewer/ai/detectors/__init__.py`
- `refactor/ai_viewer/ai/detectors/yolo.py`
- `refactor/ai_viewer/ai/recognition/__init__.py`
- `refactor/ai_viewer/ai/recognition/face_db.py`
- `refactor/ai_viewer/ai/recognition/pets_db.py`
- `refactor/ai_viewer/core/__init__.py`
- `refactor/ai_viewer/core/config.py`
- `refactor/ai_viewer/core/stream.py`
- `refactor/ai_viewer/core/tracking.py`
- `refactor/ai_viewer/tools/dupes.py`
- `refactor/ai_viewer/tools/gallery.py`
- `refactor/ai_viewer/ui/__init__.py`
- `refactor/ai_viewer/ui/camera_widget.py`
- `refactor/ai_viewer/ui/windows.py`
- `refactor/run_ai_viewer.py`
- `src/app/main.py`
- `src/camera/detection.py`
- `src/camera/stream.py`
- `src/ui/dialogs.py`
- `src/ui/widgets.py`
- `tools.py`
---
## `cam discovery.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\cam discovery.py
**Size**: 5833 bytes
**Modified**: 2025-11-04 16:49:49 +0000
**SHA256**: b4f55df41e94d63043fbfe1f2abaa549cf6205c4316cf3766a983dd264804c53
``````python#!/usr/bin/env python3
"""
discover_cams.py — Discover ESP32-CAMs on the local LAN.

Strategy
- Determine local /24 automatically via a UDP socket trick (override with --cidr).
- Probe http://IP/ping (your firmware replies "pong").
- Optionally probe :81/stream to confirm MJPEG boundary and auth requirements.
- Concurrency with ThreadPoolExecutor. Safe timeouts.

Usage
  python discover_cams.py                # auto /24 from primary NIC
  python discover_cams.py --cidr 192.168.1.0/24
  python discover_cams.py --check-stream
  python discover_cams.py --workers 256 --timeout 0.6
"""
from __future__ import annotations
import argparse
import ipaddress
import socket
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests


def guess_primary_ipv4() -> str | None:
    """Pick the local IPv4 by opening a UDP socket to a public IP."""
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.settimeout(0.2)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        return None

def default_cidr() -> str:
    return "192.168.1.0/24"

# def default_cidr() -> str:
#     ip = guess_primary_ipv4()
#     if not ip:
#         return "192.168.1.0/24"
#     # assume /24
#     parts = ip.split(".")
#     return f"{parts[0]}.{parts[1]}.{parts[2]}.0/24"


def probe_host(ip: str, timeout: float, check_stream: bool, token: str | None) -> dict | None:
    base = f"http://{ip}"
    session = requests.Session()
    session.headers.update({"User-Agent": "ESP32-CAM-Discovery/1.0"})
    # 1) /ping
    try:
        r = session.get(f"{base}/ping", timeout=timeout)
        text = (r.text or "").strip().lower()
        if r.status_code == 200 and text.startswith("pong"):
            info = {
                "ip": ip,
                "ping": True,
                "auth": False,
                "stream_ok": None,
                "notes": "",
            }
            # try / (may 401 if auth on)
            try:
                r0 = session.get(base + "/", timeout=timeout)
                if r0.status_code == 401:
                    info["auth"] = True
                elif r0.ok:
                    # maybe capture title if present
                    t = r0.text.lower()
                    if "<title" in t:
                        info["notes"] = "web ui reachable"
            except Exception:
                pass

            # 2) :81/stream (optional)
            if check_stream:
                stream_url = f"http://{ip}:81/stream"
                if token:
                    sep = "&" if "?" in stream_url else "?"
                    stream_url = f"{stream_url}{sep}token={token}"
                try:
                    r1 = session.get(stream_url, stream=True, timeout=timeout)
                    if r1.status_code == 401:
                        info["stream_ok"] = False
                        info["auth"] = True
                    elif r1.ok:
                        ctype = r1.headers.get("Content-Type", "")
                        # read a small chunk to confirm boundary, then close
                        try:
                            next(r1.iter_content(chunk_size=512))
                        except Exception:
                            pass
                        info["stream_ok"] = ("multipart/x-mixed-replace" in ctype.lower())
                    else:
                        info["stream_ok"] = False
                except Exception:
                    info["stream_ok"] = False
            return info
    except requests.exceptions.RequestException:
        return None
    return None


def main():
    ap = argparse.ArgumentParser(description="Discover ESP32-CAM devices on LAN")
    ap.add_argument("--cidr", default=default_cidr(), help="CIDR to scan, e.g. 192.168.1.0/24")
    ap.add_argument("--workers", type=int, default=128, help="Max concurrent probes")
    ap.add_argument("--timeout", type=float, default=0.8, help="Per-request timeout seconds")
    ap.add_argument("--check-stream", action="store_true", help="Also probe :81/stream")
    ap.add_argument("--token", default=None, help="Optional Base64 user:pass token for /stream")
    args = ap.parse_args()

    try:
        net = ipaddress.ip_network(args.cidr, strict=False)
    except Exception as e:
        print(f"Invalid CIDR: {e}", file=sys.stderr)
        sys.exit(2)

    hosts = [str(ip) for ip in net.hosts()]
    print(f"Scanning {len(hosts)} hosts in {args.cidr}…")
    t0 = time.time()

    found = []
    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        futs = {
            ex.submit(probe_host, ip, args.timeout, args.check_stream, args.token): ip
            for ip in hosts
        }
        for fut in as_completed(futs):
            res = fut.result()
            if res:
                found.append(res)

    dt = time.time() - t0
    if not found:
        print("No ESP32-CAMs found.")
        print(f"Done in {dt:.1f}s")
        return

    # Output table
    print("\nDiscovered devices:")
    print("{:<15} {:<6} {:<6} {}".format("IP", "PING", "STRM", "NOTES/AUTH"))
    print("-" * 54)
    for d in sorted(found, key=lambda x: x["ip"]):
        ping = "ok" if d["ping"] else "-"
        if d["stream_ok"] is None:
            strm = "-"
        else:
            strm = "ok" if d["stream_ok"] else "fail"
        notes = d["notes"]
        if d["auth"]:
            notes = (notes + " | auth").strip(" |")
        print("{:<15} {:<6} {:<6} {}".format(d["ip"], ping, strm, notes))

    print(f"\nDone in {dt:.1f}s")


if __name__ == "__main__":
    main()
## `cam_ai.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\cam_ai.py
**Size**: 63647 bytes
**Modified**: 2025-08-28 17:48:38 +0100
**SHA256**: 3b08906ea5857b97a1f008619f25c7b56d260d43e04aa2d2b3ca66e6c12db605
``````python#!/usr/bin/env python3
"""
ESP32-CAM AI Viewer

Features
- Connects to ESP32-CAM MJPEG stream (port 81) using Basic Auth or a token query param
- Pygame UI for visualization and keyboard control
- Face detection + LBPH face recognition with on-the-fly enrollment
- Object detection (person, dog, cat) via YOLOv8n ONNX (optional)
- Simple target tracking with PT (pan/tilt) commands sent back to the camera

Folders
- ai/data/faces/<person_name>/*.jpg  # stored face samples
- ai/models/{MobileNetSSD_deploy.prototxt, MobileNetSSD_deploy.caffemodel}

Controls
- q: quit
- e: enroll currently detected face (prompts for name)
- r: retrain face recognizer from saved samples
- t: cycle target tracking mode (off -> person -> face-known -> dog -> cat)

Dependencies
- python -m pip install pygame opencv-contrib-python numpy requests

Usage
- python ai/cam_ai.py --host 192.168.1.50 --user admin --password YOURPASS
  or: python ai/cam_ai.py --host 192.168.1.50 --token BASE64_USER_COLON_PASS
"""

import os
import time
import base64
import threading
import argparse
import json
from typing import Optional, Tuple, List

import numpy as np
import requests
import pygame
import cv2
import math


# ---------- Config ----------
DEFAULT_WIDTH = 640
DEFAULT_HEIGHT = 480


def ensure_dirs():
    os.makedirs(os.path.join('ai', 'data', 'faces'), exist_ok=True)
    os.makedirs(os.path.join('ai', 'data', 'pets', 'dogs'), exist_ok=True)
    os.makedirs(os.path.join('ai', 'data', 'pets', 'cats'), exist_ok=True)
    os.makedirs(os.path.join('ai', 'models'), exist_ok=True)


# (Removed MobileNet-SSD code)


class MJPEGStream:
    def __init__(self, url: str, auth_header: Optional[str] = None):
        self.url = url
        self.auth_header = auth_header
        self.session = requests.Session()
        self.resp = None
        self.lock = threading.Lock()
        self.buf = b""
        self.running = False
        self.frame = None

    def start(self):
        headers = {}
        if self.auth_header:
            headers['Authorization'] = self.auth_header
        self.resp = self.session.get(self.url, headers=headers, stream=True, timeout=10)
        self.resp.raise_for_status()
        self.running = True
        t = threading.Thread(target=self._reader, daemon=True)
        t.start()
        return self

    def _reader(self):
        boundary = b"--frame"
        try:
            for chunk in self.resp.iter_content(chunk_size=8192):
                if not self.running:
                    break
                if not chunk:
                    continue
                self.buf += chunk
                while True:
                    # Look for a Content-Length header in buffer
                    hdr_end = self.buf.find(b"\r\n\r\n")
                    if hdr_end == -1:
                        break
                    headers = self.buf[:hdr_end].decode('latin1', errors='ignore')
                    cl_idx = headers.lower().find('content-length:')
                    if cl_idx == -1:
                        # drop until next boundary
                        bidx = self.buf.find(boundary)
                        if bidx == -1:
                            self.buf = self.buf[hdr_end+4:]
                        else:
                            self.buf = self.buf[bidx:]
                        continue
                    try:
                        cl_line = headers[cl_idx:].split('\r\n', 1)[0]
                        length = int(cl_line.split(':', 1)[1].strip())
                    except Exception:
                        # Unable to parse length; resync
                        bidx = self.buf.find(boundary)
                        if bidx == -1:
                            self.buf = self.buf[hdr_end+4:]
                        else:
                            self.buf = self.buf[bidx:]
                        continue
                    start = hdr_end + 4
                    if len(self.buf) < start + length:
                        # wait for more data
                        break
                    jpg = self.buf[start:start+length]
                    # move past this part + boundary
                    tail = self.buf[start+length:]
                    bmark = b"\r\n--frame\r\n"
                    if tail.startswith(bmark):
                        self.buf = tail[len(bmark):]
                    else:
                        bpos = tail.find(bmark)
                        self.buf = tail[bpos+len(bmark):] if bpos >= 0 else b""
                    # decode
                    img = cv2.imdecode(np.frombuffer(jpg, np.uint8), cv2.IMREAD_COLOR)
                    with self.lock:
                        self.frame = img
        except Exception:
            # Stream ended or network error; mark as stopped gracefully
            pass
        finally:
            self.running = False

    def read(self):
        with self.lock:
            if self.frame is None:
                return None
            return self.frame.copy()

    def stop(self):
        self.running = False
        try:
            if self.resp is not None:
                self.resp.close()
        except Exception:
            pass


class FaceDB:
    def __init__(self, base_dir='ai/data/faces'):
        self.base = base_dir
        self.labels: List[str] = []
        self.model = None
        self.size = (160, 160)
        # Threshold for LBPH: lower distance means better match
        # Typical good matches are often < 80–100 depending on lighting/data
        self.threshold = 95.0
        # Haar cascade for face detection
        self.cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        # ORB fallback (for environments without cv2.face)
        self.orb = cv2.ORB_create(nfeatures=1000)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db_descs = {}

    def load_and_train(self):
        people = []
        samples = []
        labels = []
        label_map = {}
        next_id = 0
        for name in sorted(os.listdir(self.base)):
            p = os.path.join(self.base, name)
            if not os.path.isdir(p):
                continue
            label_map[name] = next_id
            for fname in os.listdir(p):
                if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                    continue
                img = cv2.imread(os.path.join(p, fname), cv2.IMREAD_GRAYSCALE)
                if img is None:
                    continue
                img = cv2.resize(img, self.size)
                samples.append(img)
                labels.append(next_id)
            next_id += 1
        self.labels = [None] * next_id
        for name, i in label_map.items():
            self.labels[i] = name
        if samples and hasattr(cv2, 'face') and hasattr(cv2.face, 'LBPHFaceRecognizer_create'):
            self.model = cv2.face.LBPHFaceRecognizer_create(radius=1, neighbors=8, grid_x=8, grid_y=8)
            self.model.train(samples, np.array(labels))
        else:
            self.model = None
            # Build ORB descriptor DB fallback
            self.db_descs = {}
            for name in sorted(os.listdir(self.base)):
                p = os.path.join(self.base, name)
                if not os.path.isdir(p):
                    continue
                descs = []
                for fname in os.listdir(p):
                    if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                        continue
                    img = cv2.imread(os.path.join(p, fname), cv2.IMREAD_GRAYSCALE)
                    if img is None:
                        continue
                    img = cv2.resize(img, self.size)
                    try:
                        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                        img = clahe.apply(img)
                    except Exception:
                        img = cv2.equalizeHist(img)
                    kp, d = self.orb.detectAndCompute(img, None)
                    if d is not None:
                        descs.append(d)
                if descs:
                    self.db_descs[name] = descs

    def debug_dump(self, report_path='ai/face_dump.txt'):
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        lines = []
        total_imgs = 0
        persons = []
        for name in sorted(os.listdir(self.base)):
            p = os.path.join(self.base, name)
            if not os.path.isdir(p):
                continue
            files = [f for f in os.listdir(p) if f.lower().endswith(('.jpg','.jpeg','.png'))]
            total = len(files)
            total_imgs += total
            preview = ', '.join(files[:5])
            persons.append((name, total))
            lines.append(f"[{name}] samples: {total}\n  preview: {preview}\n")
        lines.insert(0, f"Faces base: {self.base}\nPersons: {len(persons)}  Total images: {total_imgs}\n\n")
        with open(report_path,'w',encoding='utf-8') as f:
            f.write('\n'.join(lines))
        return report_path

    def debug_verify_training(self):
        """Evaluate recognizer on training images. Returns (per_person, overall)."""
        per_person = {}
        total = 0
        correct = 0
        # Evaluate using whichever recognizer is available (LBPH or ORB fallback)
        for name in sorted(os.listdir(self.base)):
            p = os.path.join(self.base, name)
            if not os.path.isdir(p):
                continue
            files = [f for f in os.listdir(p) if f.lower().endswith(('.jpg','.jpeg','.png'))]
            c = 0
            t = 0
            for fn in files:
                img = cv2.imread(os.path.join(p, fn), cv2.IMREAD_GRAYSCALE)
                if img is None:
                    continue
                pred, score = self.recognize(img, img)
                t += 1
                total += 1
                if pred == name:
                    c += 1
                    correct += 1
            per_person[name] = {'total': t, 'correct': c, 'acc': (c / t * 100.0) if t else 0.0}
        overall = {'total': total, 'correct': correct, 'acc': (correct / total * 100.0) if total else 0.0}
        return per_person, overall

    def detect_faces(self, frame_gray):
        # Improve robustness with CLAHE and tuned parameters
        try:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            eq = clahe.apply(frame_gray)
        except Exception:
            eq = cv2.equalizeHist(frame_gray)
        min_size = max(40, int(0.12 * min(frame_gray.shape[:2])))
        faces = self.cascade.detectMultiScale(eq, scaleFactor=1.1, minNeighbors=4, minSize=(min_size, min_size))
        # Fallback: try slightly different params if none found
        if len(faces) == 0:
            faces = self.cascade.detectMultiScale(eq, scaleFactor=1.05, minNeighbors=3, minSize=(min_size, min_size))
        return faces

    def recognize(self, frame_gray, face_roi) -> Tuple[str, float]:
        roi = cv2.resize(face_roi, self.size)
        if self.model is not None:
            try:
                pred_id, conf = self.model.predict(roi)
                name = self.labels[pred_id] if 0 <= pred_id < len(self.labels) else 'unknown'
                if conf <= self.threshold:
                    score = max(0.0, min(1.0, 1.0 - (conf / self.threshold)))
                    return (name, float(score))
                else:
                    return ('unknown', 0.0)
            except Exception:
                return ('unknown', 0.0)
        # ORB fallback
        if not self.db_descs:
            return ('unknown', 0.0)
        try:
            try:
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                roi_p = clahe.apply(roi)
            except Exception:
                roi_p = cv2.equalizeHist(roi)
            kp, desc = self.orb.detectAndCompute(roi_p, None)
            if desc is None:
                return ('unknown', 0.0)
            best_name = 'unknown'
            best_score = 0.0
            for name, descs in self.db_descs.items():
                matches_total = 0
                good = 0
                for d in descs:
                    matches = self.bf.match(desc, d)
                    matches_total += len(matches)
                    good += sum(1 for m in matches if m.distance < 40)
                if matches_total == 0:
                    continue
                score = good / float(matches_total)
                if score > best_score:
                    best_score = score
                    best_name = name
            return (best_name if best_score > 0.12 else 'unknown', float(best_score))
        except Exception:
            return ('unknown', 0.0)

    def enroll(self, frame_gray, x, y, w, h, name: str):
        name = ''.join(c for c in name.strip() if c.isalnum() or c in ('_', '-'))
        if not name:
            return False
        p = os.path.join(self.base, name)
        os.makedirs(p, exist_ok=True)
        roi = frame_gray[y:y+h, x:x+w]
        roi = cv2.resize(roi, self.size)
        ts = int(time.time() * 1000)
        # Save original and a mirrored version to help robustness
        cv2.imwrite(os.path.join(p, f'{ts}.jpg'), roi)
        try:
            cv2.imwrite(os.path.join(p, f'{ts}_flip.jpg'), cv2.flip(roi, 1))
        except Exception:
            pass
        return True


# (Removed SSDDetector)


class YOLODetector:
    """YOLOv8n ONNX detector using OpenCV DNN. Returns (cls, conf, x, y, w, h)."""
    COCO_CLASSES = [
        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
        'hair drier', 'toothbrush'
    ]

    def __init__(self, model_path='ai/models/yolov8n.onnx', input_size=640, conf_thresh=0.25, iou_thresh=0.45):
        self.model_path = model_path
        self.net = None
        self.input_size = input_size
        self.conf_thresh = conf_thresh
        self.iou_thresh = iou_thresh
        if os.path.exists(model_path):
            try:
                self.net = cv2.dnn.readNetFromONNX(model_path)
                try:
                    self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
                    self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
                except Exception:
                    pass
            except Exception:
                self.net = None

    def available(self):
        return self.net is not None

    def _letterbox(self, image, new_shape=640, color=(114, 114, 114)):
        h, w = image.shape[:2]
        if isinstance(new_shape, int):
            new_shape = (new_shape, new_shape)
        r = min(new_shape[0] / h, new_shape[1] / w)
        nh, nw = int(round(h * r)), int(round(w * r))
        resized = cv2.resize(image, (nw, nh), interpolation=cv2.INTER_LINEAR)
        canvas = np.full((new_shape[0], new_shape[1], 3), color, dtype=np.uint8)
        top = (new_shape[0] - nh) // 2
        left = (new_shape[1] - nw) // 2
        canvas[top:top + nh, left:left + nw] = resized
        return canvas, r, (left, top)

    def detect(self, frame_bgr):
        if self.net is None:
            return []
        img, r, (dx, dy) = self._letterbox(frame_bgr, self.input_size)
        blob = cv2.dnn.blobFromImage(img, 1/255.0, (self.input_size, self.input_size), swapRB=True, crop=False)
        self.net.setInput(blob)
        out = self.net.forward()
        # Support outputs in shapes (1, 84, N) or (1, N, 84)
        out = np.squeeze(out)
        if out.ndim == 2:
            if out.shape[0] in (84, 85):
                out = out.T
            # now (N, 84)
        elif out.ndim == 3:
            # pick the first batch and try to orient to (N, 84)
            o = out[0]
            if o.shape[0] in (84, 85):
                out = o.T
            else:
                out = o
        else:
            return []

        boxes = []
        scores = []
        classes = []
        H, W = frame_bgr.shape[:2]
        for det in out:
            cx, cy, w, h = det[:4]
            # YOLOv5 has objectness at det[4] and class scores at det[5:]
            # YOLOv8 commonly has class scores directly at det[4:]
            if det.shape[0] >= 85:
                obj = float(det[4])
                class_scores = det[5:]
                c = int(np.argmax(class_scores))
                conf = obj * float(class_scores[c])
            else:
                class_scores = det[4:]
                c = int(np.argmax(class_scores))
                conf = float(class_scores[c])
            if conf < self.conf_thresh:
                continue
            # map box back to original image coordinates
            x1 = (cx - w/2 - dx) / r
            y1 = (cy - h/2 - dy) / r
            x2 = (cx + w/2 - dx) / r
            y2 = (cy + h/2 - dy) / r
            x1 = max(0, min(W-1, x1))
            y1 = max(0, min(H-1, y1))
            x2 = max(0, min(W-1, x2))
            y2 = max(0, min(H-1, y2))
            boxes.append([int(x1), int(y1), int(x2 - x1), int(y2 - y1)])
            scores.append(conf)
            classes.append(c)

        if not boxes:
            return []
        idxs = cv2.dnn.NMSBoxes(boxes, scores, self.conf_thresh, self.iou_thresh)
        if isinstance(idxs, tuple) or isinstance(idxs, list):
            idxs = np.array(idxs).reshape(-1)
        else:
            idxs = np.array(idxs).reshape(-1)
        out_dets = []
        for i in idxs:
            cls_name = self.COCO_CLASSES[classes[i]] if 0 <= classes[i] < len(self.COCO_CLASSES) else str(classes[i])
            x, y, w, h = boxes[i]
            out_dets.append((cls_name, float(scores[i]), x, y, w, h))
        return out_dets


def download_yolo_model(dst_path='ai/models/yolo.onnx'):
    os.makedirs(os.path.dirname(dst_path), exist_ok=True)
    if os.path.exists(dst_path) and os.path.getsize(dst_path) > 0:
        print(f"[YOLO] Exists: {dst_path}")
        return True
    mirrors = [
        # Ultralytics YOLOv5 (COCO 80 classes) – reliable release asset
        'https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.onnx',
        # Other official v5 releases to try as fallback
        'https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n.onnx',
        # YOLOv8 tiny/nano variants (may work if available)
        'https://github.com/ultralytics/ultralytics/releases/download/v8.0.0/yolov8n.onnx',
        'https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.onnx',
    ]
    headers = {'User-Agent': 'ESP32-CAM-AI-Downloader/1.0'}
    last_err = None
    for url in mirrors:
        try:
            print(f"[YOLO] Downloading from {url} ...")
            with requests.get(url, headers=headers, stream=True, timeout=120, allow_redirects=True) as r:
                r.raise_for_status()
                total = int(r.headers.get('content-length', '0'))
                done = 0
                with open(dst_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=1<<15):
                        if not chunk:
                            continue
                        f.write(chunk)
                        done += len(chunk)
                        if total:
                            print(f"  {done*100//total}%\r", end='')
            print(f"[YOLO] Saved to {dst_path}")
            return True
        except Exception as e:
            last_err = e
            continue
    print(f"[YOLO] Failed to download: {last_err}")
    return False


class PetsDB:
    """Simple pet enrollment and recognition using ORB feature matching.
    Directory layout:
      ai/data/pets/dogs/<name>/*.jpg
      ai/data/pets/cats/<name>/*.jpg
    """
    def __init__(self, base_dir='ai/data/pets'):
        self.base = base_dir
        self.orb = cv2.ORB_create(nfeatures=1000)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db = { 'dogs': {}, 'cats': {} }  # species -> name -> list of desc

    def load(self):
        self.db = { 'dogs': {}, 'cats': {} }
        for species in ('dogs', 'cats'):
            root = os.path.join(self.base, species)
            if not os.path.isdir(root):
                continue
            for name in os.listdir(root):
                p = os.path.join(root, name)
                if not os.path.isdir(p):
                    continue
                descs = []
                for fn in os.listdir(p):
                    if not fn.lower().endswith(('.jpg', '.jpeg', '.png')):
                        continue
                    img = cv2.imread(os.path.join(p, fn), cv2.IMREAD_GRAYSCALE)
                    if img is None:
                        continue
                    img = cv2.resize(img, (320, 320))
                    kp, desc = self.orb.detectAndCompute(img, None)
                    if desc is not None:
                        descs.append(desc)
                if descs:
                    self.db[species][name] = descs

    def enroll(self, roi_bgr, name: str, species: str) -> bool:
        species = 'dogs' if species.lower().startswith('dog') else 'cats'
        name = ''.join(c for c in name.strip() if c.isalnum() or c in ('_', '-'))
        if not name:
            return False
        p = os.path.join(self.base, species, name)
        os.makedirs(p, exist_ok=True)
        img = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)
        img = cv2.resize(img, (320, 320))
        ts = int(time.time() * 1000)
        cv2.imwrite(os.path.join(p, f'{ts}.jpg'), img)
        return True

    def recognize(self, roi_bgr, species: str) -> Tuple[str, float]:
        """Return (name, score[0..1]) or ('unknown', 0)"""
        species = 'dogs' if species == 'dog' else 'cats'
        if not self.db[species]:
            return ('unknown', 0.0)
        img = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)
        img = cv2.resize(img, (320, 320))
        kp, desc = self.orb.detectAndCompute(img, None)
        if desc is None:
            return ('unknown', 0.0)
        best_name = 'unknown'
        best_score = 0.0
        for name, descs in self.db[species].items():
            matches_total = 0
            good = 0
            for d in descs:
                matches = self.bf.match(desc, d)
                matches_total += len(matches)
                # Count good matches (distance threshold)
                good += sum(1 for m in matches if m.distance < 40)
            if matches_total == 0:
                continue
            score = good / float(matches_total)
            if score > best_score:
                best_score = score
                best_name = name
        return (best_name if best_score > 0.12 else 'unknown', float(best_score))


# ---------- Simple Pygame Config UI ----------
class InputField:
    def __init__(self, rect, text='', masked=False):
        self.rect = pygame.Rect(rect)
        self.text = text
        self.focus = False
        self.masked = masked

    def handle(self, event):
        if event.type == pygame.MOUSEBUTTONDOWN:
            self.focus = self.rect.collidepoint(event.pos)
        elif event.type == pygame.KEYDOWN and self.focus:
            if event.key == pygame.K_BACKSPACE:
                self.text = self.text[:-1]
            elif event.key == pygame.K_TAB:
                # lose focus; external manager will change focus
                self.focus = False
            elif event.key == pygame.K_RETURN:
                self.focus = False
            else:
                ch = event.unicode
                if ch and 32 <= ord(ch) < 127:
                    self.text += ch

    def draw(self, surf, font):
        bg = (220, 220, 220)
        bd = (60, 120, 200) if self.focus else (120, 120, 120)
        pygame.draw.rect(surf, bg, self.rect, border_radius=6)
        pygame.draw.rect(surf, bd, self.rect, 2, border_radius=6)
        disp = self.text if not self.masked else ('*' * len(self.text))
        label = font.render(disp, True, (20, 20, 20))
        ty = self.rect.y + (self.rect.h - label.get_height()) // 2
        tx = self.rect.x + 8
        surf.blit(label, (tx, ty))
        # caret (blinking)
        if self.focus:
            try:
                w = font.size(disp)[0]
            except Exception:
                w = label.get_width()
            blink = int(time.time() * 2) % 2 == 0
            if blink:
                cx = tx + w + 1
                cy1 = self.rect.y + 6
                cy2 = self.rect.y + self.rect.height - 6
                pygame.draw.line(surf, (30, 30, 30), (cx, cy1), (cx, cy2), 2)


def config_ui(defaults):
    pygame.init()
    W, H = 700, 420
    screen = pygame.display.set_mode((W, H))
    pygame.display.set_caption('ESP32-CAM AI Config')
    font = pygame.font.SysFont('segoeui', 18)
    small = pygame.font.SysFont('consolas', 14)

    host = InputField((140, 40, 380, 36), defaults.get('host', ''))
    user = InputField((140, 90, 180, 36), defaults.get('user', ''))
    password = InputField((340, 90, 180, 36), defaults.get('password', ''), masked=True)
    token = InputField((140, 140, 380, 36), defaults.get('token', ''))
    width = InputField((140, 190, 100, 36), str(defaults.get('width', DEFAULT_WIDTH)))
    height = InputField((260, 190, 100, 36), str(defaults.get('height', DEFAULT_HEIGHT)))

    buttons = {
        'start': pygame.Rect(140, 260, 120, 40),
        'download_yolo': pygame.Rect(270, 260, 180, 40),
        'quit': pygame.Rect(460, 260, 120, 40),
    }

    running = True
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return None
            for f in (host, user, password, token, width, height):
                f.handle(event)
            if event.type == pygame.MOUSEBUTTONDOWN:
                if buttons['start'].collidepoint(event.pos):
                    try:
                        w = int(width.text or DEFAULT_WIDTH)
                        h = int(height.text or DEFAULT_HEIGHT)
                    except ValueError:
                        w, h = DEFAULT_WIDTH, DEFAULT_HEIGHT
                    cfg = {
                        'host': host.text.strip(),
                        'user': user.text.strip(),
                        'password': password.text,
                        'token': token.text.strip(),
                        'width': w,
                        'height': h,
                    }
                    if not cfg['host']:
                        continue
                    return cfg
                if buttons['download_yolo'].collidepoint(event.pos):
                    # Download YOLOv8n ONNX (blocking)
                    try:
                        ok = download_yolo_model('ai/models/yolo.onnx')
                        print('YOLO download:', 'OK' if ok else 'FAILED')
                    except Exception as e:
                        print('Download YOLO error:', e)
                if buttons['quit'].collidepoint(event.pos):
                    return None

        screen.fill((240, 245, 255))
        # Labels
        screen.blit(font.render('ESP32-CAM AI Configuration', True, (20, 40, 90)), (20, 10))
        screen.blit(font.render('Host (ip[:port])', True, (0, 0, 0)), (20, 45))
        screen.blit(font.render('User / Password (Basic)', True, (0, 0, 0)), (20, 95))
        screen.blit(font.render('Token (Base64 user:pass)', True, (0, 0, 0)), (20, 145))
        screen.blit(font.render('Width / Height', True, (0, 0, 0)), (20, 195))
        screen.blit(small.render('Note: If token is provided, user/password are ignored.', True, (60, 60, 60)), (20, 230))

        # Inputs
        for f in (host, user, password, token, width, height):
            f.draw(screen, font)

        # Buttons
        for key, rect in buttons.items():
            pygame.draw.rect(screen, (60, 120, 200), rect, border_radius=8)
            label = 'Start' if key == 'start' else ('Download YOLO' if key == 'download_yolo' else 'Quit')
            txt = font.render(label, True, (255, 255, 255))
            screen.blit(txt, (rect.x + (rect.w - txt.get_width()) // 2, rect.y + (rect.h - txt.get_height()) // 2))

        pygame.display.flip()


# ---------- Overlay UI in viewer ----------
def draw_button(surf, rect, text, font, bg=(60,120,200), fg=(255,255,255)):
    pygame.draw.rect(surf, bg, rect, border_radius=8)
    label = font.render(text, True, fg)
    surf.blit(label, (rect.x + (rect.w - label.get_width()) // 2, rect.y + (rect.h - label.get_height()) // 2))


def modal_text_input(screen, title, fields, confirm_label='Save'):
    """Simple blocking modal for text input.
    fields: list of dicts [{'label': 'Name', 'value': '', 'masked': False}]
    Returns list of strings or None if cancelled.
    """
    W, H = screen.get_size()
    overlay = pygame.Surface((W, H), pygame.SRCALPHA)
    overlay.fill((0, 0, 0, 160))
    box = pygame.Rect(W//2-260, H//2-160, 520, 240)
    font = pygame.font.SysFont('segoeui', 20)
    small = pygame.font.SysFont('segoeui', 16)

    inputs = []
    y = box.y + 60
    for f in fields:
        inp = InputField((box.x + 140, y, 320, 34), f.get('value',''), masked=f.get('masked', False))
        inputs.append((f['label'], inp))
        y += 46
    btn_ok = pygame.Rect(box.x + 220, box.y + box.h - 50, 100, 34)
    btn_cancel = pygame.Rect(box.x + 340, box.y + box.h - 50, 100, 34)

    # focus first
    if inputs:
        inputs[0][1].focus = True

    while True:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return None
            for _, inp in inputs:
                inp.handle(event)
            if event.type == pygame.MOUSEBUTTONDOWN:
                if btn_ok.collidepoint(event.pos):
                    return [inp.text for _, inp in inputs]
                if btn_cancel.collidepoint(event.pos):
                    return None

        screen.blit(overlay, (0, 0))
        pygame.draw.rect(screen, (245, 248, 255), box, border_radius=10)
        pygame.draw.rect(screen, (100, 120, 170), box, 2, border_radius=10)
        screen.blit(font.render(title, True, (20, 40, 90)), (box.x + 16, box.y + 16))
        yy = box.y + 60
        for label, inp in inputs:
            screen.blit(small.render(label, True, (0,0,0)), (box.x + 16, yy+6))
            inp.draw(screen, small)
            yy += 46
        draw_button(screen, btn_ok, confirm_label, small)
        draw_button(screen, btn_cancel, 'Cancel', small, bg=(160,160,160))
        pygame.display.flip()


def gallery_delete(screen, dir_path, title='Manage Images'):
    """Overlay gallery to select and delete specific images in a directory.
    - Left click toggles selection
    - Buttons: Prev, Next, Delete Selected, Close
    """
    if not os.path.isdir(dir_path):
        return False
    files = [os.path.join(dir_path, f) for f in os.listdir(dir_path)
             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    if not files:
        return False

    W, H = screen.get_size()
    overlay = pygame.Surface((W, H), pygame.SRCALPHA)
    overlay.fill((0, 0, 0, 160))
    box = pygame.Rect(40, 40, W - 80, H - 80)
    title_font = pygame.font.SysFont('segoeui', 20)
    small = pygame.font.SysFont('segoeui', 14)

    # Grid layout
    pad = 10
    thumb_w = 120
    thumb_h = 90
    cols = max(1, (box.w - 2*pad) // (thumb_w + pad))
    rows = max(1, (box.h - 100 - 2*pad) // (thumb_h + pad))  # leave space for buttons
    per_page = cols * rows
    page = 0
    selected = set()

    # Cache of thumbnails
    thumb_cache = {
        'paths': [],
        'surfs': [],
    }

    def load_page(p):
        start = p * per_page
        end = min(len(files), start + per_page)
        thumb_cache['paths'] = files[start:end]
        thumb_cache['surfs'] = []
        for fp in thumb_cache['paths']:
            try:
                img = pygame.image.load(fp)
                img = pygame.transform.smoothscale(img, (thumb_w, thumb_h))
                thumb_cache['surfs'].append(img.convert())
            except Exception:
                # placeholder
                surf = pygame.Surface((thumb_w, thumb_h))
                surf.fill((60,60,60))
                thumb_cache['surfs'].append(surf)

    load_page(page)

    # Buttons
    btn_h = 36
    btn_w = 140
    def buttons_rects():
        y = box.bottom - btn_h - 12
        x = box.left + 12
        bprev = pygame.Rect(x, y, 80, btn_h); x += 90
        bnext = pygame.Rect(x, y, 80, btn_h); x += 90
        bdel  = pygame.Rect(x, y, btn_w, btn_h); x += btn_w + 10
        bclose= pygame.Rect(x, y, 100, btn_h)
        return bprev, bnext, bdel, bclose

    def draw():
        screen.blit(overlay, (0, 0))
        pygame.draw.rect(screen, (245,248,255), box, border_radius=10)
        pygame.draw.rect(screen, (100,120,170), box, 2, border_radius=10)
        screen.blit(title_font.render(title, True, (20,40,90)), (box.x + 16, box.y + 12))
        # grid
        gx = box.x + pad
        gy = box.y + pad + 32
        idx = 0
        for r in range(rows):
            x = gx
            for c in range(cols):
                gidx = page*per_page + idx
                if gidx >= len(files):
                    break
                surf = thumb_cache['surfs'][idx]
                rect = pygame.Rect(x, gy, thumb_w, thumb_h)
                screen.blit(surf, rect.topleft)
                fp = thumb_cache['paths'][idx]
                if fp in selected:
                    pygame.draw.rect(screen, (220,60,60), rect, 3)
                else:
                    pygame.draw.rect(screen, (180,180,180), rect, 1)
                # filename
                name = os.path.basename(fp)
                label = small.render(name[:16], True, (30,30,30))
                screen.blit(label, (x, rect.bottom + 2))
                x += thumb_w + pad
                idx += 1
            gy += thumb_h + pad + 20
        # buttons
        bprev, bnext, bdel, bclose = buttons_rects()
        draw_button(screen, bprev, 'Prev', small)
        draw_button(screen, bnext, 'Next', small)
        draw_button(screen, bdel, 'Delete Selected', small, bg=(200,60,60))
        draw_button(screen, bclose, 'Close', small, bg=(160,160,160))
        # page info
        info = small.render(f"Page {page+1}/{max(1, math.ceil(len(files)/per_page))}  Selected: {len(selected)}", True, (30,30,30))
        screen.blit(info, (box.x + 16, box.bottom - btn_h - 48))
        pygame.display.flip()

    while True:
        draw()
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return False
            if event.type == pygame.MOUSEBUTTONDOWN:
                pos = event.pos
                # click grid
                gx = box.x + pad
                gy = box.y + pad + 32
                idx = 0
                clicked = False
                for r in range(rows):
                    x = gx
                    for c in range(cols):
                        gidx = page*per_page + idx
                        if gidx >= len(files):
                            break
                        rect = pygame.Rect(x, gy, thumb_w, thumb_h)
                        if rect.collidepoint(pos):
                            fp = thumb_cache['paths'][idx]
                            if fp in selected:
                                selected.remove(fp)
                            else:
                                selected.add(fp)
                            clicked = True
                            break
                        x += thumb_w + pad
                        idx += 1
                    if clicked:
                        break
                    gy += thumb_h + pad + 20
                if clicked:
                    continue
                # buttons
                bprev, bnext, bdel, bclose = buttons_rects()
                if bprev.collidepoint(pos):
                    if page > 0:
                        page -= 1
                        load_page(page)
                elif bnext.collidepoint(pos):
                    if (page+1)*per_page < len(files):
                        page += 1
                        load_page(page)
                elif bdel.collidepoint(pos):
                    # delete selected
                    for fp in list(selected):
                        try:
                            os.remove(fp)
                        except Exception:
                            pass
                    # refresh files
                    files[:] = [p for p in files if os.path.exists(p)]
                    selected.clear()
                    # reload current page (clamp page)
                    if page*per_page >= len(files) and page > 0:
                        page -= 1
                    load_page(page)
                elif bclose.collidepoint(pos):
                    return True


def build_auth_header(user: Optional[str], password: Optional[str], token: Optional[str]) -> Tuple[Optional[str], str]:
    """Return (Authorization header value or None, stream_url_suffix)"""
    if token:
        return None, ("?token=" + token)
    if user and password:
        up = f"{user}:{password}".encode('utf-8')
        return "Basic " + base64.b64encode(up).decode('ascii'), ""
    return None, ""


def send_ptz(base_http: str, auth_header: Optional[str], token_suffix: str, direction: str):
    try:
        url = f"{base_http}/action?go={direction}{('&token='+token_suffix.split('=')[1]) if token_suffix else ''}"
        headers = {'Authorization': auth_header} if auth_header else {}
        requests.get(url, headers=headers, timeout=2)
    except Exception:
        pass


def main():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('--host', help='ESP32-CAM host or host:port (port 80)')
    parser.add_argument('--user', help='Basic auth username')
    parser.add_argument('--password', help='Basic auth password')
    parser.add_argument('--token', help='Token (Base64 of user:pass) for query param auth')
    parser.add_argument('--width', type=int, default=DEFAULT_WIDTH)
    parser.add_argument('--height', type=int, default=DEFAULT_HEIGHT)
    # (removed --download-ssd)
    cli_args, _ = parser.parse_known_args()

    ensure_dirs()

    # Load saved config if present
    cfg_path = os.path.join('ai', 'config.json')
    saved = {
        'host': cli_args.host or '',
        'user': cli_args.user or '',
        'password': cli_args.password or '',
        'token': cli_args.token or '',
        'width': cli_args.width,
        'height': cli_args.height,
    }
    try:
        if os.path.exists(cfg_path):
            with open(cfg_path, 'r', encoding='utf-8') as f:
                disk = json.load(f)
            saved.update({k: disk.get(k, saved[k]) for k in saved})
    except Exception:
        pass

    # (no CLI download actions)

    # If missing host (or user input desired), show UI config screen
    if not saved['host']:
        saved = config_ui(defaults=saved)
        if saved is None:
            return
    # Persist config
    try:
        with open(cfg_path, 'w', encoding='utf-8') as f:
            json.dump(saved, f, indent=2)
    except Exception:
        pass

    host = saved['host']
    width = int(saved.get('width') or DEFAULT_WIDTH)
    height = int(saved.get('height') or DEFAULT_HEIGHT)
    user = saved.get('user') or None
    password = saved.get('password') or None
    token = saved.get('token') or None

    base_http = f"http://{host}"
    auth_header, token_suffix = build_auth_header(user, password, token)
    stream_url = f"http://{host.split(':')[0]}:81/stream{token_suffix}"

    # Start stream
    stream = MJPEGStream(stream_url, auth_header=auth_header).start()

    # Init pygame
    pygame.init()
    screen = pygame.display.set_mode((width, height))
    pygame.display.set_caption('ESP32-CAM AI Viewer')
    font = pygame.font.SysFont('consolas', 16)
    ui_font = pygame.font.SysFont('segoeui', 16)

    # Init detectors
    facedb = FaceDB()
    facedb.load_and_train()
    # Ensure YOLO model exists (attempt auto-download once if missing)
    yolo_model_path = os.path.join('ai','models','yolo.onnx')
    detector_yolo = YOLODetector(yolo_model_path)
    det_mode = 'yolo' if detector_yolo.available() else 'off'
    if det_mode == 'off':
        try:
            print('[YOLO] Model missing, attempting auto-download...')
            if download_yolo_model(yolo_model_path):
                detector_yolo = YOLODetector(yolo_model_path)
                det_mode = 'yolo' if detector_yolo.available() else 'off'
                print('[YOLO] Auto-download complete:', 'OK' if det_mode=='yolo' else 'FAILED')
        except Exception as e:
            print('[YOLO] Auto-download error:', e)
    pets = PetsDB()
    pets.load()

    # Tracking
    tracker = None
    track_bbox = None  # x,y,w,h
    track_mode = 0  # 0=off, 1=person, 2=face-known, 3=dog, 4=cat
    last_ptz = 0.0

    def start_tracker(frame, bbox):
        nonlocal tracker, track_bbox
        try:
            if hasattr(cv2, 'legacy') and hasattr(cv2.legacy, 'TrackerCSRT_create'):
                tracker = cv2.legacy.TrackerCSRT_create()
            elif hasattr(cv2, 'TrackerCSRT_create'):
                tracker = cv2.TrackerCSRT_create()
            elif hasattr(cv2, 'TrackerKCF_create'):
                tracker = cv2.TrackerKCF_create()
            else:
                tracker = None
        except Exception:
            tracker = None
        if tracker is not None:
            tracker.init(frame, tuple(bbox))
            track_bbox = bbox

    running = True
    enroll_name_input = ''
    enroll_bbox = None
    last_dog_bbox = None
    last_cat_bbox = None
    # Face sample collection state
    collect = None  # {'name': str, 'n': int, 'collected': int, 'interval': float, 'last': float}
    # Pet sample collection state
    pet_collect = None  # {'name': str, 'species': 'dog'|'cat', 'n': int, 'collected': int, 'interval': float, 'last': float}

    # UI visibility toggle (toolbar)
    show_ui = True

    # Overlay UI helpers
    def layout_buttons():
        W, H = screen.get_size()
        pad = 8
        btn_w = 130
        btn_h = 32
        y = H - btn_h - pad
        x = pad
        buttons = {}
        def add(name, label):
            nonlocal x, y
            # wrap to new row if exceeding width
            if x + btn_w > W - pad:
                x = pad
                y -= (btn_h + pad)
            rect = pygame.Rect(x, y, btn_w, btn_h)
            buttons[name] = (rect, label)
            x += btn_w + pad
        add('ui', 'UI')
        if show_ui:
            add('settings', 'Settings')
            add('mode', f"Mode:{track_mode}")
            add('detector', f"Det:{det_mode.upper()}")
            add('enroll_face', 'Enroll Face')
            add('collect', 'Collect Samples')
        add('enroll_pet', 'Enroll Pet')
        add('collect_pet', 'Collect Pet')
        add('manage_faces', 'Manage Faces')
        add('manage_pets', 'Manage Pets')
        add('retrain', 'Retrain Faces')
        add('download_yolo', 'Download YOLO')
        add('debug', 'Debug Dump')
        add('quit', 'Quit')
        return buttons

    toast_msg = ''
    toast_until = 0
    def toast(msg, dur=2.0):
        nonlocal toast_msg, toast_until
        toast_msg = msg
        toast_until = time.time() + dur

    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_q:
                    running = False
                elif event.key == pygame.K_t:
                    track_mode = (track_mode + 1) % 5
                elif event.key == pygame.K_r:
                    facedb.load_and_train()
                elif event.key == pygame.K_e:
                    # Trigger enroll on the last detected face bbox
                    if enroll_bbox is not None and enroll_name_input:
                        # Will be handled after frame is processed
                        pass
                elif event.key == pygame.K_p:
                    pass
            elif event.type == pygame.MOUSEBUTTONDOWN:
                buttons = layout_buttons()
                for name, (rect, label) in buttons.items():
                    if rect.collidepoint(event.pos):
                        if name == 'quit':
                            running = False
                        elif name == 'ui':
                            show_ui = not show_ui
                            toast('UI: %s' % ('shown' if show_ui else 'hidden'))
                        elif name == 'retrain':
                            facedb.load_and_train()
                            toast('Faces retrained')
                        elif name == 'download_yolo':
                            if download_yolo_model(os.path.join('ai','models','yolo.onnx')):
                                detector_yolo.__init__(os.path.join('ai','models','yolo.onnx'))
                                det_mode = 'yolo'
                                toast('YOLO downloaded')
                            else:
                                toast('YOLO download failed')
                        elif name == 'mode':
                            track_mode = (track_mode + 1) % 5
                        elif name == 'detector':
                            det_mode = 'off' if det_mode == 'yolo' else 'yolo'
                        elif name == 'debug':
                            # Rebuild model, dump enrollment, and verify
                            facedb.load_and_train()
                            report = facedb.debug_dump('ai/face_dump.txt')
                            per, overall = facedb.debug_verify_training()
                            print('[DEBUG] Enrollment report saved to:', report)
                            print('[DEBUG] Overall training accuracy: %.2f%% (%d/%d)' % (overall['acc'], overall['correct'], overall['total']))
                            for k,v in per.items():
                                print(' - %-16s: %5.1f%% (%d/%d)' % (k, v['acc'], v['correct'], v['total']))
                            toast('Debug dump saved (console)')
                        elif name == 'settings':
                            cfg = config_ui({'host': host, 'user': user or '', 'password': password or '', 'token': token or '', 'width': width, 'height': height})
                            if cfg:
                                try:
                                    with open(os.path.join('ai','config.json'),'w',encoding='utf-8') as f:
                                        json.dump(cfg, f, indent=2)
                                except Exception:
                                    pass
                                host = cfg['host']; user = cfg.get('user') or None; password = cfg.get('password') or None; token = cfg.get('token') or None
                                width = int(cfg.get('width') or width); height = int(cfg.get('height') or height)
                                base_http = f"http://{host}"
                                auth_header, token_suffix = build_auth_header(user, password, token)
                                stream.stop()
                                stream = MJPEGStream(f"http://{host.split(':')[0]}:81/stream{token_suffix}", auth_header=auth_header).start()
                                screen = pygame.display.set_mode((width, height))
                                toast('Settings applied')
                        elif name == 'enroll_face':
                            if enroll_bbox is None:
                                toast('No face detected')
                            else:
                                ret = modal_text_input(screen, 'Enroll Face', [{'label':'Name','value':'','masked':False}], confirm_label='Enroll')
                                if ret:
                                    (x, y, w, h) = enroll_bbox
                                    gray_now = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                                    if facedb.enroll(gray_now, x, y, w, h, ret[0]):
                                        facedb.load_and_train()
                                        toast(f"Enrolled {ret[0]}")
                        elif name == 'collect':
                            if enroll_bbox is None:
                                toast('No face detected')
                            else:
                                # If already collecting, cancel
                                if collect is not None:
                                    collect = None
                                    toast('Collection cancelled')
                                else:
                                    ret = modal_text_input(screen, 'Collect N Samples', [
                                        {'label':'Name','value':'','masked':False},
                                        {'label':'Samples (e.g., 20)','value':'20','masked':False}
                                    ], confirm_label='Start')
                                    if ret:
                                        try:
                                            n = max(1, int(ret[1]))
                                        except Exception:
                                            n = 20
                                        collect = {'name': ret[0].strip() or 'person', 'n': n, 'collected': 0, 'interval': 0.2, 'last': 0.0}
                                        toast(f"Collecting {n} samples for {collect['name']}")
                        elif name == 'enroll_pet':
                            bb = last_dog_bbox or last_cat_bbox
                            if bb is None:
                                toast('No pet detected')
                            else:
                                ret = modal_text_input(screen, 'Enroll Pet', [
                                    {'label':'Name','value':'','masked':False},
                                    {'label':'Species (dog/cat)','value':'dog','masked':False}
                                ], confirm_label='Enroll')
                                if ret:
                                    (pname, pspecies) = ret
                                    (x, y, w, h) = bb
                                    roi = frame[max(0,y):y+h, max(0,x):x+w]
                                    if pets.enroll(roi, pname, pspecies):
                                        pets.load()
                                        toast(f"Enrolled {pspecies}:{pname}")
                        elif name == 'collect_pet':
                            # Toggle pet collection
                            if pet_collect is not None:
                                pet_collect = None
                                toast('Pet collection cancelled')
                            else:
                                ret = modal_text_input(screen, 'Collect Pet Samples', [
                                    {'label':'Name','value':'','masked':False},
                                    {'label':'Species (dog/cat)','value':'dog','masked':False},
                                    {'label':'Samples (e.g., 40)','value':'40','masked':False}
                                ], confirm_label='Start')
                                if ret:
                                    try:
                                        n = max(1, int(ret[2]))
                                    except Exception:
                                        n = 40
                                    sp = 'dog' if ret[1].lower().startswith('dog') else 'cat'
                                    pet_collect = {'name': ret[0].strip() or 'pet', 'species': sp, 'n': n, 'collected': 0, 'interval': 0.25, 'last': 0.0}
                                    toast(f"Collecting {n} {sp} samples for {pet_collect['name']}")
                        elif name == 'manage_faces':
                            ret = modal_text_input(screen, 'Manage Faces', [
                                {'label':'Name','value':'','masked':False}
                            ], confirm_label='Open')
                            if ret:
                                fname = ret[0].strip()
                                target = os.path.join('ai','data','faces', fname)
                                if gallery_delete(screen, target, title=f'Faces: {fname}'):
                                    facedb.load_and_train()
                        elif name == 'manage_pets':
                            ret = modal_text_input(screen, 'Manage Pets', [
                                {'label':'Species (dog/cat)','value':'dog','masked':False},
                                {'label':'Name','value':'','masked':False}
                            ], confirm_label='Open')
                            if ret:
                                sp = 'dogs' if ret[0].lower().startswith('dog') else 'cats'
                                pname = ret[1].strip()
                                base = os.path.join('ai','data','pets', sp, pname)
                                if gallery_delete(screen, base, title=f'{sp[:-1].title()}: {pname}'):
                                    pets.load()

        frame = stream.read()
        if frame is None:
            time.sleep(0.01)
            continue
        frame = cv2.resize(frame, (width, height))
        out = frame.copy()

        # Object detection (if model available)
        if det_mode == 'yolo' and detector_yolo.available():
            dets = detector_yolo.detect(frame)
        else:
            dets = []

        # Face detection/recognition
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = facedb.detect_faces(gray)
        enroll_bbox = None

        known_face_bboxes = []
        for (x, y, w, h) in faces:
            roi = cv2.resize(gray[y:y+h, x:x+w], (160, 160))
            name, score = facedb.recognize(gray, roi)
            color = (0, 255, 0) if name != 'unknown' else (0, 165, 255)
            cv2.rectangle(out, (x, y), (x+w, y+h), color, 2)
            cv2.putText(out, f"{name} {score:.2f}", (x, y-6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)
            if name != 'unknown':
                known_face_bboxes.append((x, y, w, h))
            enroll_bbox = (x, y, w, h)

        # Handle collection of multiple samples
        if collect is not None and enroll_bbox is not None:
            now_ts = time.time()
            if now_ts - collect['last'] >= collect['interval']:
                (x, y, w, h) = enroll_bbox
                gray_now = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                if facedb.enroll(gray_now, x, y, w, h, collect['name']):
                    collect['collected'] += 1
                    collect['last'] = now_ts
                if collect['collected'] >= collect['n']:
                    facedb.load_and_train()
                    toast(f"Collected {collect['collected']} samples for {collect['name']}")
                    collect = None

        # Handle pet collection of multiple samples
        if pet_collect is not None:
            now_ts = time.time()
            if now_ts - pet_collect['last'] >= pet_collect['interval']:
                # pick bbox for requested species from current dets
                species = pet_collect['species']
                candidates = [(x,y,w,h) for (cls,conf,x,y,w,h) in dets if cls == species]
                if candidates:
                    # choose largest
                    bx = max(candidates, key=lambda b: b[2]*b[3])
                    (x,y,w,h) = bx
                    roi = frame[max(0,y):y+h, max(0,x):x+w]
                    if pets.enroll(roi, pet_collect['name'], species):
                        pet_collect['collected'] += 1
                        pet_collect['last'] = now_ts
                if pet_collect['collected'] >= pet_collect['n']:
                    pets.load()
                    toast(f"Collected {pet_collect['collected']} for {species}:{pet_collect['name']}")
                    pet_collect = None

        # Draw object detections
        dog_name = 'unknown'
        for (cls, conf, x, y, w, h) in dets:
            if cls not in ('person', 'dog', 'cat'):
                continue
            color = (255, 0, 0) if cls == 'person' else (0, 0, 255) if cls == 'dog' else (255, 0, 255)
            cv2.rectangle(out, (x, y), (x+w, y+h), color, 2)
            label = f"{cls} {conf:.2f}"
            if cls in ('dog', 'cat'):
                roi = frame[max(0,y):max(0,y)+max(1,h), max(0,x):max(0,x)+max(1,w)]
                name, score = pets.recognize(roi, cls)
                if cls == 'dog':
                    dog_name = name
                    last_dog_bbox = (x, y, w, h)
                else:
                    last_cat_bbox = (x, y, w, h)
                if name != 'unknown':
                    label = f"{cls}:{name} {conf:.2f}/{score:.2f}"
            cv2.putText(out, label, (x, max(0, y-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)

        # Tracking logic: pick target based on mode
        target_bbox = None
        if track_mode == 1:  # person
            persons = [(x, y, w, h) for (cls, conf, x, y, w, h) in dets if cls == 'person']
            if persons:
                target_bbox = max(persons, key=lambda b: b[2]*b[3])
        elif track_mode == 2:  # known face
            if known_face_bboxes:
                target_bbox = max(known_face_bboxes, key=lambda b: b[2]*b[3])
        elif track_mode == 3:  # dog
            dogs = [(x, y, w, h) for (cls, conf, x, y, w, h) in dets if cls == 'dog']
            if dogs:
                target_bbox = max(dogs, key=lambda b: b[2]*b[3])
        elif track_mode == 4:  # cat
            cats = [(x, y, w, h) for (cls, conf, x, y, w, h) in dets if cls == 'cat']
            if cats:
                target_bbox = max(cats, key=lambda b: b[2]*b[3])

        # Update or start tracker
        if target_bbox is not None:
            if tracker is None:
                start_tracker(frame, target_bbox)
            else:
                ok, bbox = tracker.update(frame)
                if ok:
                    track_bbox = tuple(map(int, bbox))
                else:
                    tracker = None
                    start_tracker(frame, target_bbox)

        # PTZ command to follow tracked bbox
        if track_bbox is not None:
            (x, y, w, h) = track_bbox
            cx = x + w/2
            cy = y + h/2
            fx = frame.shape[1] / 2
            fy = frame.shape[0] / 2
            dx = cx - fx
            dy = cy - fy
            cv2.circle(out, (int(cx), int(cy)), 4, (0, 255, 255), -1)
            # deadzone
            thresh_x = frame.shape[1] * 0.05
            thresh_y = frame.shape[0] * 0.05
            now = time.time()
            if now - last_ptz > 0.2:  # rate-limit
                if dx > thresh_x:
                    send_ptz(base_http, auth_header, token_suffix, 'left')
                    last_ptz = now
                elif dx < -thresh_x:
                    send_ptz(base_http, auth_header, token_suffix, 'right')
                    last_ptz = now
                if dy > thresh_y:
                    send_ptz(base_http, auth_header, token_suffix, 'down')
                    last_ptz = now
                elif dy < -thresh_y:
                    send_ptz(base_http, auth_header, token_suffix, 'up')
                    last_ptz = now

        # Old console-based enrollment retained as fallback via hotkeys
        keys = pygame.key.get_pressed()
        if keys[pygame.K_e] and enroll_bbox is not None:
            ret = modal_text_input(screen, 'Enroll Face', [{'label':'Name','value':'','masked':False}], confirm_label='Enroll')
            if ret:
                (x, y, w, h) = enroll_bbox
                gray_now = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                if facedb.enroll(gray_now, x, y, w, h, ret[0]):
                    facedb.load_and_train()
                    toast(f"Enrolled {ret[0]}")
        if keys[pygame.K_p] and (last_dog_bbox is not None or last_cat_bbox is not None):
            ret = modal_text_input(screen, 'Enroll Pet', [
                {'label':'Name','value':'','masked':False},
                {'label':'Species (dog/cat)','value':'dog','masked':False}
            ], confirm_label='Enroll')
            if ret:
                (pname, pspecies) = ret
                bb = last_dog_bbox if pspecies.lower().startswith('dog') or last_cat_bbox is None else last_cat_bbox
                (x, y, w, h) = bb
                roi = frame[max(0,y):y+h, max(0,x):x+w]
                if pets.enroll(roi, pname, pspecies):
                    pets.load()
                    toast(f"Enrolled {pspecies}:{pname}")

        # Pygame blit
        surf = pygame.image.frombuffer(out.tobytes(), out.shape[1::-1], 'BGR')
        screen.blit(surf, (0, 0))
        # HUD
        det_label = 'YOLO' if det_mode=='yolo' and detector_yolo.available() else 'noDet'
        text = f"mode:{track_mode} {det_label} faces:{len(faces)} dog:{dog_name}"
        screen.blit(font.render(text, True, (255, 255, 255)), (8, 8))

        # Collection progress HUD
        if collect is not None:
            prog = f"Collecting {collect['collected']}/{collect['n']} for {collect['name']} (click Collect to cancel)"
            msg = ui_font.render(prog, True, (255,255,255))
            bg_rect = pygame.Rect(8, 24, msg.get_width()+14, msg.get_height()+10)
            pygame.draw.rect(screen, (0,0,0), bg_rect, border_radius=8)
            screen.blit(msg, (bg_rect.x + 7, bg_rect.y + 5))

        # Draw overlay buttons
        buttons = layout_buttons()
        for name, (rect, label) in buttons.items():
            draw_button(screen, rect, label, ui_font)

        # Toast message and collection progress
        if time.time() < toast_until:
            msg = ui_font.render(toast_msg, True, (255,255,255))
            bg_rect = pygame.Rect(8, 40, msg.get_width()+14, msg.get_height()+10)
            pygame.draw.rect(screen, (0,0,0), bg_rect, border_radius=8)
            screen.blit(msg, (bg_rect.x + 7, bg_rect.y + 5))
        if collect is not None:
            prog = ui_font.render(f"Face: {collect['collected']}/{collect['n']} {collect['name']}", True, (255,255,255))
            screen.blit(prog, (8, 64))
        if pet_collect is not None:
            prog2 = ui_font.render(f"{pet_collect['species']}: {pet_collect['collected']}/{pet_collect['n']} {pet_collect['name']}", True, (255,255,255))
            screen.blit(prog2, (8, 84))

        pygame.display.flip()

    stream.stop()
    pygame.quit()


if __name__ == '__main__':
    main()
## `gallery.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\gallery.py
**Size**: 2563 bytes
**Modified**: 2025-08-29 00:27:14 +0100
**SHA256**: ed4c0bcf1225b6ccd0d3c7cd68b72431a3de9b44b730b25278901cce8f198aaf
``````python"""Qt thumbnail gallery dialog for selecting and deleting images."""
from __future__ import annotations
import os
from PySide6 import QtCore, QtGui, QtWidgets


class GalleryDialog(QtWidgets.QDialog):
    def __init__(self, dir_path: str, title: str, parent=None):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.resize(900, 600)
        self.dir_path = dir_path
        v = QtWidgets.QVBoxLayout(self)
        self.list = QtWidgets.QListWidget()
        self.list.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.list.setIconSize(QtCore.QSize(160, 120))
        self.list.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.list.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        v.addWidget(self.list, 1)
        btns = QtWidgets.QDialogButtonBox()
        self.btn_del = btns.addButton('Delete Selected', QtWidgets.QDialogButtonBox.ButtonRole.ActionRole)
        self.btn_close = btns.addButton(QtWidgets.QDialogButtonBox.StandardButton.Close)
        v.addWidget(btns)
        self.btn_del.clicked.connect(self.do_delete)
        self.btn_close.clicked.connect(self.accept)
        self.populate()

    def populate(self):
        self.list.clear()
        if not os.path.isdir(self.dir_path):
            return
        files = [os.path.join(self.dir_path,f) for f in os.listdir(self.dir_path)
                 if f.lower().endswith(('.jpg','.jpeg','.png'))]
        files.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        for fp in files:
            item = QtWidgets.QListWidgetItem(os.path.basename(fp))
            try:
                pix = QtGui.QPixmap(fp)
                if not pix.isNull():
                    item.setIcon(QtGui.QIcon(pix.scaled(160,120, QtCore.Qt.KeepAspectRatio, QtCore.Qt.SmoothTransformation)))
            except Exception:
                pass
            item.setData(QtCore.Qt.ItemDataRole.UserRole, fp)
            self.list.addItem(item)

    def do_delete(self):
        items = self.list.selectedItems()
        if not items:
            return
        if QtWidgets.QMessageBox.question(self, 'Delete', f'Delete {len(items)} images?') != QtWidgets.QMessageBox.Yes:
            return
        cnt=0
        for it in items:
            fp = it.data(QtCore.Qt.ItemDataRole.UserRole)
            try:
                os.remove(fp)
                cnt+=1
            except Exception:
                pass
        self.populate()
        QtWidgets.QMessageBox.information(self, 'Manage', f'Deleted {cnt} files')

## `mdi_app.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\mdi_app.py
**Size**: 90967 bytes
**Modified**: 2025-11-04 22:15:40 +0000
**SHA256**: bd37f7d7865ec318d26c78a0f1bae463681eaddfe7432d06daf182441bb81d26
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools


class CollectionDialog(QtWidgets.QDialog):
    stopClicked = QtCore.Signal()
    def __init__(self, parent=None, title: str = 'Collection'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(False)
        self.setWindowFlag(QtCore.Qt.WindowType.WindowStaysOnTopHint, True)
        self.resize(360, 120)
        v = QtWidgets.QVBoxLayout(self)
        self.lbl = QtWidgets.QLabel('Starting...')
        self.lbl.setWordWrap(True)
        v.addWidget(self.lbl, 1)
        btns = QtWidgets.QDialogButtonBox()
        self.btn_stop = btns.addButton('Stop', QtWidgets.QDialogButtonBox.ActionRole)
        self.btn_close = btns.addButton(QtWidgets.QDialogButtonBox.Close)
        v.addWidget(btns)
        self.btn_stop.clicked.connect(lambda: self.stopClicked.emit())
        self.btn_close.clicked.connect(self.accept)
    def set_status(self, text: str):
        try:
            self.lbl.setText(text)
        except Exception:
            pass

# ------------------------------
# Simple AI helpers (YOLO, FaceDB, PetsDB)
# ------------------------------

class YOLODetector:
    """Lightweight YOLO (ONNX) wrapper for COCO classes.
    Uses OpenCV DNN and letterbox preprocessing.
    """
    COCO = [
        'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light',
        'fire hydrant','stop sign','parking meter','bench','bird','cat','dog','horse','sheep','cow','elephant',
        'bear','zebra','giraffe','backpack','umbrella','handbag','tie','suitcase','frisbee','skis','snowboard',
        'sports ball','kite','baseball bat','baseball glove','skateboard','surfboard','tennis racket','bottle',
        'wine glass','cup','fork','knife','spoon','bowl','banana','apple','sandwich','orange','broccoli',
        'carrot','hot dog','pizza','donut','cake','chair','couch','potted plant','bed','dining table','toilet',
        'tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven','toaster','sink','refrigerator',
        'book','clock','vase','scissors','teddy bear','hair drier','toothbrush'
    ]
    def __init__(self, model_path='ai/models/yolo.onnx', input_size=640, conf=0.35, iou=0.45):
        self.net = None
        self.size = input_size
        self.conf = conf
        self.iou = iou
        if os.path.exists(model_path):
            try:
                self.net = cv2.dnn.readNetFromONNX(model_path)
                self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
                self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            except Exception as e:
                print('[YOLO] load failed:', e)

    def available(self):
        return self.net is not None

    def _letterbox(self, img, new=640):
        h,w = img.shape[:2]
        r = min(new/h, new/w)
        nh, nw = int(h*r), int(w*r)
        resized = cv2.resize(img, (nw, nh))
        canvas = np.full((new,new,3), 114, np.uint8)
        top = (new-nh)//2; left=(new-nw)//2
        canvas[top:top+nh, left:left+nw] = resized
        return canvas, r, left, top

    def detect(self, frame_bgr):
        if self.net is None:
            return []
        img, r, dx, dy = self._letterbox(frame_bgr, self.size)
        blob = cv2.dnn.blobFromImage(img, 1/255.0, (self.size,self.size), swapRB=True, crop=False)
        self.net.setInput(blob)
        out = self.net.forward()
        out = np.squeeze(out)
        if out.ndim == 2 and out.shape[0] in (84,85):
            out = out.T
        elif out.ndim == 3:
            o = out[0]
            out = o.T if o.shape[0] in (84,85) else o
        H,W = frame_bgr.shape[:2]
        boxes=[]; scores=[]; classes=[]
        for det in out:
            cx,cy,w,h = det[:4]
            if det.shape[0] >= 85:
                obj = float(det[4]); cls_scores = det[5:]
                c = int(np.argmax(cls_scores)); conf = obj*float(cls_scores[c])
            else:
                cls_scores = det[4:]; c = int(np.argmax(cls_scores)); conf=float(cls_scores[c])
            if conf < self.conf: continue
            x1 = int(max(0, min(W-1, (cx-w/2 - dx)/r)))
            y1 = int(max(0, min(H-1, (cy-h/2 - dy)/r)))
            x2 = int(max(0, min(W-1, (cx+w/2 - dx)/r)))
            y2 = int(max(0, min(H-1, (cy+h/2 - dy)/r)))
            boxes.append([x1,y1,x2-x1,y2-y1]); scores.append(conf); classes.append(c)
        if not boxes: return []
        idxs = cv2.dnn.NMSBoxes(boxes, scores, self.conf, self.iou)
        idxs = np.array(idxs).reshape(-1) if isinstance(idxs,(list,tuple,np.ndarray)) else np.array([])
        dets=[]
        for i in idxs:
            name = self.COCO[classes[i]] if 0<=classes[i]<len(self.COCO) else str(classes[i])
            x,y,w,h = boxes[i]
            dets.append((name, float(scores[i]), x,y,w,h))
        return dets


class FaceDB:
    """Face recognition store.
    - Trains LBPH if contrib available; falls back to ORB matching.
    - Persists samples as images on disk under ai/data/faces/<name>.
    """
    def __init__(self, base='ai/data/faces'):
        self.base = base
        os.makedirs(self.base, exist_ok=True)
        self.size = (160,160)
        self.model = None
        try:
            self.model = cv2.face.LBPHFaceRecognizer_create(radius=1, neighbors=8, grid_x=8, grid_y=8)
        except Exception:
            self.model = None
        self.labels=[]
        self.orb = cv2.ORB_create(nfeatures=1000)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db_descs={}
        self.cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    def load(self):
        samples=[]; labels=[]; label_map={}; lid=0
        for name in sorted(os.listdir(self.base)):
            p=os.path.join(self.base,name)
            if not os.path.isdir(p): continue
            label_map[name]=lid
            for fn in os.listdir(p):
                if not fn.lower().endswith(('.jpg','.jpeg','.png')): continue
                img=cv2.imread(os.path.join(p,fn),cv2.IMREAD_GRAYSCALE)
                if img is None: continue
                samples.append(cv2.resize(img,self.size)); labels.append(lid)
            lid+=1
        self.labels=[None]*lid
        for n,i in label_map.items(): self.labels[i]=n
        # LBPH
        if samples and self.model is not None:
            try:
                self.model.train(samples, np.array(labels))
            except Exception:
                self.model=None
        # ORB fallback DB
        self.db_descs={}
        for name in sorted(os.listdir(self.base)):
            p=os.path.join(self.base,name)
            if not os.path.isdir(p): continue
            ds=[]
            for fn in os.listdir(p):
                if not fn.lower().endswith(('.jpg','.jpeg','.png')): continue
                img=cv2.imread(os.path.join(p,fn),cv2.IMREAD_GRAYSCALE)
                if img is None: continue
                img=cv2.resize(img,self.size)
                kp,d=self.orb.detectAndCompute(img,None)
                if d is not None: ds.append(d)
            if ds: self.db_descs[name]=ds

    def detect_faces(self, gray):
        try:
            eq=cv2.createCLAHE(2.0,(8,8)).apply(gray)
        except Exception:
            eq=cv2.equalizeHist(gray)
        minsz=max(40,int(0.12*min(gray.shape[:2])))
        faces=self.cascade.detectMultiScale(eq,1.1,4,minSize=(minsz,minsz))
        if len(faces)==0:
            faces=self.cascade.detectMultiScale(eq,1.05,3,minSize=(minsz,minsz))
        return faces

    def recognize_roi(self, gray_roi):
        roi=cv2.resize(gray_roi,self.size)
        if self.model is not None:
            try:
                pred,dist=self.model.predict(roi)
                if 0<=pred<len(self.labels) and dist<=95.0:
                    return self.labels[pred], max(0.0,min(1.0,1.0-(dist/95.0)))
            except Exception:
                pass
        # ORB fallback
        kp,d=self.orb.detectAndCompute(roi,None)
        if d is None or not self.db_descs: return 'unknown',0.0
        best='unknown'; bs=0.0
        for name,ds in self.db_descs.items():
            tot=0; good=0
            for dbd in ds:
                m=self.bf.match(d,dbd); tot+=len(m); good+=sum(1 for mm in m if mm.distance<40)
            if tot==0: continue
            sc=good/float(tot)
            if sc>bs: bs=sc; best=name
        return (best if bs>0.12 else 'unknown'), bs

    def enroll(self, frame_gray, x,y,w,h, name:str):
        name=''.join(c for c in name.strip() if c.isalnum() or c in ('_','-')) or 'person'
        p=os.path.join(self.base,name); os.makedirs(p,exist_ok=True)
        roi=cv2.resize(frame_gray[y:y+h,x:x+w], self.size)
        now=time.time(); ts=int((now - int(now))*1000); ds=time.strftime('%Y%m%d_%H%M%S', time.localtime(now))
        base=f'{ds}_{ts:03d}'
        cv2.imwrite(os.path.join(p,f'{base}.jpg'), roi)
        try: cv2.imwrite(os.path.join(p,f'{base}_flip.jpg'), cv2.flip(roi,1))
        except Exception: pass
        return True


class PetsDB:
    """Pet recognition store (dogs/cats) using ORB descriptors.
    - Persists samples as grayscale images under ai/data/pets/{dogs,cats}/<name>.
    """
    def __init__(self, base='ai/data/pets'):
        self.base=base; os.makedirs(os.path.join(base,'dogs'),exist_ok=True); os.makedirs(os.path.join(base,'cats'),exist_ok=True)
        self.orb=cv2.ORB_create(nfeatures=1000); self.bf=cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db={'dogs':{},'cats':{}}

    def load(self):
        self.db={'dogs':{},'cats':{}}
        for sp in ('dogs','cats'):
            root=os.path.join(self.base,sp)
            if not os.path.isdir(root): continue
            for name in os.listdir(root):
                p=os.path.join(root,name)
                if not os.path.isdir(p): continue
                ds=[]
                for fn in os.listdir(p):
                    if not fn.lower().endswith(('.jpg','.jpeg','.png')): continue
                    img=cv2.imread(os.path.join(p,fn),cv2.IMREAD_GRAYSCALE)
                    if img is None: continue
                    img=cv2.resize(img,(320,320))
                    kp,d=self.orb.detectAndCompute(img,None)
                    if d is not None: ds.append(d)
                if ds: self.db[sp][name]=ds

    def enroll(self, roi_bgr, name:str, species:str):
        sp='dogs' if species.lower().startswith('dog') else 'cats'
        name=''.join(c for c in name.strip() if c.isalnum() or c in ('_','-')) or 'pet'
        p=os.path.join(self.base,sp,name); os.makedirs(p,exist_ok=True)
        gray=cv2.cvtColor(roi_bgr,cv2.COLOR_BGR2GRAY); gray=cv2.resize(gray,(320,320))
        now=time.time(); ts=int((now - int(now))*1000); ds=time.strftime('%Y%m%d_%H%M%S', time.localtime(now))
        fn=f'{ds}_{ts:03d}.jpg'
        cv2.imwrite(os.path.join(p,fn), gray); return True

    def recognize(self, roi_bgr, species:str):
        sp='dogs' if species=='dog' else 'cats'
        if not self.db[sp]: return 'unknown',0.0
        gray=cv2.cvtColor(roi_bgr,cv2.COLOR_BGR2GRAY); gray=cv2.resize(gray,(320,320))
        kp,d=self.orb.detectAndCompute(gray,None)
        if d is None: return 'unknown',0.0
        best='unknown'; bs=0.0
        for name,ds in self.db[sp].items():
            tot=0; good=0
            for dbd in ds:
                m=self.bf.match(d,dbd); tot+=len(m); good+=sum(1 for mm in m if mm.distance<40)
            if tot==0: continue
            sc=good/float(tot)
            if sc>bs: bs=sc; best=name
        return (best if bs>0.12 else 'unknown'), bs


@dataclass
class CameraConfig:
    name: str
    host: str                 # ip[:port] for port 80
    user: Optional[str] = None
    password: Optional[str] = None
    token: Optional[str] = None  # Base64 of user:pass

    def stream_url(self) -> str:
        base = self.host.split(':')[0]
        suffix = f"?token={self.token}" if self.token else ""
        return f"http://{base}:81/stream{suffix}"

    def stream_url_with_creds(self) -> Optional[str]:
        """Fallback URL form that embeds user:pass in the authority.
        Some ESP32 firmwares only honor this style for the :81 stream.
        """
        if not (self.user and self.password):
            return None
        base = self.host.split(':')[0]
        return f"http://{self.user}:{self.password}@{base}:81/stream"

    def auth_header(self) -> Optional[str]:
        if self.token:
            return None
        if self.user and self.password:
            import base64
            up = f"{self.user}:{self.password}".encode('utf-8')
            return "Basic " + base64.b64encode(up).decode('ascii')
        return None


class CameraStreamThread(QtCore.QThread):
    frameReady = QtCore.Signal(np.ndarray, float)  # (bgr_frame, timestamp)

    def __init__(self, cfg: CameraConfig, parent=None, prebuffer_seconds: float = 5.0):
        super().__init__(parent)
        self.cfg = cfg
        self._stop = threading.Event()
        self._session = None
        self._resp = None
        self._buf = bytearray()
        self._boundary = b"--frame"
        self.prebuffer: Deque[Tuple[np.ndarray, float]] = deque(maxlen=int(prebuffer_seconds * 20))  # assume ~20fps cap

    def stop(self):
        self._stop.set()
        try:
            if self._resp is not None:
                try:
                    self._resp.close()
                except Exception:
                    pass
                self._resp = None
        except Exception:
            pass
        try:
            if self._session is not None:
                self._session.close()
                self._session = None
        except Exception:
            pass

    def run(self):
        headers = {}
        # Prefer requests' BasicAuth when user/pass provided
        req_auth = None
        if not self.cfg.token and self.cfg.user and self.cfg.password:
            try:
                from requests.auth import HTTPBasicAuth
                req_auth = HTTPBasicAuth(self.cfg.user, self.cfg.password)
            except Exception:
                # fallback to manual header
                ah = self.cfg.auth_header()
                if ah:
                    headers['Authorization'] = ah
        self._session = requests.Session()
        # Attempt 1: standard URL (+token if provided) with auth/header
        try:
            self._resp = self._session.get(self.cfg.stream_url(), headers=headers, auth=req_auth, stream=True, timeout=10)
            self._resp.raise_for_status()
        except Exception as e1:
            # Attempt 2: URL-embedded credentials (user:pass@host)
            tried2 = False
            url2 = self.cfg.stream_url_with_creds()
            if url2:
                try:
                    self._resp = self._session.get(url2, stream=True, timeout=10)
                    self._resp.raise_for_status()
                    tried2 = True
                except Exception:
                    tried2 = True
            # Attempt 3: user/password as query params (some forks)
            tried3 = False
            if not (self.cfg.token) and (self.cfg.user and self.cfg.password):
                base = self.cfg.host.split(':')[0]
                url3 = f"http://{base}:81/stream?user={self.cfg.user}&password={self.cfg.password}"
                try:
                    self._resp = self._session.get(url3, stream=True, timeout=10)
                    self._resp.raise_for_status()
                    tried3 = True
                except Exception:
                    tried3 = True
            if not (tried2 or tried3):
                # No other forms attempted; report original error
                print(f"[Stream] Failed to connect {self.cfg.name}: {e1}")
                return
            # If both fallbacks failed, report combined failure
            if (self._resp is None) or (getattr(self._resp, 'status_code', 0) >= 400):
                print(f"[Stream] Failed to connect {self.cfg.name}: {e1}")
                return
        try:
            for chunk in self._resp.iter_content(chunk_size=8192):
                if self._stop.is_set():
                    break
                if not chunk:
                    continue
                self._buf += chunk
                while True:
                    hdr_end = self._buf.find(b"\r\n\r\n")
                    if hdr_end == -1:
                        break
                    headers_blob = self._buf[:hdr_end].decode('latin1', errors='ignore').lower()
                    cl_idx = headers_blob.find('content-length:')
                    if cl_idx == -1:
                        # resync to boundary
                        bidx = self._buf.find(self._boundary)
                        self._buf = self._buf[bidx:] if bidx != -1 else self._buf[hdr_end+4:]
                        continue
                    try:
                        cl_line = headers_blob[cl_idx:].split('\r\n', 1)[0]
                        length = int(cl_line.split(':', 1)[1].strip())
                    except Exception:
                        bidx = self._buf.find(self._boundary)
                        self._buf = self._buf[bidx:] if bidx != -1 else self._buf[hdr_end+4:]
                        continue
                    start = hdr_end + 4
                    if len(self._buf) < start + length:
                        break
                    jpg = self._buf[start:start+length]
                    tail = self._buf[start+length:]
                    bmark = b"\r\n--frame\r\n"
                    if tail.startswith(bmark):
                        self._buf = bytearray(tail[len(bmark):])
                    else:
                        bpos = tail.find(bmark)
                        self._buf = bytearray(tail[bpos+len(bmark):] if bpos >= 0 else b"")

                    # decode
                    arr = np.frombuffer(jpg, np.uint8)
                    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
                    if frame is None:
                        continue
                    ts = time.time()
                    self.prebuffer.append((frame, ts))
                    self.frameReady.emit(frame, ts)
        except Exception as e:
            if not self._stop.is_set():
                print(f"[Stream] Reader error on {self.cfg.name}: {e}")
        finally:
            try:
                if self._resp is not None:
                    self._resp.close()
            except Exception:
                pass
            self._resp = None


class CameraWidget(QtWidgets.QWidget):
    closed = QtCore.Signal(dict)
    eventLogged = QtCore.Signal(str)
    def __init__(self, cfg: CameraConfig, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.setWindowTitle(f"{cfg.name} [{cfg.host}]")
        self.label = QtWidgets.QLabel('Connecting…')
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setMinimumSize(320, 240)
        self.label.setStyleSheet('background:#000; color:#9cf;')

        # Recording state
        self.recording = False
        self.writer: Optional[cv2.VideoWriter] = None
        self.out_dir = os.path.join('ai', 'recordings')
        os.makedirs(self.out_dir, exist_ok=True)
        self.target_fps = 20.0
        # Event logging (per camera)
        self.log_dir = os.path.join('ai', 'logs')
        os.makedirs(self.log_dir, exist_ok=True)
        safe_name = ''.join(c if c.isalnum() or c in ('_','-') else '_' for c in (self.cfg.name or 'cam'))
        safe_host = ''.join(c if c.isalnum() or c in ('_','-','.') else '_' for c in (self.cfg.host or 'host'))
        self.log_path = os.path.join(self.log_dir, f"events_{safe_name}_{safe_host}.log")
        self.event_buffer: list[str] = []  # last N events for sidebar
        # Presence tracking with hysteresis
        self.presence = {}  # id -> {present:bool, enter:float, last:float, missing_since:float|None}
        self.grace_sec = 5  # require this absence before logging exit

        # Controls (local toolbar)
        btns = QtWidgets.QToolBar()
        btns.setMovable(False)
        act_start = btns.addAction('Start')
        act_stop = btns.addAction('Stop')
        btns.addSeparator()
        act_rec = btns.addAction('Start Rec')
        act_stoprec = btns.addAction('Stop Rec')

        act_start.triggered.connect(self.start_stream)
        act_stop.triggered.connect(self.stop_stream)
        act_rec.triggered.connect(self.start_recording)
        act_stoprec.triggered.connect(self.stop_recording)

        # Toolbar toggles
        # AI toggles moved into a dropdown menu button
        self.chk_yolo = QtWidgets.QCheckBox('YOLO'); self.chk_yolo.setChecked(True)
        self.chk_face = QtWidgets.QCheckBox('Face'); self.chk_face.setChecked(True)
        self.chk_yolo.setVisible(False); self.chk_face.setVisible(False)
        btns.addSeparator()
        ai_btn = QtWidgets.QToolButton()
        ai_btn.setText('AI')
        ai_btn.setPopupMode(QtWidgets.QToolButton.InstantPopup)
        ai_menu = QtWidgets.QMenu(ai_btn)
        act_ai_yolo = ai_menu.addAction('YOLO')
        act_ai_yolo.setCheckable(True); act_ai_yolo.setChecked(self.chk_yolo.isChecked())
        act_ai_face = ai_menu.addAction('Face')
        act_ai_face.setCheckable(True); act_ai_face.setChecked(self.chk_face.isChecked())
        # Optional dog identity recognition
        self.chk_dogid = QtWidgets.QCheckBox('Dog ID'); self.chk_dogid.setChecked(True)
        act_ai_dogid = ai_menu.addAction('Dog ID')
        act_ai_dogid.setCheckable(True); act_ai_dogid.setChecked(self.chk_dogid.isChecked())
        def _sync_ai():
            act_ai_yolo.setChecked(self.chk_yolo.isChecked())
            act_ai_face.setChecked(self.chk_face.isChecked())
            act_ai_dogid.setChecked(self.chk_dogid.isChecked())
        act_ai_yolo.toggled.connect(self.chk_yolo.setChecked)
        act_ai_face.toggled.connect(self.chk_face.setChecked)
        act_ai_dogid.toggled.connect(self.chk_dogid.setChecked)
        self.chk_yolo.toggled.connect(lambda _: _sync_ai())
        self.chk_face.toggled.connect(lambda _: _sync_ai())
        self.chk_dogid.toggled.connect(lambda _: _sync_ai())
        ai_btn.setMenu(ai_menu)
        btns.addWidget(ai_btn)

        self.lbl_status = QtWidgets.QLabel('Ready')
        self.lbl_status.setStyleSheet('color:#246; padding:2px 4px;')

        layout = QtWidgets.QVBoxLayout(self)
        layout.setContentsMargins(4,4,4,4)
        layout.addWidget(btns)
        layout.addWidget(self.label, 1)
        # intentionally not adding lbl_status to the layout (no in-window collection status)

        # Stream thread
        self.thr = CameraStreamThread(cfg)
        self.thr.frameReady.connect(self.on_frame)
        self._last_ts = None
        # AI components per camera
        self.yolo = YOLODetector()
        self.facedb = FaceDB(); self.facedb.load()
        self.pets = PetsDB(); self.pets.load()
        self.last_face_bbox=None; self.last_pet_bbox=None
        self.collect_face=None  # {'name':str,'n':int,'col':int,'last':float}
        self.collect_pet=None   # {'name':str,'sp':str,'n':int,'col':int,'last':float}
        self.dogid_thresh = 0.18
        self.collect_dlg = None # popup dialog for collection status

        # Shared frame and results (thread-safe)
        self._latest_frame = None
        self._frame_lock = threading.Lock()
        self._last_dets = []      # [(cls,conf,x,y,w,h)] in original coords
        self._last_faces = []     # [(name,score,x,y,w,h)] in original coords
        self._last_overlay_ts = 0

        # Simple tracker for object permanence + aiming
        self.tracker = SimpleTracker(ttl=1.0)  # seconds before track expires
        # Detection worker thread (decouples heavy CV from UI), interval overridable by settings
        self.det_thr = DetectionThread(self)
        self.det_thr.resultsReady.connect(self.on_results)
        self.det_thr.start()
        # PTZ aiming timer
        self._aim_timer = QtCore.QTimer(self)
        self._aim_timer.setInterval(300)
        self._aim_timer.timeout.connect(self.aim_at_target)
        self._aim_timer.start()

        # hidden inputs used by MainWindow to pass parameters
        self.ed_name = QtWidgets.QLineEdit()
        self.cmb_species = QtWidgets.QComboBox(); self.cmb_species.addItems(['dog','cat'])

    def start_stream(self):
        if not self.thr.isRunning():
            self.thr._stop.clear()
            self.thr.start()

    def stop_stream(self):
        if self.thr.isRunning():
            self.thr.stop()
            # Give the worker time to unwind network loop
            if not self.thr.wait(700):
                try:
                    # As a last resort on shutdown
                    self.thr.terminate()
                    self.thr.wait(200)
                except Exception:
                    pass


    def aim_at_target(self):
        """300 ms timer callback. Pick best target and send small PTZ step."""
        # Rate-limit
        now = time.time()
        if getattr(self, "_last_aim_ts", 0) and now - self._last_aim_ts < 0.28:
            return
        self._last_aim_ts = now

        # Need detections and YOLO enabled
        if not getattr(self, "chk_yolo", None) or not self.chk_yolo.isChecked():
            return
        dets = getattr(self, "_last_dets", None)
        if not dets:
            return

        # Choose highest priority box: person > dog > cat
        pri = {"person": 3, "dog": 2, "cat": 1}
        dets = [d for d in dets if d[0] in pri]
        if not dets:
            return
        dets.sort(key=lambda d: (pri[d[0]], d[1]), reverse=True)
        cls, conf, x, y, w, h = dets[0]
        if conf < 0.35:
            return

        # Compute normalized center error
        H, W = self.current_size()
        cx = x + w * 0.5
        cy = y + h * 0.5
        ex = (cx - W * 0.5) / (W * 0.5)   # -1..1 (right = +)
        ey = (cy - H * 0.5) / (H * 0.5)   # -1..1 (down = +)

        dead = 0.10
        if abs(ex) < dead and abs(ey) < dead:
            return

        # Map error to small PTZ steps; X inverted so right error -> pan positive left
        dx = int(max(-15, min(15, -ex * 20)))
        dy = int(max(-15, min(15,  ey * 20)))  # image down => tilt positive

        # Build endpoint
        host = self.cfg.host.split(":")[0]
        url = f"http://{host}/ptz/step?dx={dx}&dy={dy}"
        if getattr(self.cfg, "token", None):
            sep = "&" if "?" in url else "?"
            url = f"{url}{sep}token={self.cfg.token}"

        try:
            requests.get(url, timeout=0.4)
        except Exception:
            pass
                    
    def apply_config(self, new_cfg: CameraConfig):
        # Stop current stream thread and swap config
        try:
            self.stop_stream()
        except Exception:
            pass
        self.cfg = new_cfg
        self.setWindowTitle(new_cfg.name)
        # Rebuild stream thread to ensure fresh session/auth
        try:
            if hasattr(self.thr, 'frameReady'):
                try:
                    self.thr.frameReady.disconnect(self.on_frame)
                except Exception:
                    pass
        except Exception:
            pass
        try:
            self.thr = CameraStreamThread(new_cfg)
            self.thr.frameReady.connect(self.on_frame)
        except Exception:
            # If anything goes wrong, keep old thread but update its cfg
            try:
                self.thr.cfg = new_cfg
            except Exception:
                pass
        # Clear prebuffer and frames
        with self._frame_lock:
            self._latest_frame = None
        try:
            self.thr._buf = bytearray()
            if hasattr(self.thr, 'prebuffer') and hasattr(self.thr.prebuffer, 'clear'):
                self.thr.prebuffer.clear()
        except Exception:
            pass
        # Restart
        try:
            self.start_stream()
        except Exception:
            pass

    def start_recording(self):
        if self.recording:
            return
        # estimate FPS from prebuffer timing
        fps = self.target_fps
        if len(self.thr.prebuffer) >= 5:
            tspan = self.thr.prebuffer[-1][1] - self.thr.prebuffer[0][1]
            frames = len(self.thr.prebuffer)
            if tspan > 0:
                fps = max(5.0, min(30.0, frames / tspan))
        # open writer
        ts_str = time.strftime('%Y%m%d_%H%M%S')
        # Prefer MJPG/AVI for broad codec compatibility on Windows
        outfile = os.path.join(self.out_dir, f"{self.cfg.name}_{self.cfg.host}_{ts_str}.avi")
        h, w = self.current_size()
        # Ensure even dimensions for encoders like H.264/MP4
        if w % 2 == 1: w -= 1
        if h % 2 == 1: h -= 1
        fourcc = cv2.VideoWriter_fourcc(*'MJPG')
        self.writer = cv2.VideoWriter(outfile, fourcc, fps, (w, h))
        if not self.writer or not self.writer.isOpened():
            # Fallback to MP4V
            outfile = os.path.join(self.out_dir, f"{self.cfg.name}_{self.cfg.host}_{ts_str}.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            self.writer = cv2.VideoWriter(outfile, fourcc, fps, (w, h))
        if not self.writer or not self.writer.isOpened():
            QtWidgets.QMessageBox.warning(self, 'Record', 'Failed to open video writer')
            self.writer = None
            return
        # dump prebuffer first
        for frm, _ in list(self.thr.prebuffer):
            if frm.shape[1] != w or frm.shape[0] != h:
                frm = cv2.resize(frm, (w, h))
            self.writer.write(frm)
        self.recording = True
        self._update_titles(recording=True)

    def stop_recording(self):
        if self.recording and self.writer is not None:
            try:
                self.writer.release()
            except Exception:
                pass
        self.recording = False
        self.writer = None
        self._update_titles(recording=False)

    def current_size(self) -> Tuple[int,int]:
        # return H, W for writer
        pix = self.label.pixmap()
        if pix and not pix.isNull():
            return pix.height(), pix.width()
        return 480, 640

    @QtCore.Slot(np.ndarray, float)
    def on_frame(self, bgr: np.ndarray, ts: float):
        # Save latest frame for worker
        with self._frame_lock:
            self._latest_frame = bgr.copy()

        # Keep a clean copy before any overlays for saving samples
        raw_bgr = bgr.copy()

        # Draw last results as overlays
        # (Rendering stale by <= detection interval, keeps UI responsive)
        dets = self._last_dets
        faces = self._last_faces
        # Recognize dog identities if enabled
        recognized_dogs = []  # list of (name, score, x,y,w,h)
        if getattr(self, 'chk_dogid', None) and self.chk_dogid.isChecked():
            try:
                for (cls,conf,x,y,w,h) in dets:
                    if cls == 'dog':
                        roi = raw_bgr[max(0,y):y+h, max(0,x):x+w]
                        if roi.size > 0:
                            name, score = self.pets.recognize(roi, 'dog')
                            if name and name != 'unknown' and float(score) >= self.dogid_thresh:
                                recognized_dogs.append((name, float(score), x, y, w, h))
            except Exception:
                pass

        for (cls,conf,x,y,w,h) in dets:
            color = (255,0,0) if cls=='person' else (0,0,255) if cls=='dog' else (255,0,255) if cls=='cat' else (0,255,255)
            cv2.rectangle(bgr,(x,y),(x+w,y+h),color,2)
            label = f"{cls} {conf:.2f}"
            if cls == 'dog' and recognized_dogs:
                # find a matching recognized dog by IOU overlap
                best=None; best_iou=0.0
                for (nm,sc,dx,dy,dw,dh) in recognized_dogs:
                    # compute simple IOU
                    x1=max(x,dx); y1=max(y,dy); x2=min(x+w,dx+dw); y2=min(y+h,dy+dh)
                    inter=max(0,x2-x1)*max(0,y2-y1); ua=w*h + dw*dh - inter
                    iou = (inter/ua) if ua>0 else 0.0
                    if iou>best_iou: best_iou=iou; best=(nm,sc)
                if best and best_iou>=0.3:
                    label = f"dog {best[0]} {best[1]:.2f}"
            cv2.putText(bgr,label, (x,max(0,y-6)), cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA)
            if cls in ('dog','cat'):
                self.last_pet_bbox=(x,y,w,h,cls)
        self.last_face_bbox=None
        for (name,score,x,y,w,h) in faces:
            color=(0,255,0) if name!='unknown' else (0,165,255)
            cv2.rectangle(bgr,(x,y),(x+w,y+h),color,2)
            cv2.putText(bgr,f"{name} {score:.2f}", (x,max(0,y-6)), cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1,cv2.LINE_AA)
            self.last_face_bbox=(x,y,w,h)

        # handle collections
        now=time.time()

        # ---- Face collection progress & feedback ----
        if self.collect_face:
            name = self.collect_face['name']
            if self.last_face_bbox and (now - self.collect_face['last'] >= 0.2):
                x,y,w,h=self.last_face_bbox
                gray=cv2.cvtColor(raw_bgr,cv2.COLOR_BGR2GRAY)
                if self.facedb.enroll(gray,x,y,w,h,name):
                    self.collect_face['col']+=1; self.collect_face['last']=now
            # Update status every frame
            col = self.collect_face['col']; n = self.collect_face['n']
            if col >= n:
                self.facedb.load(); self.collect_face=None
                self._update_collect_dialog(f'Face collection done: {name} ({n}/{n})', done=True)
            else:
                suffix = '' if self.last_face_bbox else ' (waiting for face)'
                self._update_collect_dialog(f'Collecting face: {name} {col}/{n}{suffix}')
        # ---- Pet collection progress & feedback ----
        if self.collect_pet:
            species=self.collect_pet['sp']; name=self.collect_pet['name']
            # choose largest of desired species among current detections
            candidates=[(x,y,w,h) for (cls,conf,x,y,w,h) in dets if cls==species]
            if now - self.collect_pet['last'] >= 0.15 and candidates:
                bx=max(candidates,key=lambda b:b[2]*b[3])
                x,y,w,h=bx
                roi=raw_bgr[max(0,y):y+h, max(0,x):x+w]
                if self.pets.enroll(roi,name,species):
                    self.collect_pet['col']+=1; self.collect_pet['last']=now
            # Update status every frame
            col = self.collect_pet['col']; n = self.collect_pet['n']
            if col >= n:
                self.pets.load(); self.collect_pet=None
                self._update_collect_dialog(f'Pet collection done: {species}:{name} ({n}/{n})', done=True)
            else:
                self._update_collect_dialog(f'Collecting {species}: {name} {col}/{n}' + ('' if candidates else ' (waiting)'))

        # Update presence and events with dog identities if available
        try:
            dog_names = {nm for (nm,sc,x,y,w,h) in recognized_dogs}
            self._update_presence(dets, faces, now, dog_names)
        except Exception:
            pass

        # show (draw overlays after collection saving used raw_bgr)
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb.shape
        img = QtGui.QImage(rgb.data, w, h, ch*w, QtGui.QImage.Format.Format_RGB888)
        self.label.setPixmap(QtGui.QPixmap.fromImage(img).scaled(self.label.size(), QtCore.Qt.KeepAspectRatio, QtCore.Qt.SmoothTransformation))
        # write
        if self.recording and self.writer is not None:
            # ensure writer size consistency
            W = int(self.writer.get(cv2.CAP_PROP_FRAME_WIDTH))
            H = int(self.writer.get(cv2.CAP_PROP_FRAME_HEIGHT))
            # Fallback if writer reports invalid dimensions
            if W <= 0 or H <= 0:
                W, H = bgr.shape[1], bgr.shape[0]
            # Ensure even dims
            if W % 2 == 1: W -= 1
            if H % 2 == 1: H -= 1
            if bgr.shape[1] != W or bgr.shape[0] != H:
                try:
                    bgr = cv2.resize(bgr, (W, H))
                except Exception:
                    # As a last resort, skip resizing and write the original
                    W, H = bgr.shape[1], bgr.shape[0]
            try:
                self.writer.write(bgr)
            except Exception:
                # If writing fails, stop recording gracefully
                self.stop_recording()

    def _update_titles(self, recording: bool | None = None):
        rec = recording if recording is not None else self.recording
        title = f"{self.cfg.name} [{self.cfg.host}]" + (" (REC)" if rec else "")
        try:
            self.setWindowTitle(title)
        except Exception:
            pass
        try:
            # Also update the QMdiSubWindow title if hosted
            sw = self.window()
            if isinstance(sw, QtWidgets.QMdiSubWindow):
                sw.setWindowTitle(title)
        except Exception:
            pass

    def shutdown(self):
        # Stop timers first to prevent new actions
        try:
            if hasattr(self, '_aim_timer'):
                self._aim_timer.stop()
        except Exception:
            pass
        # Close collection dialog if open
        try:
            if getattr(self, 'collect_dlg', None):
                self.collect_dlg.accept()
                self.collect_dlg = None
        except Exception:
            pass
        # Stop recording and threads
        self.stop_recording()
        self.stop_stream()
        try:
            if self.det_thr.isRunning():
                self.det_thr.stop()
                if not self.det_thr.wait(600):
                    self.det_thr.terminate()
                    self.det_thr.wait(200)
        except Exception:
            pass

    def closeEvent(self, e: QtGui.QCloseEvent) -> None:
        self.shutdown()
        try:
            self.closed.emit(self.cfg.__dict__)
        except Exception:
            pass
        super().closeEvent(e)

    # ----- collection UI helpers (popup dialog) -----
    def _ensure_collect_dialog(self, title: str):
        if self.collect_dlg is None:
            self.collect_dlg = CollectionDialog(self, title)
            self.collect_dlg.stopClicked.connect(self.stop_collection)
            self.collect_dlg.show()

    def _update_collect_dialog(self, text: str, done: bool = False):
        if self.collect_dlg is None:
            # if collection active but dialog missing, create it
            if self.collect_face or self.collect_pet:
                self._ensure_collect_dialog('Collection')
        if self.collect_dlg is not None:
            try:
                self.collect_dlg.set_status(text)
                if done:
                    # auto-close after short delay
                    QtCore.QTimer.singleShot(600, lambda: (self.collect_dlg and self.collect_dlg.accept(), setattr(self, 'collect_dlg', None)))
            except Exception:
                pass

    # ----- actions
    def do_enroll_face(self, name: str | None = None):
        if not self.last_face_bbox:
            self.lbl_status.setText('No face detected')
            return
        if name is None:
            name=self.ed_name.text().strip() or 'person'
        x,y,w,h=self.last_face_bbox
        pix=self.label.pixmap()
        if pix is None or pix.isNull():
            self.lbl_status.setText('No frame')
            return
        # Best-effort enroll from last frame already handled in on_frame
        self.facedb.load(); self.lbl_status.setText(f'Enrolled face: {name}')

    def do_collect_face(self, name: str | None = None, n: int = 20):
        if name is None:
            name=self.ed_name.text().strip() or 'person'
        # Cancel any ongoing pet collection
        self.collect_pet = None
        self.collect_face={'name':name,'n':int(n or 20),'col':0,'last':0.0}
        self._ensure_collect_dialog('Face Collection')
        self._update_collect_dialog(f'Collecting face: {name} 0/{int(n or 20)}')

    def do_enroll_pet(self, name: str | None = None, species: str | None = None):
        sp= species or self.cmb_species.currentText()
        name= (name or self.ed_name.text().strip()) or 'pet'
        if self.last_pet_bbox:
            x,y,w,h,cls=self.last_pet_bbox
            if cls!=sp:
                self.lbl_status.setText(f'Last bbox is {cls}')
                return
            # capture current displayed frame area
            # For simplicity, rely on on_frame collection path for consistency
            self.pets.load(); self.lbl_status.setText(f'Enrolled {sp}:{name}')
        else:
            self.lbl_status.setText('No pet detected')

    def do_collect_pet(self, name: str | None = None, species: str | None = None, n: int = 40):
        sp= species or self.cmb_species.currentText()
        name= (name or self.ed_name.text().strip()) or 'pet'
        # Cancel any ongoing face collection
        self.collect_face = None
        self.collect_pet={'name':name,'sp':sp,'n':int(n or 40),'col':0,'last':0.0}
        self._ensure_collect_dialog('Pet Collection')
        self._update_collect_dialog(f'Collecting {sp}: {name} 0/{int(n or 40)}')

    def stop_collection(self):
        had_face = self.collect_face is not None
        had_pet  = self.collect_pet is not None
        self.collect_face = None
        self.collect_pet = None
        if self.collect_dlg:
            try:
                self.collect_dlg.accept()
            except Exception:
                pass
            self.collect_dlg = None

    # ----- management helpers
    def _select_and_delete_images(self, root_dir: str, title: str):
        if not os.path.isdir(root_dir):
            QtWidgets.QMessageBox.warning(self, 'Manage', f'Path not found:\n{root_dir}')
            return
        files, _ = QtWidgets.QFileDialog.getOpenFileNames(self, title, root_dir, 'Images (*.jpg *.jpeg *.png)')
        if not files:
            return
        if QtWidgets.QMessageBox.question(self, 'Delete', f'Delete {len(files)} files?') != QtWidgets.QMessageBox.Yes:
            return
        cnt=0
        for fp in files:
            try:
                os.remove(fp); cnt+=1
            except Exception:
                pass
        QtWidgets.QMessageBox.information(self, 'Manage', f'Deleted {cnt} files')

    def do_manage_faces(self):
        name=self.ed_name.text().strip()
        if not name:
            name, ok = QtWidgets.QInputDialog.getText(self, 'Manage Faces', 'Name:')
            if not ok or not name: return
        path=os.path.join('ai','data','faces',name)
        self._select_and_delete_images(path, f'Faces: {name}')
        self.facedb.load()

    def do_manage_pets(self):
        name=self.ed_name.text().strip()
        sp=self.cmb_species.currentText()
        if not name:
            name, ok = QtWidgets.QInputDialog.getText(self, 'Manage Pets', f'{sp} name:')
            if not ok or not name: return
        path=os.path.join('ai','data','pets','dogs' if sp=='dog' else 'cats', name)
        self._select_and_delete_images(path, f'{sp.title()}: {name}')
        self.pets.load()

    @QtCore.Slot(list, list)
    def on_results(self, dets, faces):
        # Called from worker thread via Qt signal (thread-safe)
        self._last_dets = dets
        self._last_faces = faces
        # Update tracker using detections
        now = time.time()
        self.tracker.update(dets, now)
        # Presence update moved to on_frame where dog identity is available

def aim_at_target(self):
    # Called by timer every 300 ms
    if not self.chk_yolo.isChecked():
        return
    dets = self._last_dets
    if not dets:
        return
    # Pick highest conf person/dog/cat
    pri = {'person':3, 'dog':2, 'cat':1}
    dets_sorted = sorted([d for d in dets if d[0] in pri], key=lambda x:(pri[x[0]], x[1]), reverse=True)
    if not dets_sorted:
        return
    _, conf, x, y, w, h = dets_sorted[0]
    if conf < 0.35:
        return
    # Compute error from frame center
    H, W = self.current_size()
    cx = x + w/2.0; cy = y + h/2.0
    ex = (cx - W/2.0) / (W/2.0)   # -1..1
    ey = (cy - H/2.0) / (H/2.0)
    dead = 0.1
    if abs(ex) < dead and abs(ey) < dead:
        return
    # Map to small steps
    dx = int(max(-15, min(15, -ex * 20)))
    dy = int(max(-15, min(15,  ey * 20)))  # invert Y: image down means tilt increases
    try:
        base = f"http://{self.cfg.host.split(':')[0]}"
        url = f"{base}/ptz/step?dx={dx}&dy={dy}"
        # Include token if needed
        if self.cfg.token:
            sep = '&' if '?' in url else '?'
            url = f"{url}{sep}token={self.cfg.token}"
        requests.get(url, timeout=0.4)
    except Exception:
        pass


    # ----- presence + event logging -----
    def _log_event(self, text: str):
        ts = time.strftime('%Y-%m-%d %H:%M:%S')
        line = f"[{ts}] {text}"
        # buffer
        self.event_buffer.append(line)
        if len(self.event_buffer) > 200:
            self.event_buffer = self.event_buffer[-200:]
        # file
        try:
            with open(self.log_path, 'a', encoding='utf-8') as f:
                f.write(line + "\n")
        except Exception:
            pass
        # signal for sidebar
        try:
            self.eventLogged.emit(line)
        except Exception:
            pass

    def _update_presence(self, dets, faces, now: float, dog_names: set[str] | None = None):
        seen_ids = set()
        # Recognized faces (skip 'unknown') with minimum confidence
        for (name, score, x, y, w, h) in faces:
            try:
                if name and name != 'unknown' and float(score) >= 0.2:
                    seen_ids.add(f"face:{name}")
            except Exception:
                continue
        # Dogs (recognized names if provided, else generic)
        try:
            if dog_names:
                for nm in dog_names:
                    if nm and nm != 'unknown':
                        seen_ids.add(f"pet:{nm}")
            else:
                if any(cls == 'dog' for (cls, conf, x, y, w, h) in dets):
                    seen_ids.add('dog')
        except Exception:
            pass

        # Update seen -> present
        for pid in seen_ids:
            p = self.presence.get(pid)
            if not p or not p.get('present'):
                self.presence[pid] = {'present': True, 'enter': now, 'last': now, 'missing_since': None}
                subj = pid.split(':', 1)[1] if (pid.startswith('face:') or pid.startswith('pet:')) else 'dog'
                self._log_event(f"{subj} detected")
            else:
                p['last'] = now
                p['missing_since'] = None

        # Handle not-seen -> maybe exit (with hysteresis)
        for pid, p in list(self.presence.items()):
            if p.get('present') and pid not in seen_ids:
                if p.get('missing_since') is None:
                    p['missing_since'] = now
                else:
                    if (now - p['missing_since']) >= self.grace_sec:
                        # Mark exit
                        duration = max(0.0, (p.get('last') or now) - (p.get('enter') or now))
                        subj = pid.split(':', 1)[1] if (pid.startswith('face:') or pid.startswith('pet:')) else 'dog'
                        self._log_event(f"{subj} exited after {duration:.1f}s")
                        p['present'] = False
                        p['enter'] = None
                        p['last'] = None
                        p['missing_since'] = None


class DetectionThread(QtCore.QThread):
    resultsReady = QtCore.Signal(list, list)  # dets, faces
    def __init__(self, widget: CameraWidget, interval_ms: int = 200):
        super().__init__(widget)
        self.w = widget
        self.interval = interval_ms
        self._stop = threading.Event()
        self._skip_cycles = 0
        self.max_skip_cycles = 1  # skip this many cycles if tracks are active

    def stop(self):
        self._stop.set()

    def run(self):
        last = 0
        while not self._stop.is_set():
            now = time.time()
            if (now - last)*1000.0 < self.interval:
                self.msleep(10)
                continue
            last = now
            # fetch latest frame
            with self.w._frame_lock:
                frame = None if self.w._latest_frame is None else self.w._latest_frame.copy()
            if frame is None:
                self.msleep(10)
                continue
            dets=[]; faces=[]
            try:
                # Skip YOLO on some cycles if we already have active tracks (object permanence)
                # But when collecting pet samples, force YOLO every cycle for more samples
                do_yolo = True
                if not self.w.collect_pet:
                    if self.w.tracker.has_active():
                        if self._skip_cycles < self.max_skip_cycles:
                            do_yolo = False; self._skip_cycles += 1
                        else:
                            self._skip_cycles = 0
                else:
                    self._skip_cycles = 0
                if do_yolo and self.w.chk_yolo.isChecked() and self.w.yolo.available():
                    dets = self.w.yolo.detect(frame)
                # Face detection/recognition
                if self.w.chk_face.isChecked():
                    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
                    for (x,y,w,h) in self.w.facedb.detect_faces(gray):
                        name,score=self.w.facedb.recognize_roi(gray[y:y+h,x:x+w])
                        faces.append((name,score,x,y,w,h))
            except Exception as e:
                # Avoid crashing the thread on sporadic errors
                pass
            self.resultsReady.emit(dets, faces)





class SimpleTracker:
    """Very simple IOU-based tracker with TTL for object permanence.
    Tracks: list of dicts with bbox, cls, last_ts.
    """
    def __init__(self, ttl: float = 1.0, iou_thresh: float = 0.3):
        self.ttl = ttl
        self.iou_thresh = iou_thresh
        self.tracks = []

    def update(self, dets, now: float):
        # Match new detections to existing tracks via IOU
        def iou(a,b):
            ax,ay,aw,ah=a; bx,by,bw,bh=b
            x1=max(ax,bx); y1=max(ay,by); x2=min(ax+aw,bx+bw); y2=min(ay+ah,by+bh)
            inter=max(0,x2-x1)*max(0,y2-y1)
            ua=aw*ah + bw*bh - inter
            return inter/ua if ua>0 else 0.0
        # decay tracks
        self.tracks = [t for t in self.tracks if now - t['last_ts'] <= self.ttl]
        for cls,conf,x,y,w,h in dets:
            bb=(x,y,w,h)
            best=None; best_iou=0
            for t in self.tracks:
                ii=iou(bb,t['bbox'])
                if ii>best_iou: best_iou=ii; best=t
            if best and best_iou>=self.iou_thresh and best['cls']==cls:
                best['bbox']=bb; best['last_ts']=now
            else:
                self.tracks.append({'cls':cls,'bbox':bb,'last_ts':now})

    def has_active(self) -> bool:
        return len(self.tracks)>0

    def primary_target(self, prefer: str = 'cat'):
        # choose preferred class if present, otherwise largest box
        if not self.tracks:
            return None
        cats=[t for t in self.tracks if t['cls']==prefer]
        chosen = cats or self.tracks
        # Largest area
        t=max(chosen, key=lambda t: t['bbox'][2]*t['bbox'][3])
        x,y,w,h = t['bbox']
        return (x,y,w,h,t['cls'], t['last_ts'])


class AddCameraDialog(QtWidgets.QDialog):
    def __init__(self, parent=None, initial: Optional[CameraConfig]=None, title: str='Add Camera'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(True)
        form = QtWidgets.QFormLayout(self)
        self.ed_name = QtWidgets.QLineEdit(initial.name if initial else 'Camera')
        self.ed_host = QtWidgets.QLineEdit(initial.host if initial else '192.168.1.100')
        self.ed_user = QtWidgets.QLineEdit(initial.user or '' if initial else '')
        self.ed_pass = QtWidgets.QLineEdit(initial.password or '' if initial else '')
        self.ed_pass.setEchoMode(QtWidgets.QLineEdit.Password)
        self.ed_token = QtWidgets.QLineEdit(initial.token or '' if initial else '')
        form.addRow('Name', self.ed_name)
        form.addRow('Host (ip[:port])', self.ed_host)
        form.addRow('User', self.ed_user)
        form.addRow('Password', self.ed_pass)
        form.addRow('Token (Base64 user:pass)', self.ed_token)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept)
        btns.rejected.connect(self.reject)
        form.addRow(btns)

    def get_config(self) -> Optional[CameraConfig]:
        if self.exec() == QtWidgets.QDialog.Accepted:
            return CameraConfig(
                name=self.ed_name.text().strip() or 'Camera',
                host=self.ed_host.text().strip(),
                user=(self.ed_user.text().strip() or None),
                password=self.ed_pass.text(),
                token=(self.ed_token.text().strip() or None),
            )
        return None


class MainWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle('ESP32-CAM MDI')
        self.resize(1200, 800)
        self.mdi = QtWidgets.QMdiArea()
        self.setCentralWidget(self.mdi)
        # Events sidebar (shows events for active camera)
        self.eventsDock = QtWidgets.QDockWidget('Events', self)
        self.eventsView = QtWidgets.QListWidget()
        self.eventsDock.setWidget(self.eventsView)
        self.addDockWidget(QtCore.Qt.RightDockWidgetArea, self.eventsDock)
        self._active_cam_ref = None
        try:
            self.mdi.subWindowActivated.connect(self.on_sub_activated)
        except Exception:
            pass

        # Menu bar
        mb = self.menuBar()
        menu_file = mb.addMenu('&File')
        menu_tools = mb.addMenu('&Tools')
        act_dl_yolo = menu_tools.addAction('Download YOLO Model')
        act_scan_net = menu_tools.addAction('Scan For Cameras')
        act_manage_cams = menu_tools.addAction('Manage Cameras')
        act_dl_yolo.triggered.connect(self.download_yolo_model)
        act_scan_net.triggered.connect(self.scan_network)
        act_manage_cams.triggered.connect(self.manage_cameras)
        # Import and Cull submenus
        menu_import = menu_tools.addMenu('Import')
        act_imp_faces = menu_import.addAction('Import Faces...')
        act_imp_pets = menu_import.addAction('Import Pets...')
        act_imp_faces.triggered.connect(self.import_faces)
        act_imp_pets.triggered.connect(self.import_pets)
        menu_cull = menu_tools.addMenu('Cull Similar')
        act_cull_faces = menu_cull.addAction('Cull Faces...')
        act_cull_pets = menu_cull.addAction('Cull Pets...')
        act_cull_faces.triggered.connect(self.cull_faces)
        act_cull_pets.triggered.connect(self.cull_pets)
        # Settings dialog
        act_settings = menu_tools.addAction('Settings...')
        act_settings.triggered.connect(self.open_settings)
        # File set actions
        self.act_new = menu_file.addAction('New Set')
        self.act_load = menu_file.addAction('Load Set...')
        self.act_save = menu_file.addAction('Save Set')
        self.act_save_as = menu_file.addAction('Save Set As...')
        menu_file.addSeparator()
        self.act_exit = menu_file.addAction('Exit')

        tb = QtWidgets.QToolBar('Main')
        tb.setIconSize(QtCore.QSize(16,16))
        self.addToolBar(tb)

        act_add = tb.addAction('Add Camera')
        act_tile = tb.addAction('Tile')
        act_cascade = tb.addAction('Cascade')
        tb.addSeparator()
        act_edit = tb.addAction('Edit Camera')
        tb.addSeparator()
        act_rec_all = tb.addAction('Start Rec All')
        act_stop_all = tb.addAction('Stop Rec All')
        tb.addSeparator()
        # Global enrollment/management controls
        tb.addWidget(QtWidgets.QLabel('Name:'))
        self.ed_name_global = QtWidgets.QLineEdit(); self.ed_name_global.setMaximumWidth(160)
        tb.addWidget(self.ed_name_global)
        tb.addWidget(QtWidgets.QLabel('Species:'))
        self.cmb_species_global = QtWidgets.QComboBox(); self.cmb_species_global.addItems(['dog','cat'])
        tb.addWidget(self.cmb_species_global)
        # Enrollment actions moved to Tools menu (below)
        act_manage_faces_g = tb.addAction('Manage Faces')
        act_manage_pets_g = tb.addAction('Manage Pets')

        act_add.triggered.connect(self.add_camera)
        act_tile.triggered.connect(self.mdi.tileSubWindows)
        act_cascade.triggered.connect(self.mdi.cascadeSubWindows)
        act_rec_all.triggered.connect(self.start_rec_all)
        act_stop_all.triggered.connect(self.stop_rec_all)
        act_edit.triggered.connect(self.edit_camera)
        # File set wiring
        self.act_new.triggered.connect(self.new_set)
        self.act_load.triggered.connect(self.load_set_dialog)
        self.act_save.triggered.connect(self.save_set)
        self.act_save_as.triggered.connect(self.save_set_as)
        self.act_exit.triggered.connect(self.close)

        act_manage_faces_g.triggered.connect(self.manage_faces_active)
        act_manage_pets_g.triggered.connect(self.manage_pets_active)

        os.makedirs(os.path.join('ai','recordings'), exist_ok=True)

        # Load saved cameras
        self._prefs_path = os.path.join('ai','cameras.json')
        self._cam_defs = []
        self.load_cameras()
        seen_hosts=set()
        for d in self._cam_defs:
            try:
                h=(d.get('host') or '').strip()
            except Exception:
                h=''
            if h and h in seen_hosts:
                continue
            seen_hosts.add(h)
            self.add_camera_from_cfg(CameraConfig(**d))

        # Add enrollment to Tools menu
        enroll_menu = menu_tools.addMenu('Enrollment')
        act_enroll_face = enroll_menu.addAction('Enroll Face')
        act_collect_face = enroll_menu.addAction('Collect Face (20)')
        act_enroll_pet = enroll_menu.addAction('Enroll Pet')
        act_collect_pet = enroll_menu.addAction('Collect Pet (40)')
        act_enroll_face.triggered.connect(self.enroll_face_active)
        act_collect_face.triggered.connect(self.collect_face_active)
        act_enroll_pet.triggered.connect(self.enroll_pet_active)
        act_collect_pet.triggered.connect(self.collect_pet_active)

    @QtCore.Slot(QtWidgets.QMdiSubWindow)
    def on_sub_activated(self, sub):
        # Rebind events view to the active camera widget
        try:
            if self._active_cam_ref:
                try:
                    self._active_cam_ref.eventLogged.disconnect(self._on_event_line)
                except Exception:
                    pass
        except Exception:
            pass
        w = sub.widget() if sub else None
        if isinstance(w, CameraWidget):
            self._active_cam_ref = w
            try:
                w.eventLogged.connect(self._on_event_line)
            except Exception:
                pass
            # Refresh list from widget buffer
            self.eventsView.clear()
            try:
                for line in getattr(w, 'event_buffer', []):
                    self.eventsView.addItem(line)
                if self.eventsView.count() > 0:
                    self.eventsView.scrollToBottom()
            except Exception:
                pass
        else:
            self._active_cam_ref = None
            self.eventsView.clear()

    @QtCore.Slot(str)
    def _on_event_line(self, line: str):
        self.eventsView.addItem(line)
        self.eventsView.scrollToBottom()

    # -------- Tools --------
    def download_yolo_model(self):
        """Download YOLO ONNX to ai/models/yolo.onnx with progress."""
        urls = [
            'https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.onnx',
            'https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n.onnx',
            'https://github.com/ultralytics/ultralytics/releases/download/v8.0.0/yolov8n.onnx',
        ]
        dst = os.path.join('ai','models','yolo.onnx')
        os.makedirs(os.path.dirname(dst), exist_ok=True)
        for url in urls:
            try:
                dlg = QtWidgets.QProgressDialog('Downloading YOLO model...', 'Cancel', 0, 100, self)
                dlg.setWindowModality(QtCore.Qt.WindowModality.ApplicationModal)
                dlg.show()
                with requests.get(url, stream=True, timeout=120, allow_redirects=True) as r:
                    r.raise_for_status()
                    total = int(r.headers.get('content-length', '0'))
                    done = 0
                    with open(dst, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=1<<15):
                            if dlg.wasCanceled():
                                raise RuntimeError('Canceled')
                            if not chunk:
                                continue
                            f.write(chunk)
                            done += len(chunk)
                            if total:
                                dlg.setValue(int(done*100/total))
                                QtWidgets.QApplication.processEvents()
                dlg.setValue(100)
                QtWidgets.QMessageBox.information(self, 'Download', f'Saved model to {dst}')
                return
            except Exception as e:
                # try next URL
                pass
        QtWidgets.QMessageBox.warning(self, 'Download', 'Failed to download YOLO model from known mirrors')

    def add_camera(self):
        dlg = AddCameraDialog(self)
        cfg = dlg.get_config()
        if not cfg:
            return
        # save to prefs (de-dup by host)
        host = (cfg.host or '').strip()
        updated = False
        for i, d in enumerate(list(self._cam_defs)):
            if (d.get('host') or '').strip() == host:
                self._cam_defs[i] = cfg.__dict__
                updated = True
                break
        if not updated:
            self._cam_defs.append(cfg.__dict__)
        self.save_cameras()
        self.add_camera_from_cfg(cfg)

    def add_camera_from_cfg(self, cfg: CameraConfig):
        w = CameraWidget(cfg)
        # listen for close to update prefs
        try:
            w.closed.connect(self.on_camera_closed)
        except Exception:
            pass
        sub = QtWidgets.QMdiSubWindow()
        sub.setWidget(w)
        sub.setAttribute(QtCore.Qt.WidgetAttribute.WA_DeleteOnClose)
        sub.setWindowTitle(cfg.name)
        self.mdi.addSubWindow(sub)
        sub.resize(500, 420)
        sub.show()
        w.start_stream()

    def edit_camera(self):
        w = self._active_camera()
        if not w:
            QtWidgets.QMessageBox.information(self, 'Edit Camera', 'Select a camera window to edit its settings.')
            return
        old_cfg = w.cfg
        dlg = AddCameraDialog(self, initial=old_cfg, title='Edit Camera')
        new_cfg = dlg.get_config()
        if not new_cfg:
            return
        # Update prefs: replace matching entry on name+host (fallback: first matching host)
        try:
            replaced = False
            for i, d in enumerate(self._cam_defs):
                if (d.get('name') == old_cfg.name and d.get('host') == old_cfg.host) or (d.get('host') == old_cfg.host):
                    self._cam_defs[i] = new_cfg.__dict__
                    replaced = True
                    break
            if not replaced:
                self._cam_defs.append(new_cfg.__dict__)
            self.save_cameras()
        except Exception:
            pass
        # Apply to widget: stop worker, swap config, start again
        try:
            w.apply_config(new_cfg)
        except Exception:
            # Fallback: close and re-add
            sub = self.mdi.activeSubWindow()
            if sub:
                sub.close()
            self.add_camera_from_cfg(new_cfg)

    def start_rec_all(self):
        for sub in self.mdi.subWindowList():
            w = sub.widget()
            if isinstance(w, CameraWidget):
                w.start_recording()

    def stop_rec_all(self):
        for sub in self.mdi.subWindowList():
            w = sub.widget()
            if isinstance(w, CameraWidget):
                w.stop_recording()

    # ----- Prefs (ensure defined as class methods)
    def load_cameras(self):
        try:
            import json
            self._cam_defs = []
            if hasattr(self, '_prefs_path') and os.path.exists(self._prefs_path):
                with open(self._prefs_path,'r',encoding='utf-8') as f:
                    defs = json.load(f)
                    # de-duplicate by host (prefer last occurrence)
                    uniq = {}
                    for d in defs if isinstance(defs, list) else []:
                        h = (d.get('host') or '').strip()
                        if not h: continue
                        uniq[h] = d
                    self._cam_defs = list(uniq.values())
        except Exception:
            self._cam_defs = []

    def save_cameras(self):
        try:
            import json
            if not hasattr(self, '_prefs_path'):
                self._prefs_path = os.path.join('ai','cameras.json')
            os.makedirs(os.path.dirname(self._prefs_path), exist_ok=True)
            # sanitize and de-duplicate by host before save
            uniq = {}
            for d in self._cam_defs:
                if not isinstance(d, dict):
                    continue
                h = (d.get('host') or '').strip()
                if not h: continue
                uniq[h] = d
            with open(self._prefs_path,'w',encoding='utf-8') as f:
                json.dump(list(uniq.values()), f, indent=2)
        except Exception:
            pass

    @QtCore.Slot(object)
    def on_camera_closed(self, cfg: CameraConfig):
        try:
            # remove by host (unique key)
            self._cam_defs = [d for d in self._cam_defs if (d.get('host') or '').strip() != (cfg.host or '').strip()]
            self.save_cameras()
        except Exception:
            pass

    def scan_network(self):
        # Discover ESP32-CAMs by probing /api/advertise (no auth required)
        try:
            import socket, ipaddress, concurrent.futures
            # Determine local IPv4
            local_ip = None
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.settimeout(0.2)
            try:
                s.connect(('8.8.8.8', 80))
                local_ip = s.getsockname()[0]
            except Exception:
                pass
            finally:
                try: s.close()
                except Exception: pass
            if not local_ip:
                QtWidgets.QMessageBox.warning(self, 'Scan', 'Could not determine local IP')
                return
            net = ipaddress.ip_network(local_ip + '/24', strict=False)
            targets = [str(ip) for ip in net.hosts()]
            found = []
            def probe(ip):
                try:
                    r = requests.get(f'http://{ip}/api/advertise', timeout=0.3)
                    if r.status_code == 200:
                        j = r.json()
                        name = j.get('name') or 'ESP32-CAM'
                        host = ip
                        return {'name': name, 'host': host}
                except Exception:
                    return None
                return None
            with concurrent.futures.ThreadPoolExecutor(max_workers=32) as ex:
                for res in ex.map(probe, targets):
                    if res:
                        found.append(res)
            # merge into defs
            existing = { (d.get('host') or '').strip() for d in self._cam_defs }
            added = 0
            for d in found:
                h = (d['host'] or '').strip()
                if h and h not in existing:
                    self._cam_defs.append(d)
                    try:
                        self.add_camera_from_cfg(CameraConfig(**d))
                    except Exception:
                        pass
                    added += 1
            self.save_cameras()
            QtWidgets.QMessageBox.information(self, 'Scan', f'Found {len(found)} device(s), added {added} new')
        except Exception as ex:
            QtWidgets.QMessageBox.warning(self, 'Scan', f'Failed to scan: {ex}')

    def manage_cameras(self):
        # Simple dialog to view/remove saved cameras
        class _ManageDlg(QtWidgets.QDialog):
            def __init__(self, items, parent=None):
                super().__init__(parent)
                self.setWindowTitle('Manage Cameras')
                v = QtWidgets.QVBoxLayout(self)
                self.list = QtWidgets.QListWidget(); self.list.setSelectionMode(QtWidgets.QAbstractItemView.ExtendedSelection)
                for it in items:
                    name = it.get('name') or 'Camera'
                    host = it.get('host') or ''
                    lw = QtWidgets.QListWidgetItem(f"{name}  [{host}]")
                    lw.setData(QtCore.Qt.UserRole, host)
                    self.list.addItem(lw)
                v.addWidget(self.list)
                btns = QtWidgets.QDialogButtonBox()
                self.btn_rm = btns.addButton('Remove Selected', QtWidgets.QDialogButtonBox.ActionRole)
                btn_close = btns.addButton(QtWidgets.QDialogButtonBox.Close)
                v.addWidget(btns)
                self.btn_rm.clicked.connect(self._on_remove)
                btn_close.clicked.connect(self.accept)
                self.removed = []
            def _on_remove(self):
                sel = self.list.selectedItems()
                if not sel:
                    return
                if QtWidgets.QMessageBox.question(self, 'Remove', f'Remove {len(sel)} camera(s)?') != QtWidgets.QMessageBox.Yes:
                    return
                for it in sel:
                    host = it.data(QtCore.Qt.UserRole)
                    self.removed.append(host)
                    row = self.list.row(it)
                    self.list.takeItem(row)
        # Build and run dialog
        dlg = _ManageDlg(self._cam_defs, self)
        dlg.exec()
        if not getattr(dlg, 'removed', None):
            return
        removed_hosts = set((h or '').strip() for h in dlg.removed)
        if not removed_hosts:
            return
        # Update saved list
        self._cam_defs = [d for d in self._cam_defs if (d.get('host') or '').strip() not in removed_hosts]
        self.save_cameras()
        # Close any open windows for removed hosts
        for sub in list(self.mdi.subWindowList()):
            w = sub.widget()
            try:
                if hasattr(w, 'cfg') and (w.cfg.host or '').strip() in removed_hosts:
                    sub.close()
            except Exception:
                continue

    # ----- Active camera helpers
    def _active_camera(self) -> CameraWidget | None:
        sub = self.mdi.activeSubWindow()
        if not sub: return None
        w = sub.widget()
        return w if isinstance(w, CameraWidget) else None

    def enroll_face_active(self):
        w=self._active_camera();
        if not w: return
        name=self.ed_name_global.text().strip()
        # Prompt to choose/confirm a specific person
        if not name or name.lower()=='person':
            # Offer existing names as hint
            faces_dir=os.path.join('ai','data','faces')
            names=[d for d in os.listdir(faces_dir) if os.path.isdir(os.path.join(faces_dir,d))] if os.path.isdir(faces_dir) else []
            name, ok = QtWidgets.QInputDialog.getText(self, 'Enroll Face', f'Enter person name to enroll{" (existing: "+", ".join(names)+")" if names else ""}:')
            if not ok or not name: return
        w.do_enroll_face(name)

    def collect_face_active(self):
        w=self._active_camera();
        if not w: return
        name=self.ed_name_global.text().strip()
        if not name or name.lower()=='person':
            faces_dir=os.path.join('ai','data','faces')
            names=[d for d in os.listdir(faces_dir) if os.path.isdir(os.path.join(faces_dir,d))] if os.path.isdir(faces_dir) else []
            name, ok = QtWidgets.QInputDialog.getText(self, 'Collect Face Samples', f'Collect samples for which person?{" (existing: "+", ".join(names)+")" if names else ""}:')
            if not ok or not name: return
        w.do_collect_face(name, 20)

    def enroll_pet_active(self):
        w=self._active_camera();
        if not w: return
        name=self.ed_name_global.text().strip() or None
        sp=self.cmb_species_global.currentText()
        # Policy: cats are treated generically for prevention; skip cat enrollment
        if sp=='cat':
            QtWidgets.QMessageBox.information(self, 'Pet Enrollment', 'Cat prevention mode: specific cat enrollment is disabled. Any cat will be treated generically.')
            return
        # For dogs, ensure a named identity
        if not name:
            dogs_dir=os.path.join('ai','data','pets','dogs')
            names=[d for d in os.listdir(dogs_dir) if os.path.isdir(os.path.join(dogs_dir,d))] if os.path.isdir(dogs_dir) else []
            name, ok = QtWidgets.QInputDialog.getText(self, 'Enroll Dog', f'Enter dog name to enroll{" (existing: "+", ".join(names)+")" if names else ""}:')
            if not ok or not name: return
        w.do_enroll_pet(name, sp)

    def collect_pet_active(self):
        w=self._active_camera();
        if not w: return
        name=self.ed_name_global.text().strip() or None
        sp=self.cmb_species_global.currentText()
        if sp=='cat':
            QtWidgets.QMessageBox.information(self, 'Collect Pet', 'Cat prevention mode: specific cat datasets are not required. Any detected cat triggers prevention.')
            return
        if not name:
            dogs_dir=os.path.join('ai','data','pets','dogs')
            names=[d for d in os.listdir(dogs_dir) if os.path.isdir(os.path.join(dogs_dir,d))] if os.path.isdir(dogs_dir) else []
            name, ok = QtWidgets.QInputDialog.getText(self, 'Collect Dog Samples', f'Collect samples for which dog?{" (existing: "+", ".join(names)+")" if names else ""}:')
            if not ok or not name: return
        w.do_collect_pet(name, sp, 40)

    def manage_faces_active(self):
        w=self._active_camera();
        if not w: return
        name=self.ed_name_global.text().strip()
        if not name:
            name, ok = QtWidgets.QInputDialog.getText(self, 'Manage Faces', 'Name:')
            if not ok or not name: return
        path=os.path.join('ai','data','faces',name)
        # Open built-in gallery dialog
        dlg = GalleryDialog(path, f'Faces: {name}', self)
        dlg.exec()
        w.facedb.load()

    def manage_pets_active(self):
        w=self._active_camera();
        if not w: return
        sp=self.cmb_species_global.currentText()
        name=self.ed_name_global.text().strip()
        if not name:
            name, ok = QtWidgets.QInputDialog.getText(self, 'Manage Pets', f'{sp} name:')
            if not ok or not name: return
        path=os.path.join('ai','data','pets','dogs' if sp=='dog' else 'cats', name)
        dlg = GalleryDialog(path, f'{sp.title()}: {name}', self)
        dlg.exec()
        w.pets.load()

    # -------- Import helpers (menu actions) --------
    def import_faces(self):
        """Import a directory of images into faces/<name>."""
        name, ok = QtWidgets.QInputDialog.getText(self, 'Import Faces', 'Person name:')
        if not ok or not name:
            return
        src_dir = QtWidgets.QFileDialog.getExistingDirectory(self, 'Select Source Directory')
        if not src_dir:
            return
        safe = ''.join(c for c in name if c.isalnum() or c in ('_','-'))
        dest = os.path.join('ai','data','faces', safe)
        os.makedirs(dest, exist_ok=True)
        count = tools.ingest_images(src_dir, dest, (160,160), gray=True)
        QtWidgets.QMessageBox.information(self, 'Import Faces', f'Imported {count} images to {dest}')

    def import_pets(self):
        """Import a directory of images into pets/<species>/<name>."""
        species, ok = QtWidgets.QInputDialog.getItem(self, 'Import Pets', 'Species:', ['dog','cat'], 0, False)
        if not ok:
            return
        name, ok = QtWidgets.QInputDialog.getText(self, 'Import Pets', f'{species} name:')
        if not ok or not name:
            return
        src_dir = QtWidgets.QFileDialog.getExistingDirectory(self, 'Select Source Directory')
        if not src_dir:
            return
        sp_dir = 'dogs' if species=='dog' else 'cats'
        safe = ''.join(c for c in name if c.isalnum() or c in ('_','-'))
        dest = os.path.join('ai','data','pets', sp_dir, safe)
        os.makedirs(dest, exist_ok=True)
        count = tools.ingest_images(src_dir, dest, (320,320), gray=True)
        QtWidgets.QMessageBox.information(self, 'Import Pets', f'Imported {count} images to {dest}')

    # -------- Cull helpers (menu actions) --------
    def cull_faces(self):
        name, ok = QtWidgets.QInputDialog.getText(self, 'Cull Faces', 'Person name:')
        if not ok or not name:
            return
        target = os.path.join('ai','data','faces', name)
        files, remove = tools.find_similar_in_dir(target)
        dlg = CullDialog(target, files, remove, f'Cull Faces: {name}', self)
        dlg.exec()

    def cull_pets(self):
        species, ok = QtWidgets.QInputDialog.getItem(self, 'Cull Pets', 'Species:', ['dog','cat'], 0, False)
        if not ok:
            return
        name, ok = QtWidgets.QInputDialog.getText(self, 'Cull Pets', f'{species} name:')
        if not ok or not name:
            return
        target = os.path.join('ai','data','pets', 'dogs' if species=='dog' else 'cats', name)
        files, remove = tools.find_similar_in_dir(target)
        dlg = CullDialog(target, files, remove, f'Cull Pets: {species} {name}', self)
        dlg.exec()

    # -------- Camera Set management --------
    def closeEvent(self, e: QtGui.QCloseEvent) -> None:
        # Proactively stop all camera workers before app quits
        try:
            for sub in list(self.mdi.subWindowList()):
                w = sub.widget()
                if hasattr(w, 'shutdown'):
                    try:
                        w.shutdown()
                    except Exception:
                        pass
                try:
                    sub.close()
                except Exception:
                    pass
        except Exception:
            pass
        super().closeEvent(e)
    def new_set(self):
        """Close all camera windows and clear current set."""
        for sub in list(self.mdi.subWindowList()):
            sub.close()
        self._cam_defs = []
        self.save_cameras()

    def load_set_dialog(self):
        """Load camera definitions from JSON and recreate subwindows."""
        path, _ = QtWidgets.QFileDialog.getOpenFileName(self, 'Load Camera Set', 'ai', 'JSON (*.json)')
        if not path:
            return
        try:
            import json
            with open(path,'r',encoding='utf-8') as f:
                defs = json.load(f)
            for sub in list(self.mdi.subWindowList()):
                sub.close()
            self._cam_defs = defs if isinstance(defs, list) else []
            self._prefs_path = path
            self.save_cameras()
            for d in self._cam_defs:
                self.add_camera_from_cfg(CameraConfig(**d))
        except Exception as ex:
            QtWidgets.QMessageBox.warning(self, 'Load', f'Failed to load set:\n{ex}')

    def save_set(self):
        """Save to current prefs path."""
        self.save_cameras()
        QtWidgets.QMessageBox.information(self, 'Save', f'Saved: {self._prefs_path}')

    def save_set_as(self):
        """Save camera set to a new JSON path."""
        path, _ = QtWidgets.QFileDialog.getSaveFileName(self, 'Save Camera Set As', 'ai/cameras.json', 'JSON (*.json)')
        if not path:
            return
        self._prefs_path = path
        self.save_cameras()
        QtWidgets.QMessageBox.information(self, 'Save As', f'Saved: {self._prefs_path}')
    # -------- Settings dialog --------
    def open_settings(self):
        dlg = SettingsDialog(self)
        # prefill from current state
        dlg.s_det_interval.setValue(getattr(self, '_det_interval', 200))
        dlg.s_hash_size.setValue(getattr(self, '_hash_size', 8))
        dlg.s_hamming.setValue(getattr(self, '_hamming', 4))
        dlg.s_ptz_interval.setValue(getattr(self, '_ptz_interval', 300))
        dlg.s_deadzone.setValue(getattr(self, '_deadzone_pct', 5))
        if dlg.exec() == QtWidgets.QDialog.Accepted:
            self._det_interval = dlg.s_det_interval.value()
            self._hash_size = dlg.s_hash_size.value()
            self._hamming = dlg.s_hamming.value()
            self._ptz_interval = dlg.s_ptz_interval.value()
            self._deadzone_pct = dlg.s_deadzone.value()
            # apply to open cameras
            for sub in self.mdi.subWindowList():
                w=sub.widget()
                if isinstance(w, CameraWidget):
                    w.det_thr.interval = self._det_interval
                    w.det_thr.max_skip_cycles = 1
                    w._aim_timer.setInterval(self._ptz_interval)
            QtWidgets.QMessageBox.information(self, 'Settings', 'Settings applied')


class SettingsDialog(QtWidgets.QDialog):
    """App settings: detector interval, aHash grid size + Hamming, PTZ aim timing, deadzone."""
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle('Settings')
        self.resize(420, 240)
        form = QtWidgets.QFormLayout(self)
        self.s_det_interval = QtWidgets.QSpinBox(); self.s_det_interval.setRange(50, 2000); self.s_det_interval.setSingleStep(50); self.s_det_interval.setSuffix(' ms')
        self.s_hash_size = QtWidgets.QSpinBox(); self.s_hash_size.setRange(4, 16)
        self.s_hamming = QtWidgets.QSpinBox(); self.s_hamming.setRange(0, 32)
        self.s_ptz_interval = QtWidgets.QSpinBox(); self.s_ptz_interval.setRange(100, 2000); self.s_ptz_interval.setSingleStep(50); self.s_ptz_interval.setSuffix(' ms')
        self.s_deadzone = QtWidgets.QSpinBox(); self.s_deadzone.setRange(2, 20); self.s_deadzone.setSuffix(' %')
        form.addRow('Detector interval', self.s_det_interval)
        form.addRow('aHash grid size', self.s_hash_size)
        form.addRow('Hamming threshold', self.s_hamming)
        form.addRow('PTZ aim interval', self.s_ptz_interval)
        form.addRow('PTZ deadzone', self.s_deadzone)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept); btns.rejected.connect(self.reject)
        form.addRow(btns)

class CullDialog(QtWidgets.QDialog):
    """Preview duplicates with highlight before deletion.
    Includes a tolerance slider to adjust the near-duplicate matching threshold.
    Lower values are stricter; higher tolerate more difference.
    """
    def __init__(self, dir_path: str, files: list[str], remove: set[int], title: str, parent=None):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.resize(1000, 700)
        self.dir_path = dir_path
        self.files = files or []
        self.remove = set(remove or set())
        # Matching parameters (aHash on 8x8 grid → Hamming distance 0..64).
        # Practical duplicate tolerance range ~0..16; start at 4 by default.
        self.hash_size = 8
        self.thresh = 4
        v = QtWidgets.QVBoxLayout(self)
        self.view = QtWidgets.QListWidget()
        self.view.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.view.setIconSize(QtCore.QSize(160, 120))
        self.view.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.view.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        v.addWidget(self.view, 1)
        # Controls
        h = QtWidgets.QHBoxLayout()
        # Tolerance slider block
        tol_box = QtWidgets.QHBoxLayout()
        self.lbl_tol = QtWidgets.QLabel('Tolerance:')
        self.sld_tol = QtWidgets.QSlider(QtCore.Qt.Orientation.Horizontal)
        self.sld_tol.setMinimum(0)
        self.sld_tol.setMaximum(16)
        self.sld_tol.setValue(self.thresh)
        self.sld_tol.setTickInterval(1)
        self.sld_tol.setSingleStep(1)
        self.lbl_tol_val = QtWidgets.QLabel(str(self.thresh))
        tol_box.addWidget(self.lbl_tol)
        tol_box.addWidget(self.sld_tol)
        tol_box.addWidget(self.lbl_tol_val)
        tol_box_w = QtWidgets.QWidget(); tol_box_w.setLayout(tol_box)
        h.addWidget(tol_box_w, 2)
        # Info + actions
        self.lbl_info = QtWidgets.QLabel()
        h.addWidget(self.lbl_info, 1)
        self.btn_confirm = QtWidgets.QPushButton('Confirm Delete')
        self.btn_cancel = QtWidgets.QPushButton('Cancel')
        h.addWidget(self.btn_confirm)
        h.addWidget(self.btn_cancel)
        v.addLayout(h)
        self.btn_confirm.clicked.connect(self.on_confirm)
        self.btn_cancel.clicked.connect(self.reject)
        self.sld_tol.valueChanged.connect(self.on_tol_changed)
        self.populate()

    def populate(self):
        self.view.clear()
        removed = 0
        for idx, fp in enumerate(self.files):
            if not os.path.exists(fp):
                continue
            item = QtWidgets.QListWidgetItem(os.path.basename(fp))
            pix = QtGui.QPixmap(fp)
            if not pix.isNull():
                item.setIcon(QtGui.QIcon(pix.scaled(160,120, QtCore.Qt.KeepAspectRatio, QtCore.Qt.SmoothTransformation)))
            if idx in self.remove:
                item.setBackground(QtGui.QBrush(QtGui.QColor(255, 220, 220)))
            item.setData(QtCore.Qt.ItemDataRole.UserRole, (idx, fp))
            self.view.addItem(item)
        self.lbl_info.setText(f"Marked for removal: {len(self.remove)} / {len(self.files)}")

    def on_confirm(self):
        # Optionally allow manual deselect: unmark by selecting those to keep
        for it in self.view.selectedItems():
            idx, _ = it.data(QtCore.Qt.ItemDataRole.UserRole)
            if idx in self.remove:
                self.remove.remove(idx)
        cnt=0
        for idx in sorted(self.remove):
            fp = self.files[idx]
            try:
                os.remove(fp); cnt+=1
            except Exception:
                pass
        QtWidgets.QMessageBox.information(self, 'Cull', f'Deleted {cnt} files')
        self.accept()

    def on_tol_changed(self, val: int):
        """Recompute suggested deletions when the tolerance slider changes."""
        self.thresh = int(val)
        self.lbl_tol_val.setText(str(self.thresh))
        # Recompute using tools.find_similar_in_dir with new threshold
        files, remove = tools.find_similar_in_dir(self.dir_path, hash_size=self.hash_size, hamming_thresh=self.thresh)
        if files:
            self.files = files
            self.remove = remove
            self.populate()


def main():
    app = QtWidgets.QApplication(sys.argv)
    mw = MainWindow()
    mw.show()
    sys.exit(app.exec())


if __name__ == '__main__':
    main()





## `refactor/ai_viewer/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\__init__.py
**Size**: 20 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: bd2bb7253368725b542a933b07fae240fa726465b550779548acb0c2c6679956
``````python# ai_viewer package
## `refactor/ai_viewer/ai/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\__init__.py
**Size**: 10 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 527c668d5ae4f9e34e46c8ad88ce3c5b4a41eb27ec684ace6a4f11621fc42edf
``````python# package
## `refactor/ai_viewer/ai/detectors/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\detectors\__init__.py
**Size**: 37 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: d1f3931c3245e751d1b160958c97a17f72527f8fc41c399e9022cd46f361f583
``````python# package
__all__ = ["YOLODetector"]
## `refactor/ai_viewer/ai/detectors/yolo.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\detectors\yolo.py
**Size**: 2760 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 5d27d2177f452b6abbddaa129b927130f40eb8c2130ba7f61e25654f8355bf38
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class YOLODetector:
    """Lightweight YOLO (ONNX) wrapper for COCO classes.
    Uses OpenCV DNN and letterbox preprocessing.
    """
    COCO = [
        'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light',
        'fire hydrant','stop sign','parking meter','bench','bird','cat','dog','horse','sheep','cow','elephant',
        'bear','zebra','giraffe','backpack','umbrella','handbag','tie','suitcase','frisbee','skis','snowboard',
        'sports ball','kite','baseball bat','baseball glove','skateboard','surfboard','tennis racket','bottle',
        'wine glass','cup','fork','knife','spoon','bowl','banana','apple','sandwich','orange','broccoli',
        'carrot','hot dog','pizza','donut','cake','chair','couch','potted plant','bed','dining table','toilet',
        'tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven','toaster','sink','refrigerator',
        'book','clock','vase','scissors','teddy bear','hair drier','toothbrush'
    ]
    def __init__(self, model_path='ai/models/yolo.onnx', input_size=640, conf=0.35, iou=0.45):
        self.net = None
        self.size = input_size
        self.conf = conf
        self.iou = iou
        if os.path.exists(model_path):
            try:
                self.net = cv2.dnn.readNetFromONNX(model_path)
                self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
                self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            except Exception as e:
                print('[YOLO] load failed:', e)

## `refactor/ai_viewer/ai/recognition/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\recognition\__init__.py
**Size**: 41 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: ed734539c933a3a34bb9e541e1c4e6b33521f993798ff7fa693338129d736a44
``````python# package
__all__ = ["FaceDB", "PetsDB"]
## `refactor/ai_viewer/ai/recognition/face_db.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\recognition\face_db.py
**Size**: 2020 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: b367e6421a6114fa7ebb065523127dd75d8159d8f26f3cb3680988f4f7162c0a
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class FaceDB:
    """Face recognition store.
    - Trains LBPH if contrib available; falls back to ORB matching.
    - Persists samples as images on disk under ai/data/faces/<name>.
    """
    def __init__(self, base='ai/data/faces'):
        self.base = base
        os.makedirs(self.base, exist_ok=True)
        self.size = (160,160)
        self.model = None
        try:
            self.model = cv2.face.LBPHFaceRecognizer_create(radius=1, neighbors=8, grid_x=8, grid_y=8)
        except Exception:
            self.model = None
        self.labels=[]
        self.orb = cv2.ORB_create(nfeatures=1000)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db_descs={}
        self.cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

## `refactor/ai_viewer/ai/recognition/pets_db.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\recognition\pets_db.py
**Size**: 1700 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: d10c458f63d26b5a805f3d5d42b7ef9b1b895f37382c58a1888673880179b2b7
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class PetsDB:
    """Pet recognition store (dogs/cats) using ORB descriptors.
    - Persists samples as grayscale images under ai/data/pets/{dogs,cats}/<name>.
    """
    def __init__(self, base='ai/data/pets'):
        self.base=base; os.makedirs(os.path.join(base,'dogs'),exist_ok=True); os.makedirs(os.path.join(base,'cats'),exist_ok=True)
        self.orb=cv2.ORB_create(nfeatures=1000); self.bf=cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db={'dogs':{},'cats':{}}

## `refactor/ai_viewer/core/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\__init__.py
**Size**: 76 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 70f8d242b6a8f542db5033d778d462e9609cfd4d5d7b23c8853cfbf6089a18c7
``````python# package
__all__ = ["CameraConfig", "CameraStreamThread", "SimpleTracker"]
## `refactor/ai_viewer/core/config.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\config.py
**Size**: 1421 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 8c80b427dd7de971e81acdab4074924191376fb2f529114b1c4be167a5162a82
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class CameraConfig:
    name: str
    host: str                 # ip[:port] for port 80
    user: Optional[str] = None
    password: Optional[str] = None
    token: Optional[str] = None  # Base64 of user:pass

## `refactor/ai_viewer/core/stream.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\stream.py
**Size**: 1330 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: e58383c6d94d46a0701e665731c7882be89ee50d0ae30f7d4d2bdc3a88f3a588
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class CameraStreamThread(QtCore.QThread):
    frameReady = QtCore.Signal(np.ndarray, float)  # (bgr_frame, timestamp)

## `refactor/ai_viewer/core/tracking.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\tracking.py
**Size**: 1513 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 6a32426fb44dd6b178ee556aeed2eb9cfe454b1daf4929a758ecc307590df105
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class SimpleTracker:
    """Very simple IOU-based tracker with TTL for object permanence.
    Tracks: list of dicts with bbox, cls, last_ts.
    """
    def __init__(self, ttl: float = 1.0, iou_thresh: float = 0.3):
        self.ttl = ttl
        self.iou_thresh = iou_thresh
        self.tracks = []

## `refactor/ai_viewer/tools/dupes.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\tools\dupes.py
**Size**: 3486 bytes
**Modified**: 2025-09-01 00:09:02 +0100
**SHA256**: dd5746ceb3a99a54a34531a4516e26cf30aeaed8216a18a2bd670856dfe67385
``````python"""Utility tools for the Qt MDI app.
 - Image ingestion from a source directory into face/pet stores
 - Simple near-duplicate culling via average-hash (aHash)
"""
from __future__ import annotations
import os
import time
from typing import Tuple
import numpy as np
import cv2


def ingest_images(src_dir: str, dest_dir: str, size: Tuple[int,int], gray: bool=False) -> int:
    os.makedirs(dest_dir, exist_ok=True)
    count=0
    for root,_,files in os.walk(src_dir):
        for fn in files:
            if not fn.lower().endswith(('.jpg','.jpeg','.png')):
                continue
            try:
                img=cv2.imread(os.path.join(root,fn))
                if img is None: continue
                if gray:
                    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
                img=cv2.resize(img, size)
                now=time.time(); ms=int((now-int(now))*1000); ds=time.strftime('%Y%m%d_%H%M%S', time.localtime(now))
                out=os.path.join(dest_dir, f'{ds}_{ms:03d}.jpg')
                cv2.imwrite(out, img)
                count+=1
            except Exception:
                pass
    return count


def cull_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4) -> int:
    """Remove near-duplicate images in a folder using aHash similarity.
    Returns number of removed files.
    """
    if not os.path.isdir(target):
        return 0
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; removed=0
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for fp in files:
        h=ahash(fp)
        if h is None: continue
        dup=False
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                try:
                    os.remove(fp); removed+=1
                except Exception:
                    pass
                dup=True
                break
        if not dup:
            hashes.append(h)
    return removed


def find_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4):
    """Analyze a directory for near-duplicates by aHash.
    Returns (files:list[str], remove_indices:set[int]) where remove_indices
    contains indices in files suggested for deletion.
    """
    if not os.path.isdir(target):
        return [], set()
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; remove=set()
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for idx, fp in enumerate(files):
        h=ahash(fp)
        if h is None: continue
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                remove.add(idx)
                break
        else:
            hashes.append(h)
    return files, remove
## `refactor/ai_viewer/tools/gallery.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\tools\gallery.py
**Size**: 2563 bytes
**Modified**: 2025-09-01 00:09:02 +0100
**SHA256**: ed4c0bcf1225b6ccd0d3c7cd68b72431a3de9b44b730b25278901cce8f198aaf
``````python"""Qt thumbnail gallery dialog for selecting and deleting images."""
from __future__ import annotations
import os
from PySide6 import QtCore, QtGui, QtWidgets


class GalleryDialog(QtWidgets.QDialog):
    def __init__(self, dir_path: str, title: str, parent=None):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.resize(900, 600)
        self.dir_path = dir_path
        v = QtWidgets.QVBoxLayout(self)
        self.list = QtWidgets.QListWidget()
        self.list.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.list.setIconSize(QtCore.QSize(160, 120))
        self.list.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.list.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        v.addWidget(self.list, 1)
        btns = QtWidgets.QDialogButtonBox()
        self.btn_del = btns.addButton('Delete Selected', QtWidgets.QDialogButtonBox.ButtonRole.ActionRole)
        self.btn_close = btns.addButton(QtWidgets.QDialogButtonBox.StandardButton.Close)
        v.addWidget(btns)
        self.btn_del.clicked.connect(self.do_delete)
        self.btn_close.clicked.connect(self.accept)
        self.populate()

    def populate(self):
        self.list.clear()
        if not os.path.isdir(self.dir_path):
            return
        files = [os.path.join(self.dir_path,f) for f in os.listdir(self.dir_path)
                 if f.lower().endswith(('.jpg','.jpeg','.png'))]
        files.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        for fp in files:
            item = QtWidgets.QListWidgetItem(os.path.basename(fp))
            try:
                pix = QtGui.QPixmap(fp)
                if not pix.isNull():
                    item.setIcon(QtGui.QIcon(pix.scaled(160,120, QtCore.Qt.KeepAspectRatio, QtCore.Qt.SmoothTransformation)))
            except Exception:
                pass
            item.setData(QtCore.Qt.ItemDataRole.UserRole, fp)
            self.list.addItem(item)

    def do_delete(self):
        items = self.list.selectedItems()
        if not items:
            return
        if QtWidgets.QMessageBox.question(self, 'Delete', f'Delete {len(items)} images?') != QtWidgets.QMessageBox.Yes:
            return
        cnt=0
        for it in items:
            fp = it.data(QtCore.Qt.ItemDataRole.UserRole)
            try:
                os.remove(fp)
                cnt+=1
            except Exception:
                pass
        self.populate()
        QtWidgets.QMessageBox.information(self, 'Manage', f'Deleted {cnt} files')

## `refactor/ai_viewer/ui/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ui\__init__.py
**Size**: 141 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 5da5f6efc2ba7bb8642e5b42a8312221dbffab2d1dc1e25b4aec16c1b0cb10b8
``````python# package
__all__ = ["AddCameraDialog", "CameraWidget", "CollectionDialog", "CullDialog", "DetectionThread", "MainWindow", "SettingsDialog"]
## `refactor/ai_viewer/ui/camera_widget.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ui\camera_widget.py
**Size**: 3104 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: da11cd263950e3b6d678e68d84fba65c2c86dfec988d748df6563fd494a57b6d
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class CollectionDialog(QtWidgets.QDialog):
    stopClicked = QtCore.Signal()
    def __init__(self, parent=None, title: str = 'Collection'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(False)
        self.setWindowFlag(QtCore.Qt.WindowType.WindowStaysOnTopHint, True)
        self.resize(360, 120)
        v = QtWidgets.QVBoxLayout(self)
        self.lbl = QtWidgets.QLabel('Starting...')
        self.lbl.setWordWrap(True)
        v.addWidget(self.lbl, 1)
        btns = QtWidgets.QDialogButtonBox()
        self.btn_stop = btns.addButton('Stop', QtWidgets.QDialogButtonBox.ActionRole)
        self.btn_close = btns.addButton(QtWidgets.QDialogButtonBox.Close)
        v.addWidget(btns)
        self.btn_stop.clicked.connect(lambda: self.stopClicked.emit())
        self.btn_close.clicked.connect(self.accept)
    def set_status(self, text: str):
        try:
            self.lbl.setText(text)
        except Exception:
            pass

class CameraWidget(QtWidgets.QWidget):
    closed = QtCore.Signal(dict)
    eventLogged = QtCore.Signal(str)
    def __init__(self, cfg: CameraConfig, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.setWindowTitle(f"{cfg.name} [{cfg.host}]")
        self.label = QtWidgets.QLabel('Connecting…')
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setMinimumSize(320, 240)
        self.label.setStyleSheet('background:#000; color:#9cf;')

class DetectionThread(QtCore.QThread):
    resultsReady = QtCore.Signal(list, list)  # dets, faces
    def __init__(self, widget: CameraWidget, interval_ms: int = 200):
        super().__init__(widget)
        self.w = widget
        self.interval = interval_ms
        self._stop = threading.Event()
        self._skip_cycles = 0
        self.max_skip_cycles = 1  # skip this many cycles if tracks are active

## `refactor/ai_viewer/ui/windows.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ui\windows.py
**Size**: 7148 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 81fb94986bf50f786583c28d25cf99199e10e398ed012f96a5ff828416a432d5
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class AddCameraDialog(QtWidgets.QDialog):
    def __init__(self, parent=None, initial: Optional[CameraConfig]=None, title: str='Add Camera'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(True)
        form = QtWidgets.QFormLayout(self)
        self.ed_name = QtWidgets.QLineEdit(initial.name if initial else 'Camera')
        self.ed_host = QtWidgets.QLineEdit(initial.host if initial else '192.168.1.100')
        self.ed_user = QtWidgets.QLineEdit(initial.user or '' if initial else '')
        self.ed_pass = QtWidgets.QLineEdit(initial.password or '' if initial else '')
        self.ed_pass.setEchoMode(QtWidgets.QLineEdit.Password)
        self.ed_token = QtWidgets.QLineEdit(initial.token or '' if initial else '')
        form.addRow('Name', self.ed_name)
        form.addRow('Host (ip[:port])', self.ed_host)
        form.addRow('User', self.ed_user)
        form.addRow('Password', self.ed_pass)
        form.addRow('Token (Base64 user:pass)', self.ed_token)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept)
        btns.rejected.connect(self.reject)
        form.addRow(btns)

class SettingsDialog(QtWidgets.QDialog):
    """App settings: detector interval, aHash grid size + Hamming, PTZ aim timing, deadzone."""
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle('Settings')
        self.resize(420, 240)
        form = QtWidgets.QFormLayout(self)
        self.s_det_interval = QtWidgets.QSpinBox(); self.s_det_interval.setRange(50, 2000); self.s_det_interval.setSingleStep(50); self.s_det_interval.setSuffix(' ms')
        self.s_hash_size = QtWidgets.QSpinBox(); self.s_hash_size.setRange(4, 16)
        self.s_hamming = QtWidgets.QSpinBox(); self.s_hamming.setRange(0, 32)
        self.s_ptz_interval = QtWidgets.QSpinBox(); self.s_ptz_interval.setRange(100, 2000); self.s_ptz_interval.setSingleStep(50); self.s_ptz_interval.setSuffix(' ms')
        self.s_deadzone = QtWidgets.QSpinBox(); self.s_deadzone.setRange(2, 20); self.s_deadzone.setSuffix(' %')
        form.addRow('Detector interval', self.s_det_interval)
        form.addRow('aHash grid size', self.s_hash_size)
        form.addRow('Hamming threshold', self.s_hamming)
        form.addRow('PTZ aim interval', self.s_ptz_interval)
        form.addRow('PTZ deadzone', self.s_deadzone)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept); btns.rejected.connect(self.reject)
        form.addRow(btns)

class CullDialog(QtWidgets.QDialog):
    """Preview duplicates with highlight before deletion.
    Includes a tolerance slider to adjust the near-duplicate matching threshold.
    Lower values are stricter; higher tolerate more difference.
    """
    def __init__(self, dir_path: str, files: list[str], remove: set[int], title: str, parent=None):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.resize(1000, 700)
        self.dir_path = dir_path
        self.files = files or []
        self.remove = set(remove or set())
        # Matching parameters (aHash on 8x8 grid → Hamming distance 0..64).
        # Practical duplicate tolerance range ~0..16; start at 4 by default.
        self.hash_size = 8
        self.thresh = 4
        v = QtWidgets.QVBoxLayout(self)
        self.view = QtWidgets.QListWidget()
        self.view.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.view.setIconSize(QtCore.QSize(160, 120))
        self.view.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.view.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        v.addWidget(self.view, 1)
        # Controls
        h = QtWidgets.QHBoxLayout()
        # Tolerance slider block
        tol_box = QtWidgets.QHBoxLayout()
        self.lbl_tol = QtWidgets.QLabel('Tolerance:')
        self.sld_tol = QtWidgets.QSlider(QtCore.Qt.Orientation.Horizontal)
        self.sld_tol.setMinimum(0)
        self.sld_tol.setMaximum(16)
        self.sld_tol.setValue(self.thresh)
        self.sld_tol.setTickInterval(1)
        self.sld_tol.setSingleStep(1)
        self.lbl_tol_val = QtWidgets.QLabel(str(self.thresh))
        tol_box.addWidget(self.lbl_tol)
        tol_box.addWidget(self.sld_tol)
        tol_box.addWidget(self.lbl_tol_val)
        tol_box_w = QtWidgets.QWidget(); tol_box_w.setLayout(tol_box)
        h.addWidget(tol_box_w, 2)
        # Info + actions
        self.lbl_info = QtWidgets.QLabel()
        h.addWidget(self.lbl_info, 1)
        self.btn_confirm = QtWidgets.QPushButton('Confirm Delete')
        self.btn_cancel = QtWidgets.QPushButton('Cancel')
        h.addWidget(self.btn_confirm)
        h.addWidget(self.btn_cancel)
        v.addLayout(h)
        self.btn_confirm.clicked.connect(self.on_confirm)
        self.btn_cancel.clicked.connect(self.reject)
        self.sld_tol.valueChanged.connect(self.on_tol_changed)
        self.populate()

class MainWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle('ESP32-CAM MDI')
        self.resize(1200, 800)
        self.mdi = QtWidgets.QMdiArea()
        self.setCentralWidget(self.mdi)
        # Events sidebar (shows events for active camera)
        self.eventsDock = QtWidgets.QDockWidget('Events', self)
        self.eventsView = QtWidgets.QListWidget()
        self.eventsDock.setWidget(self.eventsView)
        self.addDockWidget(QtCore.Qt.RightDockWidgetArea, self.eventsDock)
        self._active_cam_ref = None
        try:
            self.mdi.subWindowActivated.connect(self.on_sub_activated)
        except Exception:
            pass


def main():
    app = QtWidgets.QApplication(sys.argv)
    mw = MainWindow()
    mw.show()
    sys.exit(app.exec())
## `refactor/run_ai_viewer.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\run_ai_viewer.py
**Size**: 103 bytes
**Modified**: 2025-09-01 00:17:07 +0100
**SHA256**: 11addfebb0dd30e9a9f30d0f362b8a4a2c09ae4af43e9800d3c52c6eb03fe648
``````python#!/usr/bin/env python3
from ai_viewer.ui.windows import main

if __name__ == "__main__":
    main()## `src/app/main.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\app\main.py
**Size**: 277 bytes
**Modified**: 2025-10-23 18:14:11 +0100
**SHA256**: d2ffb5966caa86a3a6f98413d3b34abc01ff6cb21cb18eab04ce67c8a4e435f7
``````python# app/main.py

import sys
from PySide6.QtWidgets import QApplication
from ui.main_window import MainWindow

def main() -> None:
    app = QApplication(sys.argv)
    win = MainWindow()
    win.show()
    sys.exit(app.exec())

if __name__ == "__main__":
    main()
## `src/camera/detection.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\camera\detection.py
**Size**: 524 bytes
**Modified**: 2025-08-31 22:27:27 +0100
**SHA256**: e795dbcb175265732d159cbc36342c44f9031a236c7ecdf53ae24386d0767b6a
``````python# camera/detection.py

from PySide6 import QtCore
import threading
import time
from typing import List, Tuple

class DetectionThread(QtCore.QThread):
    resultsReady = QtCore.Signal(list, list)  # dets, faces

    def __init__(self, widget: 'CameraWidget', interval_ms: int = 200, parent=None):
        super().__init__(parent)
        self.w = widget
        self.interval = interval_ms
        self._stop = threading.Event()
        self.max_skip_cycles = 1

    # … (same implementation as before) …## `src/camera/stream.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\camera\stream.py
**Size**: 787 bytes
**Modified**: 2025-08-31 22:26:17 +0100
**SHA256**: f1cca6c7fbc7ca45ae31f8286ee852baf66b3bfd98161e76bf71dfccb0cb57f0
``````python# camera/stream.py

import requests
import numpy as np
import cv2
from collections import deque
from typing import Deque, Tuple
from PySide6 import QtCore
import os
import time
import threading

class CameraStreamThread(QtCore.QThread):
    frameReady = QtCore.Signal(np.ndarray, float)  # (frame, ts)

    def __init__(self, cfg: CameraConfig, prebuffer_seconds: float = 5.0, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.prebuffer: Deque[Tuple[np.ndarray, float]] = deque(
            maxlen=int(prebuffer_seconds * 20))
        self._stop = threading.Event()
        self._session = None
        self._resp = None
        self._buf = bytearray()

    # … (same implementation as before, only moved into its own file) …## `src/ui/dialogs.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\ui\dialogs.py
**Size**: 1750 bytes
**Modified**: 2025-08-31 22:28:02 +0100
**SHA256**: 1a2657e57dbc90eae2474486a75e8782cd0341d44848ad8176ef0ed27ca78224
``````python# ui/dialogs.py

from PySide6 import QtWidgets
from ..core.config import CameraConfig

class AddCameraDialog(QtWidgets.QDialog):
    def __init__(self, parent=None, initial: CameraConfig | None = None, title: str = 'Add Camera'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(True)
        form = QtWidgets.QFormLayout(self)

        self.ed_name   = QtWidgets.QLineEdit(initial.name if initial else 'Camera')
        self.ed_host   = QtWidgets.QLineEdit(initial.host if initial else '192.168.1.100')
        self.ed_user   = QtWidgets.QLineEdit(initial.user or '')
        self.ed_pass   = QtWidgets.QLineEdit(initial.password or '')
        self.ed_pass.setEchoMode(QtWidgets.QLineEdit.Password)
        self.ed_token  = QtWidgets.QLineEdit(initial.token or '')

        form.addRow('Name',   self.ed_name)
        form.addRow('Host',   self.ed_host)
        form.addRow('User',   self.ed_user)
        form.addRow('Pass',   self.ed_pass)
        form.addRow('Token',  self.ed_token)

        btns = QtWidgets.QDialogButtonBox(
            QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept)
        btns.rejected.connect(self.reject)
        form.addRow(btns)

    def get_config(self) -> CameraConfig | None:
        if self.exec() == QtWidgets.QDialog.Accepted:
            return CameraConfig(
                name=self.ed_name.text().strip() or 'Camera',
                host=self.ed_host.text().strip(),
                user=self.ed_user.text().strip() or None,
                password=self.ed_pass.text(),
                token=self.ed_token.text().strip() or None
            )
        return None## `src/ui/widgets.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\ui\widgets.py
**Size**: 4345 bytes
**Modified**: 2025-08-31 22:25:06 +0100
**SHA256**: b8766fc67b3d30facea44bca78951575f272233906d5af34c68deaf37fedd132
``````python# ui/widgets.py

from PySide6 import QtCore, QtGui, QtWidgets
import cv2
import numpy as np
import os
import time
from ..ai.yolo import YOLODetector
from ..ai.facedb import FaceDB
from ..ai.petsdb import PetsDB
from ..ai.tracker import SimpleTracker
from ..camera.detection import DetectionThread
from ..camera.stream import CameraStreamThread
from ..core.config import CameraConfig
from ..core.settings_dialog import SettingsDialog

class CameraWidget(QtWidgets.QWidget):
    """Widget that displays a single ESP32‑CAM feed and handles all per‑camera UI."""
    closed = QtCore.Signal(dict)          # emitted on close
    eventLogged = QtCore.Signal(str)      # emits a log line

    def __init__(self, cfg: CameraConfig, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.setWindowTitle(f"{cfg.name} [{cfg.host}]")
        self._setup_ui()
        self._init_ai_components()
        self._init_threads()
        self._recording = False
        self._writer = None
        self._aim_timer = QtCore.QTimer(self)
        self._aim_timer.setInterval(300)          # PTZ aim interval
        self._aim_timer.timeout.connect(self._aim_at_target)

    # ------------------------------------------------------------------
    #  UI set‑up (toolbar, labels, etc.)
    # ------------------------------------------------------------------
    def _setup_ui(self):
        self.lbl = QtWidgets.QLabel('Connecting…')
        self.lbl.setAlignment(QtCore.Qt.AlignCenter)
        self.lbl.setMinimumSize(320, 240)
        self.lbl.setStyleSheet('background:#000; color:#9cf;')

        toolbar = QtWidgets.QToolBar()
        toolbar.setMovable(False)
        toolbar.addAction('Start', self.start_stream)
        toolbar.addAction('Stop',  self.stop_stream)
        toolbar.addSeparator()
        toolbar.addAction('Start Rec', self.start_recording)
        toolbar.addAction('Stop Rec',  self.stop_recording)

        # AI toggles
        self.chk_yolo = QtWidgets.QCheckBox('YOLO')
        self.chk_face = QtWidgets.QCheckBox('Face')
        self.chk_dogid = QtWidgets.QCheckBox('Dog ID')
        ai_btn = QtWidgets.QToolButton()
        ai_btn.setText('AI')
        ai_btn.setPopupMode(QtWidgets.QToolButton.InstantPopup)
        ai_menu = QtWidgets.QMenu(ai_btn)
        ai_menu.addAction('YOLO', self.chk_yolo.toggle)
        ai_menu.addAction('Face', self.chk_face.toggle)
        ai_menu.addAction('Dog ID', self.chk_dogid.toggle)
        ai_btn.setMenu(ai_menu)

        self.lbl_status = QtWidgets.QLabel('Ready')
        self.lbl_status.setStyleSheet('color:#246; padding:2px 4px;')

        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(toolbar)
        layout.addWidget(self.lbl, 1)
        layout.setContentsMargins(4,4,4,4)

    # ------------------------------------------------------------------
    #  AI / DB helpers
    # ------------------------------------------------------------------
    def _init_ai_components(self):
        self.yolo   = YOLODetector()
        self.facedb = FaceDB()
        self.facedb.load()
        self.pets   = PetsDB()
        self.pets.load()
        self.tracker = SimpleTracker(ttl=1.0)

    # ------------------------------------------------------------------
    #  Threads
    # ------------------------------------------------------------------
    def _init_threads(self):
        self.stream_thread = CameraStreamThread(self.cfg)
        self.stream_thread.frameReady.connect(self._on_frame)
        self.stream_thread.start()

        self.det_thr = DetectionThread(self)
        self.det_thr.resultsReady.connect(self._on_results)
        self.det_thr.start()

    # ------------------------------------------------------------------
    #  Public API (mostly UI actions)
    # ------------------------------------------------------------------
    def start_stream(self):
        if not self.stream_thread.isRunning():
            self.stream_thread._stop.clear()
            self.stream_thread.start()

    def stop_stream(self):
        if self.stream_thread.isRunning():
            self.stream_thread.stop()
            self.stream_thread.wait(700)   # graceful shutdown

    # … (recording, enrollment, collection, etc.) …## `tools.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\tools.py
**Size**: 3486 bytes
**Modified**: 2025-08-29 00:37:45 +0100
**SHA256**: dd5746ceb3a99a54a34531a4516e26cf30aeaed8216a18a2bd670856dfe67385
``````python"""Utility tools for the Qt MDI app.
 - Image ingestion from a source directory into face/pet stores
 - Simple near-duplicate culling via average-hash (aHash)
"""
from __future__ import annotations
import os
import time
from typing import Tuple
import numpy as np
import cv2


def ingest_images(src_dir: str, dest_dir: str, size: Tuple[int,int], gray: bool=False) -> int:
    os.makedirs(dest_dir, exist_ok=True)
    count=0
    for root,_,files in os.walk(src_dir):
        for fn in files:
            if not fn.lower().endswith(('.jpg','.jpeg','.png')):
                continue
            try:
                img=cv2.imread(os.path.join(root,fn))
                if img is None: continue
                if gray:
                    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
                img=cv2.resize(img, size)
                now=time.time(); ms=int((now-int(now))*1000); ds=time.strftime('%Y%m%d_%H%M%S', time.localtime(now))
                out=os.path.join(dest_dir, f'{ds}_{ms:03d}.jpg')
                cv2.imwrite(out, img)
                count+=1
            except Exception:
                pass
    return count


def cull_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4) -> int:
    """Remove near-duplicate images in a folder using aHash similarity.
    Returns number of removed files.
    """
    if not os.path.isdir(target):
        return 0
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; removed=0
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for fp in files:
        h=ahash(fp)
        if h is None: continue
        dup=False
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                try:
                    os.remove(fp); removed+=1
                except Exception:
                    pass
                dup=True
                break
        if not dup:
            hashes.append(h)
    return removed


def find_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4):
    """Analyze a directory for near-duplicates by aHash.
    Returns (files:list[str], remove_indices:set[int]) where remove_indices
    contains indices in files suggested for deletion.
    """
    if not os.path.isdir(target):
        return [], set()
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; remove=set()
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for idx, fp in enumerate(files):
        h=ahash(fp)
        if h is None: continue
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                remove.add(idx)
                break
        else:
            hashes.append(h)
    return files, remove
