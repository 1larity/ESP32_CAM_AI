## Prompt for Copilot
```
(no prompt provided)
```
# Packaged Project (Python)
- **Project root**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI
- **Generated**: 2025-11-08 00:11:53 +0000
- **Tool**: Copilot Python Packager v1.6
## Table of contents
- `cam discovery.py`
- `camera_recorder.py`
- `detectors.py`
- `discovery_dialog.py`
- `enrollment.py`
- `enrollment_service.py`
- `events_pane.py`
- `gallery.py`
- `image_manager.py`
- `mdi_app.py`
- `models.py`
- `overlays.py`
- `presence.py`
- `ptz.py`
- `recorder.py`
- `refactor/ai_viewer/__init__.py`
- `refactor/ai_viewer/ai/__init__.py`
- `refactor/ai_viewer/ai/detectors/__init__.py`
- `refactor/ai_viewer/ai/detectors/yolo.py`
- `refactor/ai_viewer/ai/recognition/__init__.py`
- `refactor/ai_viewer/ai/recognition/face_db.py`
- `refactor/ai_viewer/ai/recognition/pets_db.py`
- `refactor/ai_viewer/core/__init__.py`
- `refactor/ai_viewer/core/config.py`
- `refactor/ai_viewer/core/stream.py`
- `refactor/ai_viewer/core/tracking.py`
- `refactor/ai_viewer/tools/dupes.py`
- `refactor/ai_viewer/tools/gallery.py`
- `refactor/ai_viewer/ui/__init__.py`
- `refactor/ai_viewer/ui/camera_widget.py`
- `refactor/ai_viewer/ui/windows.py`
- `refactor/run_ai_viewer.py`
- `settings.py`
- `src/app/main.py`
- `src/camera/detection.py`
- `src/camera/stream.py`
- `src/ui/dialogs.py`
- `src/ui/widgets.py`
- `stream.py`
- `tools.py`
- `utils.py`
---
## `cam discovery.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\cam discovery.py
**Size**: 5833 bytes
**Modified**: 2025-11-04 16:49:49 +0000
**SHA256**: b4f55df41e94d63043fbfe1f2abaa549cf6205c4316cf3766a983dd264804c53
``````python#!/usr/bin/env python3
"""
discover_cams.py — Discover ESP32-CAMs on the local LAN.

Strategy
- Determine local /24 automatically via a UDP socket trick (override with --cidr).
- Probe http://IP/ping (your firmware replies "pong").
- Optionally probe :81/stream to confirm MJPEG boundary and auth requirements.
- Concurrency with ThreadPoolExecutor. Safe timeouts.

Usage
  python discover_cams.py                # auto /24 from primary NIC
  python discover_cams.py --cidr 192.168.1.0/24
  python discover_cams.py --check-stream
  python discover_cams.py --workers 256 --timeout 0.6
"""
from __future__ import annotations
import argparse
import ipaddress
import socket
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests


def guess_primary_ipv4() -> str | None:
    """Pick the local IPv4 by opening a UDP socket to a public IP."""
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.settimeout(0.2)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        return None

def default_cidr() -> str:
    return "192.168.1.0/24"

# def default_cidr() -> str:
#     ip = guess_primary_ipv4()
#     if not ip:
#         return "192.168.1.0/24"
#     # assume /24
#     parts = ip.split(".")
#     return f"{parts[0]}.{parts[1]}.{parts[2]}.0/24"


def probe_host(ip: str, timeout: float, check_stream: bool, token: str | None) -> dict | None:
    base = f"http://{ip}"
    session = requests.Session()
    session.headers.update({"User-Agent": "ESP32-CAM-Discovery/1.0"})
    # 1) /ping
    try:
        r = session.get(f"{base}/ping", timeout=timeout)
        text = (r.text or "").strip().lower()
        if r.status_code == 200 and text.startswith("pong"):
            info = {
                "ip": ip,
                "ping": True,
                "auth": False,
                "stream_ok": None,
                "notes": "",
            }
            # try / (may 401 if auth on)
            try:
                r0 = session.get(base + "/", timeout=timeout)
                if r0.status_code == 401:
                    info["auth"] = True
                elif r0.ok:
                    # maybe capture title if present
                    t = r0.text.lower()
                    if "<title" in t:
                        info["notes"] = "web ui reachable"
            except Exception:
                pass

            # 2) :81/stream (optional)
            if check_stream:
                stream_url = f"http://{ip}:81/stream"
                if token:
                    sep = "&" if "?" in stream_url else "?"
                    stream_url = f"{stream_url}{sep}token={token}"
                try:
                    r1 = session.get(stream_url, stream=True, timeout=timeout)
                    if r1.status_code == 401:
                        info["stream_ok"] = False
                        info["auth"] = True
                    elif r1.ok:
                        ctype = r1.headers.get("Content-Type", "")
                        # read a small chunk to confirm boundary, then close
                        try:
                            next(r1.iter_content(chunk_size=512))
                        except Exception:
                            pass
                        info["stream_ok"] = ("multipart/x-mixed-replace" in ctype.lower())
                    else:
                        info["stream_ok"] = False
                except Exception:
                    info["stream_ok"] = False
            return info
    except requests.exceptions.RequestException:
        return None
    return None


def main():
    ap = argparse.ArgumentParser(description="Discover ESP32-CAM devices on LAN")
    ap.add_argument("--cidr", default=default_cidr(), help="CIDR to scan, e.g. 192.168.1.0/24")
    ap.add_argument("--workers", type=int, default=128, help="Max concurrent probes")
    ap.add_argument("--timeout", type=float, default=0.8, help="Per-request timeout seconds")
    ap.add_argument("--check-stream", action="store_true", help="Also probe :81/stream")
    ap.add_argument("--token", default=None, help="Optional Base64 user:pass token for /stream")
    args = ap.parse_args()

    try:
        net = ipaddress.ip_network(args.cidr, strict=False)
    except Exception as e:
        print(f"Invalid CIDR: {e}", file=sys.stderr)
        sys.exit(2)

    hosts = [str(ip) for ip in net.hosts()]
    print(f"Scanning {len(hosts)} hosts in {args.cidr}…")
    t0 = time.time()

    found = []
    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        futs = {
            ex.submit(probe_host, ip, args.timeout, args.check_stream, args.token): ip
            for ip in hosts
        }
        for fut in as_completed(futs):
            res = fut.result()
            if res:
                found.append(res)

    dt = time.time() - t0
    if not found:
        print("No ESP32-CAMs found.")
        print(f"Done in {dt:.1f}s")
        return

    # Output table
    print("\nDiscovered devices:")
    print("{:<15} {:<6} {:<6} {}".format("IP", "PING", "STRM", "NOTES/AUTH"))
    print("-" * 54)
    for d in sorted(found, key=lambda x: x["ip"]):
        ping = "ok" if d["ping"] else "-"
        if d["stream_ok"] is None:
            strm = "-"
        else:
            strm = "ok" if d["stream_ok"] else "fail"
        notes = d["notes"]
        if d["auth"]:
            notes = (notes + " | auth").strip(" |")
        print("{:<15} {:<6} {:<6} {}".format(d["ip"], ping, strm, notes))

    print(f"\nDone in {dt:.1f}s")


if __name__ == "__main__":
    main()
## `camera_recorder.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\camera_recorder.py
**Size**: 8099 bytes
**Modified**: 2025-11-05 23:35:10 +0000
**SHA256**: baca0ce8a4922894da5ecf073f4041882fabb46de3de9037886adff6cf415c2e
``````python
"""
CameraRecorder: pre‑event buffered video recorder for PySide6 apps.

- Ring buffer of (ts_ms, frame_bgr) in RAM.
- On start(): flushes last N seconds then appends live frames.
- Background writer thread with target FPS and codec fallback.
- Safe write to .tmp then rename on stop().
"""

from __future__ import annotations
import threading
import time
from collections import deque
from dataclasses import dataclass
from pathlib import Path
from typing import Deque, Optional, Tuple, List

import numpy as np
import cv2


@dataclass
class RecorderConfig:
    out_dir: Path
    prebuffer_sec: int = 5
    fps: int = 20
    codec_primary: str = "MJPG"
    codec_fallback: str = "mp4v"
    max_ram_mb: int = 256  # soft cap for ring buffer


class CameraRecorder:
    def __init__(self, name: str, cfg: RecorderConfig):
        self.name = name
        self.cfg = cfg
        self.cfg.out_dir.mkdir(parents=True, exist_ok=True)
        (self.cfg.out_dir / ".tmp").mkdir(parents=True, exist_ok=True)

        self._ring: Deque[Tuple[int, np.ndarray]] = deque()
        self._ring_lock = threading.Lock()
        self._ring_bytes = 0

        self._writer = None
        self._writer_path_tmp: Optional[Path] = None
        self._writer_path_final: Optional[Path] = None
        self._writer_lock = threading.Lock()
        self._running = False
        self._write_thread: Optional[threading.Thread] = None
        self._q: Deque[Tuple[int, np.ndarray]] = deque()
        self._q_lock = threading.Lock()

        self._last_written_ts = 0
        self._target_dt = int(1000 / max(1, self.cfg.fps))

        self._w = None
        self._h = None

    # ---- ingestion ----
    def ingest_frame(self, frame_bgr: np.ndarray, ts_ms: Optional[int] = None):
        if frame_bgr is None or frame_bgr.size == 0:
            return
        if ts_ms is None:
            ts_ms = int(time.time() * 1000)

        h, w = frame_bgr.shape[:2]
        if self._w is None:
            self._w, self._h = int(w), int(h)

        # update ring buffer
        b = int(frame_bgr.nbytes) + 16
        with self._ring_lock:
            self._ring.append((ts_ms, frame_bgr.copy()))
            self._ring_bytes += b
            # trim by time
            cutoff = ts_ms - self.cfg.prebuffer_sec * 1000
            while self._ring and self._ring[0][0] < cutoff:
                _, old = self._ring.popleft()
                self._ring_bytes -= int(old.nbytes) + 16
            # trim by RAM
            cap_bytes = self.cfg.max_ram_mb * 1024 * 1024
            while self._ring and self._ring_bytes > cap_bytes:
                _, old = self._ring.popleft()
                self._ring_bytes -= int(old.nbytes) + 16

        # enqueue to writer if recording
        if self._running:
            with self._q_lock:
                self._q.append((ts_ms, frame_bgr.copy()))

    # ---- control ----
    def start(self) -> Path:
        if self._running:
            return self._writer_path_final or self._writer_path_tmp or Path()

        now = time.localtime()
        stamp = time.strftime("%Y%m%d-%H%M%S", now)
        base = f"{self._sanitize(self.name)}_{stamp}"
        tmp = self.cfg.out_dir / ".tmp" / f"{base}.avi"
        final = self.cfg.out_dir / f"{base}.avi"
        self._writer_path_tmp = tmp
        self._writer_path_final = final

        # open writer lazily after we know size
        if self._w is None or self._h is None:
            # fallback default if no frames yet
            self._w, self._h = 640, 480

        w_even = self._w - (self._w % 2)
        h_even = self._h - (self._h % 2)
        if w_even <= 0 or h_even <= 0:
            w_even, h_even = 640, 480

        self._open_writer(tmp, w_even, h_even)

        # seed queue with ring buffer content near target FPS
        seed = []
        with self._ring_lock:
            seed = list(self._ring)
        seed = self._resample(seed, self.cfg.fps)
        with self._q_lock:
            self._q.extend(seed)

        self._running = True
        self._last_written_ts = 0
        self._write_thread = threading.Thread(target=self._writer_loop, name=f"{self.name}-recorder", daemon=True)
        self._write_thread.start()
        return final

    def stop(self) -> Optional[Path]:
        if not self._running:
            return self._writer_path_final
        self._running = False
        # wait for thread
        if self._write_thread:
            self._write_thread.join(timeout=5.0)
        self._write_thread = None

        # close writer
        with self._writer_lock:
            try:
                if self._writer:
                    self._writer.release()
            finally:
                self._writer = None

        # rename tmp -> final if tmp exists and is non-empty
        try:
            if self._writer_path_tmp and self._writer_path_tmp.exists():
                if self._writer_path_tmp.stat().st_size > 0:
                    if self._writer_path_final:
                        try:
                            self._writer_path_final.unlink()
                        except FileNotFoundError:
                            pass
                        self._writer_path_tmp.replace(self._writer_path_final)
                        return self._writer_path_final
        except Exception:
            pass
        return self._writer_path_tmp

    # ---- internals ----
    def _open_writer(self, path: Path, w: int, h: int):
        fourcc = cv2.VideoWriter_fourcc(*self.cfg.codec_primary)
        writer = cv2.VideoWriter(str(path), fourcc, float(self.cfg.fps), (w, h))
        if not writer.isOpened():
            # fallback to mp4 in tmp folder
            alt = path.with_suffix(".mp4")
            fourcc2 = cv2.VideoWriter_fourcc(*self.cfg.codec_fallback)
            writer2 = cv2.VideoWriter(str(alt), fourcc2, float(self.cfg.fps), (w, h))
            if writer2.isOpened():
                self._writer_path_tmp = alt
                writer = writer2
            else:
                raise RuntimeError("Failed to open VideoWriter for both MJPG and mp4v")
        with self._writer_lock:
            self._writer = writer

    def _writer_loop(self):
        target_dt = self._target_dt
        while self._running or self._q:
            item = None
            with self._q_lock:
                if self._q:
                    item = self._q.popleft()
            if item is None:
                time.sleep(0.005)
                continue

            ts, frame = item
            # resample live to target FPS using timestamps
            if self._last_written_ts == 0 or ts - self._last_written_ts >= target_dt:
                self._write_frame(frame)
                self._last_written_ts = ts
            else:
                # drop frame
                continue

    def _write_frame(self, frame: np.ndarray):
        if frame is None or frame.size == 0:
            return
        h, w = frame.shape[:2]
        w_even = w - (w % 2)
        h_even = h - (h % 2)
        if (w_even != w) or (h_even != h):
            frame = cv2.resize(frame, (w_even, h_even), interpolation=cv2.INTER_AREA)
        with self._writer_lock:
            if self._writer:
                self._writer.write(frame)

    @staticmethod
    def _resample(samples: List[Tuple[int, np.ndarray]], fps: int):
        """Return a list of frames at ~fps from timestamped samples."""
        if not samples:
            return []
        target_dt = int(1000 / max(1, fps))
        out: List[Tuple[int, np.ndarray]] = []
        last_ts = 0
        for ts, frame in samples:
            if not out:
                out.append((ts, frame))
                last_ts = ts
            else:
                if ts - last_ts >= target_dt:
                    out.append((ts, frame))
                    last_ts = ts
        return out

    @staticmethod
    def _sanitize(name: str) -> str:
        keep = []
        for ch in name:
            if ch.isalnum() or ch in ("-", "_"):
                keep.append(ch)
            elif ch == " ":
                keep.append("_")
        return "".join(keep)[:64]
## `detectors.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\detectors.py
**Size**: 9831 bytes
**Modified**: 2025-11-08 00:10:08 +0000
**SHA256**: f9e4568a89f8e9d6eb0b9a280e6c90a31f4c3cb51873b7f7095f47e3facdbd01
``````python# detectors.py
# Letterbox-correct YOLO, plus face recognition inference using LBPH.
from __future__ import annotations
import os, threading, time, json
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional
import cv2 as cv, numpy as np
from PyQt6 import QtCore
from utils import monotonic_ms
from enrollment_service import EnrollmentService

@dataclass
class DetectorConfig:
    yolo_model: str
    yolo_conf: float = 0.35
    yolo_nms: float = 0.45
    interval_ms: int = 100
    face_cascade: Optional[str] = None
    @classmethod
    def from_app(cls, app_cfg):
        m = app_cfg.models_dir
        return cls(
            yolo_model=str((m / "yolov8n.onnx").resolve()),
            yolo_conf=app_cfg.thresh_yolo,
            yolo_nms=0.45,
            interval_ms=app_cfg.detect_interval_ms,
            face_cascade=str((m / "haarcascade_frontalface_default.xml").resolve()),
        )

@dataclass
class DetBox:
    cls: str
    score: float
    xyxy: Tuple[int, int, int, int]

@dataclass
class DetectionPacket:
    name: str
    ts_ms: int
    size: Tuple[int, int]
    yolo: List[DetBox] = field(default_factory=list)
    faces: List[DetBox] = field(default_factory=list)
    pets: List[DetBox] = field(default_factory=list)
    timing_ms: Dict[str, int] = field(default_factory=dict)

COCO_ID_TO_NAME = {0: "person", 15: "cat", 16: "dog"}

def _letterbox(img: np.ndarray, new_shape=(640, 640), color=(114,114,114)):
    h, w = img.shape[:2]
    r = min(new_shape[1]/h, new_shape[0]/w)
    nh, nw = int(round(h*r)), int(round(w*r))
    pad_top = (new_shape[1]-nh)//2
    pad_left = (new_shape[0]-nw)//2
    resized = cv.resize(img, (nw, nh), interpolation=cv.INTER_LINEAR)
    canvas = np.full((new_shape[1], new_shape[0], 3), color, dtype=resized.dtype)
    canvas[pad_top:pad_top+nh, pad_left:pad_left+nw] = resized
    return canvas, r, pad_left, pad_top

class DetectorThread(QtCore.QThread):
    resultsReady = QtCore.pyqtSignal(DetectionPacket)

    def __init__(self, cfg: DetectorConfig, name: str):
        super().__init__()
        self.cfg = cfg
        self.name = name
        self._latest = None
        self._lock = threading.RLock()
        self._stop = threading.Event()

        # YOLO
        self._net = None
        if os.path.exists(self.cfg.yolo_model):
            try:
                self._net = cv.dnn.readNet(self.cfg.yolo_model)
            except Exception:
                self._net = None

        # Faces: detector
        self._face = None
        if self.cfg.face_cascade and os.path.exists(self.cfg.face_cascade):
            self._face = cv.CascadeClassifier(self.cfg.face_cascade)

        # Faces: recognizer
        self._rec = None
        self._labels: Dict[int, str] = {}
        self._load_face_recognizer()
        try:
            EnrollmentService.instance().status_changed.connect(self._on_enroll_status)
        except Exception:
            pass
        @QtCore.pyqtSlot(dict)
        def _on_enroll_status(self, st: dict):
            if bool(st.get("done", False)):
                self._load_face_recognizer()
                
    def _load_face_recognizer(self):
        try:
            model_path = os.path.join(os.path.dirname(self.cfg.yolo_model), "lbph_faces.xml")
            labels_path = os.path.join(os.path.dirname(self.cfg.yolo_model), "labels_faces.json")
            if os.path.exists(model_path):
                self._rec = cv.face.LBPHFaceRecognizer_create()
                self._rec.read(model_path)
            if os.path.exists(labels_path):
                with open(labels_path, "r", encoding="utf-8") as fp:
                    m = json.load(fp)
                # stored as {name: id}; invert
                self._labels = {int(v): k for k, v in m.items()}
        except Exception:
            self._rec = None
            self._labels = {}

    def submit_frame(self, cam_name: str, bgr: np.ndarray, ts_ms: int):
        if cam_name != self.name:
            return
        with self._lock:
            self._latest = (bgr.copy(), ts_ms)

    def stop(self):
        self._stop.set()

    def run(self):
        next_due = 0
        while not self._stop.is_set():
            now = monotonic_ms()
            if now < next_due:
                time.sleep(max(0, (next_due - now)/1000.0))
                continue
            next_due = now + self.cfg.interval_ms

            with self._lock:
                snap = self._latest
            if snap is None:
                continue

            bgr, ts_ms = snap
            H, W = bgr.shape[:2]
            pkt = DetectionPacket(self.name, ts_ms, (W, H))
            t0 = monotonic_ms()

            # --- YOLO with true letterbox mapping back to original image ---
            if self._net is not None:
                try:
                    lb, r, padl, padt = _letterbox(bgr, (640, 640))
                    blob = cv.dnn.blobFromImage(lb, 1/255.0, (640, 640), swapRB=True, crop=False)
                    self._net.setInput(blob)
                    out = self._net.forward()
                    boxes, scores, ids = self._parse_yolov8(out)  # in 640-letterboxed space
                    # Map back to original image using r and pads
                    mapped = []
                    for (cx, cy, ww, hh), sc, cid in boxes:
                        # Accept both normalized [0..1] and 640-pixel coords
                        mx = max(abs(cx), abs(cy), abs(ww), abs(hh))
                        if mx <= 1.5:  # normalized
                            cx, cy, ww, hh = cx*640.0, cy*640.0, ww*640.0, hh*640.0
                        # de-letterbox
                        x1 = (cx - ww/2) - padl
                        y1 = (cy - hh/2) - padt
                        x2 = (cx + ww/2) - padl
                        y2 = (cy + hh/2) - padt
                        x1 = int(np.clip(x1 / r, 0, W-1))
                        y1 = int(np.clip(y1 / r, 0, H-1))
                        x2 = int(np.clip(x2 / r, 0, W-1))
                        y2 = int(np.clip(y2 / r, 0, H-1))
                        bw, bh = x2 - x1, y2 - y1
                        if bw <= 2 or bh <= 2:
                            continue
                        mapped.append(([x1, y1, bw, bh], sc, cid))
                    if mapped:
                        nms_boxes = [m[0] for m in mapped]
                        nms_scores = [float(m[1]) for m in mapped]
                        idxs = cv.dnn.NMSBoxes(nms_boxes, nms_scores, self.cfg.yolo_conf, self.cfg.yolo_nms)
                        if len(idxs) > 0:
                            for i in idxs.flatten():
                                x, y, bw, bh = nms_boxes[i]
                                cid = int(mapped[i][2])
                                pkt.yolo.append(DetBox(COCO_ID_TO_NAME.get(cid, str(cid)),
                                                       float(nms_scores[i]), (x, y, x+bw, y+bh)))
                except Exception:
                    pass

            pkt.timing_ms["det"] = monotonic_ms() - t0

            # --- Haar faces + LBPH recognition ---
            if self._face is not None:
                try:
                    gray = cv.cvtColor(bgr, cv.COLOR_BGR2GRAY)
                    faces = self._face.detectMultiScale(gray, 1.2, 4, minSize=(32, 32))
                    for (fx, fy, fw, fh) in faces:
                        name = "face"
                        score = 1.0
                        if self._rec is not None:
                            try:
                                roi = cv.resize(gray[fy:fy+fh, fx:fx+fw], (128, 128), interpolation=cv.INTER_AREA)
                                lab, conf = self._rec.predict(roi)
                                # LBPH: lower conf is better. Empirical threshold ~65–85.
                                if conf <= 75 and lab in self._labels:
                                    name = self._labels[lab]
                                    # Map LBPH conf to 0..1 score for display
                                    score = float(max(0.0, min(1.0, 1.0 - (conf / 100.0))))
                            except Exception:
                                pass
                        pkt.faces.append(DetBox(name, score, (fx, fy, fx+fw, fy+fh)))
                except Exception:
                    pass

            self.resultsReady.emit(pkt)

    def _parse_yolov8(self, out: np.ndarray):
        """Return list of (cx,cy,w,h),score,class_id in the 640x640 letterbox space.
        Handles (1,84,N) and (1,N,85). Does NOT scale to image here."""
        out = out
        dets = []
        if out.ndim == 3 and out.shape[1] >= 84:  # (1,84,N)
            a = out[0]
            xywh = a[0:4, :]
            cls = a[4:, :]
            cls_ids = np.argmax(cls, axis=0)
            cls_scores = cls.max(axis=0)
            sel = cls_scores >= self.cfg.yolo_conf
            idxs = np.where(sel)[0]
            for i in idxs:
                cx, cy, ww, hh = [float(v) for v in xywh[:, i]]
                dets.append(((cx, cy, ww, hh), float(cls_scores[i]), int(cls_ids[i])))
        elif out.ndim == 3 and out.shape[2] >= 85:  # (1,N,85)
            a = out[0]
            xywh = a[:, 0:4]
            cls = a[:, 5:]
            cls_ids = np.argmax(cls, axis=1)
            cls_scores = cls.max(axis=1)
            sel = cls_scores >= self.cfg.yolo_conf
            idxs = np.where(sel)[0]
            for i in idxs:
                cx, cy, ww, hh = [float(v) for v in xywh[i]]
                dets.append(((cx, cy, ww, hh), float(cls_scores[i]), int(cls_ids[i])))
        return dets
## `discovery_dialog.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\discovery_dialog.py
**Size**: 3902 bytes
**Modified**: 2025-11-07 23:58:42 +0000
**SHA256**: 8371e9132b270ca16d50b073a1dd312bb2c17eaf7a93398d82a09a5cd409810f
``````python# discovery_dialog.py
# Local subnet scanner for ESP32-CAM. Looks for /status or /stream on :80 and :81.
from __future__ import annotations
import socket
import threading
from PyQt6 import QtWidgets, QtCore
import requests

def _default_subnet() -> str:
    # Try to guess local /24
    try:
        host = socket.gethostbyname(socket.gethostname())
        parts = host.split(".")
        if len(parts) == 4:
            return ".".join(parts[:3]) + "."
    except Exception:
        pass
    return "192.168.1."

class DiscoveryDialog(QtWidgets.QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Discover ESP32-CAM")
        self.edit_subnet = QtWidgets.QLineEdit(_default_subnet())
        self.edit_range_from = QtWidgets.QSpinBox(); self.edit_range_from.setRange(1,254); self.edit_range_from.setValue(1)
        self.edit_range_to   = QtWidgets.QSpinBox(); self.edit_range_to.setRange(1,254); self.edit_range_to.setValue(254)
        self.btn_scan = QtWidgets.QPushButton("Scan")
        self.btn_stop = QtWidgets.QPushButton("Stop"); self.btn_stop.setEnabled(False)
        self.list = QtWidgets.QListWidget()
        self.lbl = QtWidgets.QLabel("Finds devices responding on port 80/81 with /status or /stream.")
        form = QtWidgets.QFormLayout()
        form.addRow("Subnet prefix", self.edit_subnet)
        form.addRow("Range", self._range_row())
        btns = QtWidgets.QHBoxLayout(); btns.addWidget(self.btn_scan); btns.addWidget(self.btn_stop); btns.addStretch(1)
        lay = QtWidgets.QVBoxLayout(self); lay.addLayout(form); lay.addLayout(btns); lay.addWidget(self.list); lay.addWidget(self.lbl)
        self._stop = threading.Event()
        self.btn_scan.clicked.connect(self._start)
        self.btn_stop.clicked.connect(self._cancel)

    def _range_row(self):
        w = QtWidgets.QWidget(); h = QtWidgets.QHBoxLayout(w)
        h.setContentsMargins(0,0,0,0)
        h.addWidget(self.edit_range_from); h.addWidget(QtWidgets.QLabel("to")); h.addWidget(self.edit_range_to); h.addStretch(1)
        return w

    def _start(self):
        self.list.clear()
        self._stop.clear()
        self.btn_scan.setEnabled(False); self.btn_stop.setEnabled(True)
        subnet = self.edit_subnet.text().strip()
        a = int(self.edit_range_from.value()); b = int(self.edit_range_to.value())
        t = threading.Thread(target=self._scan_range, args=(subnet, a, b), daemon=True)
        t.start()

    def _cancel(self):
        self._stop.set()
        self.btn_scan.setEnabled(True); self.btn_stop.setEnabled(False)

    def _scan_range(self, subnet: str, a: int, b: int):
        sess = requests.Session()
        sess.headers.update({"User-Agent":"ESP32-CAM-AI-Discovery/1.0"})
        for i in range(a, b+1):
            if self._stop.is_set(): break
            ip = f"{subnet}{i}"
            for port in (81, 80):
                for path in ("/status", "/api/status", "/stream", "/"):
                    url = f"http://{ip}:{port}{path}"
                    try:
                        r = sess.get(url, timeout=0.6)
                        if r.status_code == 200:
                            self._add_item(ip, port, path)
                            raise StopIteration  # found something on this IP
                    except StopIteration:
                        break
                    except Exception:
                        pass

        self._done()

    @QtCore.pyqtSlot()
    def _done(self):
        self.btn_scan.setEnabled(True); self.btn_stop.setEnabled(False)

    def _add_item(self, ip: str, port: int, path: str):
        QtCore.QMetaObject.invokeMethod(self.list, "addItem", QtCore.Qt.ConnectionType.QueuedConnection,
                                        QtCore.Q_ARG(str, f"{ip}:{port}  {path}"))
## `enrollment.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\enrollment.py
**Size**: 2694 bytes
**Modified**: 2025-11-07 00:18:11 +0000
**SHA256**: 933d2a0ed53898b90de96feea2d34b81558e5aa0a91471b6082c8f8fc478bb8c
``````python# enrollment.py
# Progress UI with label and progress bar, bound to EnrollmentService signals.
from __future__ import annotations
from PyQt6 import QtWidgets, QtCore
from settings import AppSettings
from enrollment_service import EnrollmentService

class EnrollDialog(QtWidgets.QDialog):
    def __init__(self, app_cfg: AppSettings, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Enrollment")
        self.svc = EnrollmentService.instance()

        self.name = QtWidgets.QLineEdit()
        self.target = QtWidgets.QSpinBox(); self.target.setRange(5, 400); self.target.setValue(40)
        self.btn_start = QtWidgets.QPushButton("Start Face Enrollment")
        self.btn_stop = QtWidgets.QPushButton("Stop")
        self.lbl_status = QtWidgets.QLabel("Idle")
        self.pb = QtWidgets.QProgressBar(); self.pb.setRange(0, 100); self.pb.setValue(0)

        form = QtWidgets.QFormLayout()
        form.addRow("Name", self.name)
        form.addRow("Samples", self.target)
        btns = QtWidgets.QHBoxLayout(); btns.addWidget(self.btn_start); btns.addWidget(self.btn_stop)
        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(form); lay.addLayout(btns); lay.addWidget(self.lbl_status); lay.addWidget(self.pb)

        self.btn_start.clicked.connect(self._start)
        self.btn_stop.clicked.connect(self.svc.end)
        self.svc.status_changed.connect(self._on_status)

        # preset last used name if dialog reopened quickly
        self._on_status({
            "active": self.svc.active, "name": self.svc.target_name,
            "got": self.svc.samples_got, "need": self.svc.samples_needed,
            "folder": "", "done": False
        })

    def _start(self):
        nm = self.name.text().strip()
        if not nm:
            QtWidgets.QMessageBox.warning(self, "Enrollment", "Enter a name.")
            return
        self.svc.begin_face(nm, int(self.target.value()))

    @QtCore.pyqtSlot(dict)
    def _on_status(self, st: dict):
        got = int(st.get("got", 0)); need = max(1, int(st.get("need", 1)))
        pct = int(round(100.0 * got / need))
        self.pb.setValue(pct)
        nm = st.get("name", "")
        active = bool(st.get("active", False))
        done = bool(st.get("done", False))
        if nm and not self.name.text().strip():
            self.name.setText(nm)
        if active:
            self.lbl_status.setText(f"Collecting {got}/{need} → {st.get('folder','')}")
        elif done:
            self.lbl_status.setText(f"Done {got}/{need}. Training saved to models/lbph_faces.xml.")
        else:
            self.lbl_status.setText("Idle")
## `enrollment_service.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\enrollment_service.py
**Size**: 6949 bytes
**Modified**: 2025-11-07 23:57:59 +0000
**SHA256**: 3c0ace4b9f572901a4ba600c60a3ff9a14d77c819d16d1e86cb8ccc7cab88904
``````python# enrollment_service.py
# Progress signals, reliable saving, debounce, and final LBPH training + labels.
from __future__ import annotations
from pathlib import Path
import json
import time
import cv2 as cv
import numpy as np
from PyQt6 import QtCore
from detectors import DetectionPacket
from settings import BASE_DIR

class EnrollmentService(QtCore.QObject):
    _inst = None

    # Emitted on any state change:
    #   {"active":bool,"name":str,"got":int,"need":int,"folder":str,"done":bool}
    status_changed = QtCore.pyqtSignal(dict)

    def __init__(self):
        super().__init__()
        self.active = False
        self.target_name = ""
        self.samples_needed = 20
        self.samples_got = 0
        self._last_save_ms = 0
        self._min_gap_ms = 150  # avoid saving identical consecutive frames
        self._last_gray = None  # for quick similarity pruning

        # Discover faces root compatible with legacy layout
        candidates = [
            BASE_DIR / "data" / "faces",
            BASE_DIR / "data" / "enroll" / "faces",
            BASE_DIR / "faces",
        ]
        for c in candidates:
            if c.exists():
                self.face_dir = c
                break
        else:
            self.face_dir = BASE_DIR / "data" / "faces"
        self.models_dir = BASE_DIR / "models"

    @classmethod
    def instance(cls) -> "EnrollmentService":
        if cls._inst is None:
            cls._inst = EnrollmentService()
        return cls._inst

    # ---- API ----
    def begin_face(self, name: str, n: int):
        self.target_name = name.strip()
        self.samples_needed = max(1, int(n))
        self.samples_got = 0
        self._last_save_ms = 0
        self._last_gray = None
        self.active = True
        (self.face_dir / self.target_name).mkdir(parents=True, exist_ok=True)
        self._emit()

    def end(self):
        self.active = False
        self._emit()

    # Called from CameraWidget._on_detections on each detection packet
    def on_detections(self, cam_name: str, bgr: np.ndarray, pkt: DetectionPacket):
        if not self.active or not self.target_name:
            return

        # pick largest face
        best = None
        best_area = 0
        for f in pkt.faces:
            x1, y1, x2, y2 = f.xyxy
            area = max(0, x2 - x1) * max(0, y2 - y1)
            if area > best_area:
                best_area = area
                best = (x1, y1, x2, y2)
        if best is None:
            return

        # debounce by time
        now_ms = pkt.ts_ms or int(time.monotonic() * 1000)
        if now_ms - self._last_save_ms < self._min_gap_ms:
            return

        x1, y1, x2, y2 = best
        h, w = bgr.shape[:2]
        x1 = max(0, min(w - 1, x1)); x2 = max(0, min(w - 1, x2))
        y1 = max(0, min(h - 1, y1)); y2 = max(0, min(h - 1, y2))
        if x2 <= x1 + 2 or y2 <= y1 + 2:
            return

        crop = bgr[y1:y2, x1:x2]
        gray = cv.cvtColor(crop, cv.COLOR_BGR2GRAY)
        gray = cv.resize(gray, (128, 128), interpolation=cv.INTER_AREA)

        # simple similarity prune vs last saved
        if self._last_gray is not None:
            diff = cv.absdiff(gray, self._last_gray)
            if float(cv.mean(diff)[0]) < 2.0:
                # too similar to last; skip this frame
                return

        self._last_gray = gray
        idx = self.samples_got + 1
        out = (self.face_dir / self.target_name / f"{idx:03d}.png")
        out.parent.mkdir(parents=True, exist_ok=True)
        cv.imwrite(str(out), gray)
        self.samples_got += 1
        self._last_save_ms = now_ms
        self._emit()

        if self.samples_got >= self.samples_needed:
            self.active = False
            self._emit()
            self._train_lbph()

    # ---- internals ----
    def _emit(self):
        self.status_changed.emit({
            "active": self.active,
            "name": self.target_name,
            "got": self.samples_got,
            "need": self.samples_needed,
            "folder": str(self.face_dir / self.target_name),
            "done": (not self.active and self.samples_got >= self.samples_needed)
        })

    def _train_lbph(self):
        # Aggregate all persons; write model + labels
        subs = [p for p in (self.face_dir).iterdir() if p.is_dir()]
        imgs = []; labels = []; label_map = {}; next_id = 0
        for p in sorted(subs):
            label_map[p.name] = next_id
            for f in sorted(list(p.glob("*.png")) + list(p.glob("*.jpg")) + list(p.glob("*.jpeg"))):
                im = cv.imread(str(f), cv.IMREAD_GRAYSCALE)
                if im is None:
                    continue
                imgs.append(im); labels.append(next_id)
            next_id += 1
        if not imgs:
            return
        try:
            rec = cv.face.LBPHFaceRecognizer_create(radius=1, neighbors=8, grid_x=8, grid_y=8)
        except Exception:
            return
        rec.train(imgs, np.array(labels))
        self.models_dir.mkdir(parents=True, exist_ok=True)
        rec.write(str(self.models_dir / "lbph_faces.xml"))
        with open(self.models_dir / "labels_faces.json", "w", encoding="utf-8") as fp:
            json.dump(label_map, fp, indent=2)
        # notify completion
        self.status_changed.emit({
            "active": False, "name": self.target_name, "got": self.samples_got,
            "need": self.samples_needed, "folder": str(self.face_dir / self.target_name), "done": True
        })
    def rebuild_from_disk(self) -> bool:
        subs = [p for p in (self.face_dir).iterdir() if p.is_dir()]
        imgs = []; labels = []; label_map = {}; next_id = 0
        for p in sorted(subs):
            label_map[p.name] = next_id
            for f in sorted(list(p.glob("*.png")) + list(p.glob("*.jpg")) + list(p.glob("*.jpeg"))):
                im = cv.imread(str(f), cv.IMREAD_GRAYSCALE)
                if im is None:
                    continue
                imgs.append(im); labels.append(next_id)
            next_id += 1
        if not imgs:
            return False
        try:
            rec = cv.face.LBPHFaceRecognizer_create(radius=1, neighbors=8, grid_x=8, grid_y=8)
        except Exception:
            return False
        rec.train(imgs, np.array(labels))
        self.models_dir.mkdir(parents=True, exist_ok=True)
        rec.write(str(self.models_dir / "lbph_faces.xml"))
        with open(self.models_dir / "labels_faces.json", "w", encoding="utf-8") as fp:
            json.dump(label_map, fp, indent=2)
        self.status_changed.emit({"active": False, "name": self.target_name, "got": self.samples_got,
                                  "need": self.samples_needed, "folder": str(self.face_dir), "done": True})
        return True## `events_pane.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\events_pane.py
**Size**: 2048 bytes
**Modified**: 2025-11-06 23:54:27 +0000
**SHA256**: ff70ffcd75a7cb0133d7ef568eedad360a9f1ec87f6ef7a8890d082380076b17
``````python# events_pane.py
# Dockable pane that tails JSONL event logs and shows a live list.
from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, List
from PyQt6 import QtWidgets, QtCore

class EventsPane(QtWidgets.QWidget):
    def __init__(self, logs_dir: Path, parent=None):
        super().__init__(parent)
        self.logs_dir = Path(logs_dir)
        self.list = QtWidgets.QListWidget()
        self.btn_open = QtWidgets.QPushButton("Open Logs Folder")
        self.btn_clear = QtWidgets.QPushButton("Clear View")
        btns = QtWidgets.QHBoxLayout(); btns.addWidget(self.btn_open); btns.addStretch(1); btns.addWidget(self.btn_clear)
        lay = QtWidgets.QVBoxLayout(self); lay.addWidget(self.list); lay.addLayout(btns)
        self.btn_open.clicked.connect(self._open_logs)
        self.btn_clear.clicked.connect(self.list.clear)
        self._pos: Dict[Path, int] = {}
        self._timer = QtCore.QTimer(self); self._timer.timeout.connect(self._poll); self._timer.start(500)

    def _open_logs(self):
        from utils import open_folder_or_warn
        open_folder_or_warn(self, self.logs_dir)

    def _poll(self):
        if not self.logs_dir.exists(): return
        for p in sorted(self.logs_dir.glob("*.jsonl")):
            last = self._pos.get(p, 0)
            try:
                with p.open("rb") as fp:
                    fp.seek(last)
                    for line in fp:
                        try:
                            rec = json.loads(line.decode("utf-8", "ignore"))
                        except Exception:
                            continue
                        ts = rec.get("ts")
                        cam = rec.get("camera")
                        ev = rec.get("event")
                        typ = rec.get("type")
                        self.list.addItem(f"{cam} — {ev} {typ} @ {ts}")
                    self._pos[p] = fp.tell()
            except FileNotFoundError:
                self._pos.pop(p, None)
## `gallery.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\gallery.py
**Size**: 4480 bytes
**Modified**: 2025-11-07 00:32:41 +0000
**SHA256**: 017f2ac2e9f96277e6bf05d812333346394fea2f78a66f6b93a6c8261f517068
``````python# gallery.py
# (unchanged from previous message except for a small guard to display empty dirs gracefully)
from __future__ import annotations
from pathlib import Path
from typing import List
import cv2 as cv, numpy as np
from PyQt6 import QtWidgets, QtGui, QtCore

def _thumb(path: Path, max_size: int = 160) -> QtGui.QPixmap:
    im = cv.imread(str(path), cv.IMREAD_UNCHANGED)
    if im is None:
        return QtGui.QPixmap(160,160)
    if im.ndim == 2:
        im = cv.cvtColor(im, cv.COLOR_GRAY2RGB)
    elif im.shape[2] == 4:
        im = cv.cvtColor(im, cv.COLOR_BGRA2RGB)
    else:
        im = cv.cvtColor(im, cv.COLOR_BGR2RGB)
    h, w = im.shape[:2]
    s = max_size / float(max(h, w)) if max(h, w) else 1.0
    im = cv.resize(im, (max(1, int(w*s)), max(1, int(h*s))), interpolation=cv.INTER_AREA)
    qimg = QtGui.QImage(im.data, im.shape[1], im.shape[0], int(im.strides[0]), QtGui.QImage.Format.Format_RGB888).copy()
    return QtGui.QPixmap.fromImage(qimg)

class GalleryDialog(QtWidgets.QDialog):
    def __init__(self, folder: Path, parent=None):
        super().__init__(parent)
        self.setWindowTitle(f"Gallery — {Path(folder).name}")
        self.folder = Path(folder)
        self.view = QtWidgets.QListWidget()
        self.view.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.view.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.view.setIconSize(QtCore.QSize(160, 160))
        self.view.setMovement(QtWidgets.QListView.Movement.Static)
        self.view.setSpacing(8)
        self.view.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        self.btn_del = QtWidgets.QPushButton("Delete Selected")
        self.btn_prune = QtWidgets.QPushButton("Self-Prune Near Duplicates")
        self.btn_refresh = QtWidgets.QPushButton("Refresh")

        btns = QtWidgets.QHBoxLayout()
        btns.addWidget(self.btn_refresh); btns.addStretch(1); btns.addWidget(self.btn_prune); btns.addWidget(self.btn_del)

        lay = QtWidgets.QVBoxLayout(self)
        lay.addWidget(self.view); lay.addLayout(btns)

        self.btn_refresh.clicked.connect(self._load)
        self.btn_del.clicked.connect(self._delete_selected)
        self.btn_prune.clicked.connect(self._self_prune)
        self._load()

    def _load(self):
        self.view.clear()
        if not self.folder.exists():
            return
        pats = ["*.png", "*.PNG", "*.jpg", "*.JPG", "*.jpeg", "*.JPEG"]
        files: List[Path] = []
        for p in pats:
            files += sorted(self.folder.glob(p))
        for f in files:
            pm = _thumb(f)
            it = QtWidgets.QListWidgetItem(QtGui.QIcon(pm), f.name)
            it.setData(QtCore.Qt.ItemDataRole.UserRole, str(f))
            self.view.addItem(it)

    def _delete_selected(self):
        sel = self.view.selectedItems()
        for it in sel:
            Path(it.data(QtCore.Qt.ItemDataRole.UserRole)).unlink(missing_ok=True)
            self.view.takeItem(self.view.row(it))

    def _self_prune(self):
        files = [Path(self.view.item(i).data(QtCore.Qt.ItemDataRole.UserRole)) for i in range(self.view.count())]
        imgs = []
        for f in files:
            im = cv.imread(str(f), cv.IMREAD_GRAYSCALE)
            if im is None: continue
            imgs.append((f, cv.resize(im, (160, 160), interpolation=cv.INTER_AREA)))
        if len(imgs) < 2:
            QtWidgets.QMessageBox.information(self, "Self-Prune", "Not enough images to compare.")
            return
        orb = cv.ORB_create()
        kept = []
        pruned = 0
        for f, im in imgs:
            k, d = orb.detectAndCompute(im, None)
            if d is None or len(k) < 10:
                kept.append((f, k, d)); continue
            drop = False
            for fk, kk, dk in kept:
                if dk is None: continue
                bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)
                m = bf.match(d, dk)
                if not m: continue
                dmean = float(np.mean([mm.distance for mm in m]))
                sim = 1.0 - (dmean / 100.0)
                if sim >= 0.82:
                    try: f.unlink()
                    except Exception: pass
                    pruned += 1
                    drop = True
                    break
            if not drop:
                kept.append((f, k, d))
        self._load()
        QtWidgets.QMessageBox.information(self, "Self-Prune", f"Removed {pruned} near-duplicates.")
## `image_manager.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\image_manager.py
**Size**: 3336 bytes
**Modified**: 2025-11-06 23:55:07 +0000
**SHA256**: 8c6d889ee16914bf49bfbecc723b886b424893c3312a7b414ae61343c99500c2
``````python# image_manager.py
# Faces and Pets manager with rename, delete, open folder, and Gallery launcher.
from __future__ import annotations
from pathlib import Path
from PyQt6 import QtWidgets
from utils import open_folder_or_warn
from settings import BASE_DIR
from gallery import GalleryDialog

class ImageManagerDialog(QtWidgets.QDialog):
    def __init__(self, app_cfg, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Image Manager")
        self.faces_root = BASE_DIR / "data" / "faces"
        self.pets_root  = BASE_DIR / "data" / "pets"

        self.tabs = QtWidgets.QTabWidget()
        self.faces_tab = self._build_tab(self.faces_root, is_pets=False)
        self.pets_tab  = self._build_tab(self.pets_root,  is_pets=True)
        self.tabs.addTab(self.faces_tab, "Faces")
        self.tabs.addTab(self.pets_tab,  "Pets")

        lay = QtWidgets.QVBoxLayout(self); lay.addWidget(self.tabs)

    def _build_tab(self, root: Path, is_pets: bool) -> QtWidgets.QWidget:
        w = QtWidgets.QWidget()
        lst = QtWidgets.QListWidget(); lst.setObjectName("list")
        btn_open = QtWidgets.QPushButton("Open Folder"); btn_refresh = QtWidgets.QPushButton("Refresh")
        btn_rename = QtWidgets.QPushButton("Rename…"); btn_delete = QtWidgets.QPushButton("Delete…")
        btn_gallery = QtWidgets.QPushButton("Open Gallery…")

        buttons = QtWidgets.QHBoxLayout()
        for b in (btn_open, btn_refresh, btn_rename, btn_delete, btn_gallery): buttons.addWidget(b)
        buttons.addStretch(1)

        lay = QtWidgets.QVBoxLayout(w); lay.addWidget(lst); lay.addLayout(buttons)

        def _refresh():
            lst.clear(); root.mkdir(parents=True, exist_ok=True)
            for d in sorted(root.glob("*")):
                if d.is_dir(): lst.addItem(d.name)
        def _sel_path():
            it = lst.currentItem()
            if not it: return None
            return root / it.text()
        def _open():
            open_folder_or_warn(self, root)
        def _rename():
            p = _sel_path()
            if not p: return
            new, ok = QtWidgets.QInputDialog.getText(self, "Rename", f"New name for '{p.name}':", text=p.name)
            if not ok or not new.strip(): return
            (root / new.strip()).mkdir(parents=True, exist_ok=True)
            for f in p.glob("*"): f.rename(root / new.strip() / f.name)
            try: p.rmdir()
            except Exception: pass
            _refresh()
        def _delete():
            p = _sel_path()
            if not p: return
            if QtWidgets.QMessageBox.question(self, "Delete", f"Delete '{p.name}' and all samples?") != QtWidgets.QMessageBox.StandardButton.Yes:
                return
            for f in p.glob("**/*"): f.unlink(missing_ok=True)
            try: p.rmdir()
            except Exception: pass
            _refresh()
        def _gallery():
            p = _sel_path()
            if not p: return
            GalleryDialog(p, parent=self).exec()

        btn_open.clicked.connect(_open)
        btn_refresh.clicked.connect(_refresh)
        btn_rename.clicked.connect(_rename)
        btn_delete.clicked.connect(_delete)
        btn_gallery.clicked.connect(_gallery)

        _refresh()
        return w
## `mdi_app.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\mdi_app.py
**Size**: 17581 bytes
**Modified**: 2025-11-08 00:05:07 +0000
**SHA256**: 9a0e76b6803bf682d2005c1793aba48044a1fd17f55c40e7539ecc891b8db878
``````python# mdi_app.py
# Multi-camera MDI app with zoom/pan, overlays, enrollment progress, events dock, and discovery.
# Entry point included. High-DPI tweaks removed for maximum PyQt6 compatibility.

from __future__ import annotations
import sys
from typing import Optional
from PyQt6 import QtCore, QtGui, QtWidgets

from detectors import DetectorThread, DetectorConfig, DetectionPacket
from overlays import OverlayFlags, draw_overlays
from recorder import PrebufferRecorder
from presence import PresenceBus
from settings import AppSettings, CameraSettings, load_settings, save_settings
from ptz import PTZClient
from utils import qimage_from_bgr, open_folder_or_warn
from stream import StreamCapture
from enrollment import EnrollDialog
from image_manager import ImageManagerDialog
from models import ModelManager
from enrollment_service import EnrollmentService
from events_pane import EventsPane
from discovery_dialog import DiscoveryDialog


# ---------- Graphics view with zoom/pan ----------
class GraphicsView(QtWidgets.QGraphicsView):
    zoomChanged = QtCore.pyqtSignal(float)

    def __init__(self, scene, parent=None):
        super().__init__(scene, parent)
        self.setRenderHints(
            QtGui.QPainter.RenderHint.Antialiasing
            | QtGui.QPainter.RenderHint.SmoothPixmapTransform
            | QtGui.QPainter.RenderHint.TextAntialiasing
        )
        self.setDragMode(QtWidgets.QGraphicsView.DragMode.ScrollHandDrag)
        self._scale = 1.0
        self._pan = QtCore.QPointF(0, 0)
        self.setTransformationAnchor(QtWidgets.QGraphicsView.ViewportAnchor.NoAnchor)
        self.setResizeAnchor(QtWidgets.QGraphicsView.ViewportAnchor.NoAnchor)
        self.setMouseTracking(True)

    def wheelEvent(self, e: QtGui.QWheelEvent):
        if QtWidgets.QApplication.keyboardModifiers() == QtCore.Qt.KeyboardModifier.ControlModifier:
            s = 1.0 + (0.0015 * e.angleDelta().y())
            self._scale = float(max(0.1, min(8.0, self._scale * s)))
            t = QtGui.QTransform()
            t.translate(self._pan.x(), self._pan.y())
            t.scale(self._scale, self._scale)
            self.setTransform(t)
            self.zoomChanged.emit(self._scale)
            e.accept()
            return
        super().wheelEvent(e)

    def fit_to_window(self):
        rect = None
        for it in self.scene().items():
            if isinstance(it, QtWidgets.QGraphicsPixmapItem):
                rect = it.boundingRect()
                break
        if not rect or rect.isEmpty():
            return
        self.resetTransform()
        self.fitInView(rect, QtCore.Qt.AspectRatioMode.KeepAspectRatio)
        self._scale = self.transform().m11()
        self._pan = QtCore.QPointF(self.transform().m31(), self.transform().m32())
        self.zoomChanged.emit(self._scale)

    def set_100(self):
        self._scale = 1.0
        self._pan = QtCore.QPointF(0, 0)
        t = QtGui.QTransform()
        t.translate(self._pan.x(), self._pan.y())
        t.scale(self._scale, self._scale)
        self.setTransform(t)
        self.zoomChanged.emit(self._scale)


# ---------- Camera widget ----------
class CameraWidget(QtWidgets.QWidget):
    def __init__(self, cam_cfg: CameraSettings, app_cfg: AppSettings, parent=None):
        super().__init__(parent)
        self.cam_cfg = cam_cfg
        self.app_cfg = app_cfg

        self._last_qimage: Optional[QtGui.QImage] = None
        self._last_bgr = None
        self._overlay_pkt: Optional[DetectionPacket] = None
        self._overlay_flags = OverlayFlags(yolo=True, faces=True, pets=True, tracks=True)

        self._scene = QtWidgets.QGraphicsScene(self)
        self._pix_item = QtWidgets.QGraphicsPixmapItem()
        self._scene.addItem(self._pix_item)
        self.view = GraphicsView(self._scene, self)

        self.btn_fit = QtWidgets.QToolButton(text="Fit")
        self.btn_100 = QtWidgets.QToolButton(text="100%")
        self.chk_yolo = QtWidgets.QCheckBox("YOLO"); self.chk_yolo.setChecked(True)
        self.chk_faces = QtWidgets.QCheckBox("Faces"); self.chk_faces.setChecked(True)
        self.chk_pets = QtWidgets.QCheckBox("Pets"); self.chk_pets.setChecked(True)
        self.chk_tracks = QtWidgets.QCheckBox("Tracks"); self.chk_tracks.setChecked(True)
        self.lbl_info = QtWidgets.QLabel("—")

        self.btn_ptz = QtWidgets.QToolButton(text="PTZ"); self.btn_ptz.setCheckable(True)
        self.ptz_enabled = False

        top = QtWidgets.QHBoxLayout()
        for w in (self.btn_fit, self.btn_100, self.chk_yolo, self.chk_faces, self.chk_pets, self.chk_tracks, self.btn_ptz):
            top.addWidget(w)
        top.addStretch(1)
        top.addWidget(self.lbl_info)

        lay = QtWidgets.QVBoxLayout(self)
        lay.setContentsMargins(0, 0, 0, 0)
        lay.addLayout(top)
        lay.addWidget(self.view, 1)

        self.btn_fit.clicked.connect(self.view.fit_to_window)
        self.btn_100.clicked.connect(self.view.set_100)
        self.chk_yolo.toggled.connect(self._on_flags)
        self.chk_faces.toggled.connect(self._on_flags)
        self.chk_pets.toggled.connect(self._on_flags)
        self.chk_tracks.toggled.connect(self._on_flags)
        self.btn_ptz.toggled.connect(lambda on: setattr(self, "ptz_enabled", bool(on)))

        # Stream
        self.cap = StreamCapture(self.cam_cfg)
        self.cap.start()

        # Detector
        self.detector = DetectorThread(DetectorConfig.from_app(self.app_cfg), name=self.cam_cfg.name)
        self.detector.resultsReady.connect(self._on_detections)
        self.detector.start()

        # Recorder, presence, PTZ
        self.recorder = PrebufferRecorder(self.cam_cfg.name, self.app_cfg.output_dir,
                                          fps=25, pre_ms=self.app_cfg.prebuffer_ms)
        self.presence = PresenceBus(self.cam_cfg.name, self.app_cfg.logs_dir)
        self.ptz = PTZClient(self.cam_cfg)

        # UI timer
        self._ui_timer = QtCore.QTimer(self)
        self._ui_timer.timeout.connect(self._tick)
        self._ui_timer.start(33)

        # Context menu: edit/delete
        self.setContextMenuPolicy(QtCore.Qt.ContextMenuPolicy.CustomContextMenu)
        self.customContextMenuRequested.connect(self._context_menu)

    def closeEvent(self, e: QtGui.QCloseEvent):
        self.detector.stop()
        self.detector.wait(1500)
        self.cap.stop()
        self.recorder.close()
        super().closeEvent(e)

    def _context_menu(self, pos):
        menu = QtWidgets.QMenu(self)
        act_edit = menu.addAction("Edit Camera…")
        act_delete = menu.addAction("Delete Camera…")
        a = menu.exec(self.mapToGlobal(pos))
        if a is act_edit:
            self._edit_camera()
        elif a is act_delete:
            self._delete_camera()

    def _edit_camera(self):
        d = QtWidgets.QDialog(self)
        d.setWindowTitle(f"Edit Camera — {self.cam_cfg.name}")
        url = QtWidgets.QLineEdit(self.cam_cfg.stream_url)
        user = QtWidgets.QLineEdit(self.cam_cfg.user or "")
        pw = QtWidgets.QLineEdit(self.cam_cfg.password or ""); pw.setEchoMode(QtWidgets.QLineEdit.EchoMode.Password)
        token = QtWidgets.QLineEdit(self.cam_cfg.token or "")
        form = QtWidgets.QFormLayout(d)
        form.addRow("URL", url)
        form.addRow("User", user)
        form.addRow("Password", pw)
        form.addRow("Token", token)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.StandardButton.Ok |
                                          QtWidgets.QDialogButtonBox.StandardButton.Cancel)
        form.addRow(btns)
        btns.accepted.connect(d.accept); btns.rejected.connect(d.reject)
        if d.exec() != QtWidgets.QDialog.DialogCode.Accepted:
            return
        self.cam_cfg.stream_url = url.text().strip()
        self.cam_cfg.user = user.text().strip() or None
        self.cam_cfg.password = pw.text() or None
        self.cam_cfg.token = token.text().strip() or None
        save_settings(self.app_cfg)
        self.cap.stop()
        self.cap = StreamCapture(self.cam_cfg)
        self.cap.start()

    def _delete_camera(self):
        if QtWidgets.QMessageBox.question(self, "Delete camera",
                                          f"Remove '{self.cam_cfg.name}' from this layout?") != QtWidgets.QMessageBox.StandardButton.Yes:
            return
        self.app_cfg.cameras = [c for c in self.app_cfg.cameras if c.name != self.cam_cfg.name]
        save_settings(self.app_cfg)
        self.parentWidget().close()

    def _on_flags(self):
        self._overlay_flags = OverlayFlags(
            yolo=self.chk_yolo.isChecked(),
            faces=self.chk_faces.isChecked(),
            pets=self.chk_pets.isChecked(),
            tracks=self.chk_tracks.isChecked()
        )

    def _tick(self):
        ok, bgr, ts_ms = self.cap.read()
        if not ok or bgr is None:
            self.lbl_info.setText(f"{self.cap.last_backend}  zoom:{self.view.transform().m11():.2f}x")
            return
        self._last_bgr = bgr
        self.recorder.on_frame(bgr, ts_ms)
        self.detector.submit_frame(self.cam_cfg.name, bgr, ts_ms)
        qimg = qimage_from_bgr(bgr)
        self._last_qimage = qimg
        self._update_pixmap()

    def _update_pixmap(self):
        if self._last_qimage is None:
            return
        pix = QtGui.QPixmap.fromImage(self._last_qimage)
        if self._overlay_pkt is not None:
            p = QtGui.QPainter(pix)
            p.setRenderHint(QtGui.QPainter.RenderHint.Antialiasing, True)
            draw_overlays(p, self._overlay_pkt, self._overlay_flags)
            p.end()
        self._pix_item.setPixmap(pix)
        if self.view.transform().isIdentity():
            self.view.fit_to_window()
        det = f"det:{self._overlay_pkt.timing_ms['det']}ms" if self._overlay_pkt else "det:—"
        self.lbl_info.setText(f"{self.cap.last_backend}  {det}  zoom:{self.view.transform().m11():.2f}x")

    @QtCore.pyqtSlot(DetectionPacket)
    def _on_detections(self, pkt: DetectionPacket):
        if pkt.name != self.cam_cfg.name:
            return
        self._overlay_pkt = pkt
        self.presence.update(pkt)
        if self._last_bgr is not None:
            EnrollmentService.instance().on_detections(self.cam_cfg.name, self._last_bgr, pkt)
        self._update_pixmap()


# ---------- Main window ----------
class MainWindow(QtWidgets.QMainWindow):
    def __init__(self, app_cfg: AppSettings):
        super().__init__()
        self.app_cfg = app_cfg

        # Ensure models exist or fetch
        ModelManager.ensure_models(self, self.app_cfg)

        self.mdi = QtWidgets.QMdiArea()
        self.setCentralWidget(self.mdi)
        self._build_menus()
        self._load_initial_cameras()

        # Event log dock
        self.events = EventsPane(self.app_cfg.logs_dir, parent=self)
        dock = QtWidgets.QDockWidget("Events", self)
        dock.setWidget(self.events)
        dock.setObjectName("EventsDock")
        self.addDockWidget(QtCore.Qt.DockWidgetArea.RightDockWidgetArea, dock)

        self.statusBar().showMessage("Ready")

    def _build_menus(self):
        menubar = self.menuBar()

        m_file = menubar.addMenu("File")
        act_add_ip = m_file.addAction("Add Camera by IP…")
        act_add_url = m_file.addAction("Add Camera by URL…")
        m_file.addSeparator()
        act_save = m_file.addAction("Save Settings")
        m_file.addSeparator()
        act_exit = m_file.addAction("Exit")

        m_tools = menubar.addMenu("Tools")
        act_enroll = m_tools.addAction("Enrollment…")
        act_images = m_tools.addAction("Image Manager…")
        m_tools.addSeparator()
        act_models = m_tools.addAction("Open Models Folder")
        act_record = m_tools.addAction("Open Recordings Folder")
        act_logs = m_tools.addAction("Open Logs Folder")
        m_tools.addSeparator()
        act_fetch_models = m_tools.addAction("Download Default Models…")
        act_discover = m_tools.addAction("Discover ESP32-CAM…")
        act_rebuild_faces = m_tools.addAction("Rebuild Face Model from Disk…")

        m_view = menubar.addMenu("View")
        act_tile = m_view.addAction("Tile")
        act_cascade = m_view.addAction("Cascade")
        act_fit_all = m_view.addAction("Fit All")
        act_100_all = m_view.addAction("100% All")

        act_add_ip.triggered.connect(self._add_camera_ip_dialog)
        act_add_url.triggered.connect(self._add_camera_url_dialog)
        act_save.triggered.connect(lambda: save_settings(self.app_cfg))
        act_exit.triggered.connect(self.close)

        act_enroll.triggered.connect(self._open_enrollment)
        act_images.triggered.connect(self._open_image_manager)
        act_models.triggered.connect(lambda: open_folder_or_warn(self, self.app_cfg.models_dir))
        act_record.triggered.connect(lambda: open_folder_or_warn(self, self.app_cfg.output_dir))
        act_logs.triggered.connect(lambda: open_folder_or_warn(self, self.app_cfg.logs_dir))
        act_fetch_models.triggered.connect(lambda: ModelManager.fetch_defaults(self, self.app_cfg))
        act_discover.triggered.connect(self._discover_esp32)
        act_rebuild_faces.triggered.connect(self._rebuild_faces)

        tb = self.addToolBar("Main")
        tb.addAction("Add IP").triggered.connect(self._add_camera_ip_dialog)
        tb.addAction("Add URL").triggered.connect(self._add_camera_url_dialog)
        tb.addSeparator()
        tb.addAction("Tile").triggered.connect(self.mdi.tileSubWindows)
        tb.addAction("Cascade").triggered.connect(self.mdi.cascadeSubWindows)

        act_tile.triggered.connect(self.mdi.tileSubWindows)
        act_cascade.triggered.connect(self.mdi.cascadeSubWindows)
        act_fit_all.triggered.connect(self._fit_all)
        act_100_all.triggered.connect(self._100_all)

    def _load_initial_cameras(self):
        for c in self.app_cfg.cameras:
            self._add_camera(c)

    def _add_camera(self, cam_cfg: CameraSettings):
        w = CameraWidget(cam_cfg, self.app_cfg)
        sub = QtWidgets.QMdiSubWindow()
        sub.setWidget(w)
        sub.setAttribute(QtCore.Qt.WidgetAttribute.WA_DeleteOnClose, True)
        sub.setWindowTitle(cam_cfg.name)
        self.mdi.addSubWindow(sub)
        sub.resize(680, 540)
        sub.show()

    def _add_camera_ip_dialog(self):
        d = QtWidgets.QDialog(self)
        d.setWindowTitle("Add Camera by IP")
        ip = QtWidgets.QLineEdit()
        user = QtWidgets.QLineEdit(); user.setPlaceholderText("optional")
        pw = QtWidgets.QLineEdit(); pw.setPlaceholderText("optional"); pw.setEchoMode(QtWidgets.QLineEdit.EchoMode.Password)
        token = QtWidgets.QLineEdit(); token.setPlaceholderText("optional ?token=XYZ")
        name = QtWidgets.QLineEdit()
        name.setText(f"cam{len(self.app_cfg.cameras)+1}")
        form = QtWidgets.QFormLayout(d)
        form.addRow("IP or Host", ip)
        form.addRow("User", user)
        form.addRow("Password", pw)
        form.addRow("Token", token)
        form.addRow("Name", name)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.StandardButton.Ok |
                                          QtWidgets.QDialogButtonBox.StandardButton.Cancel)
        form.addRow(btns)
        btns.accepted.connect(d.accept); btns.rejected.connect(d.reject)
        if d.exec() != QtWidgets.QDialog.DialogCode.Accepted:
            return
        cam = CameraSettings.from_ip(
            name=name.text().strip() or f"cam{len(self.app_cfg.cameras)+1}",
            host=ip.text().strip(),
            user=user.text().strip() or None,
            password=pw.text() or None,
            token=token.text().strip() or None
        )
        self.app_cfg.cameras.append(cam)
        save_settings(self.app_cfg)
        self._add_camera(cam)

    def _add_camera_url_dialog(self):
        text, ok = QtWidgets.QInputDialog.getText(self, "Add camera by URL",
                                                  "rtsp/http URL (supports ?token=…):")
        if not ok or not text:
            return
        name = f"cam{len(self.app_cfg.cameras)+1}"
        cam = CameraSettings(name=name, stream_url=text.strip())
        self.app_cfg.cameras.append(cam)
        save_settings(self.app_cfg)
        self._add_camera(cam)

    def _fit_all(self):
        for sw in self.mdi.subWindowList():
            w: CameraWidget = sw.widget()
            w.view.fit_to_window()

    def _100_all(self):
        for sw in self.mdi.subWindowList():
            w: CameraWidget = sw.widget()
            w.view.set_100()

    def _open_enrollment(self):
        d = EnrollDialog(self.app_cfg, self)
        d.exec()

    def _open_image_manager(self):
        d = ImageManagerDialog(self.app_cfg, self)
        d.exec()

    def _discover_esp32(self):
        DiscoveryDialog(self).exec()


def main():
    app = QtWidgets.QApplication(sys.argv)

    # No explicit HiDPI attributes. Qt 6 handles this by default across versions.

    cfg = load_settings()
    win = MainWindow(cfg)
    win.resize(1400, 900)
    win.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
## `models.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\models.py
**Size**: 4241 bytes
**Modified**: 2025-11-06 23:31:06 +0000
**SHA256**: 9651d3d7354d692677e210e2ae99b43e56c3bda7c59c09cdb88b3be60e135a1e
``````python# models.py
# Uses settings.BASE_DIR-backed folders; unchanged logic with explicit BASE_DIR use.
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Tuple
import requests
from PyQt6 import QtWidgets
from settings import AppSettings, BASE_DIR
from utils import ensure_dir

YOLO_URL_DEFAULT = "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.onnx"
HAAR_URL_DEFAULT = "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"

@dataclass
class ModelPaths:
    yolo: Path
    haar: Path

class ModelManager:
    @staticmethod
    def paths(cfg: AppSettings) -> ModelPaths:
        return ModelPaths(
            yolo=Path(cfg.models_dir) / "yolov8n.onnx",
            haar=Path(cfg.models_dir) / "haarcascade_frontalface_default.xml"
        )

    @staticmethod
    def ensure_models(parent: QtWidgets.QWidget, cfg: AppSettings):
        p = ModelManager.paths(cfg)
        missing = []
        if not p.yolo.exists():
            missing.append(("YOLOv8n ONNX", p.yolo, cfg.yolo_url or YOLO_URL_DEFAULT))
        if not p.haar.exists():
            missing.append(("Haar face cascade", p.haar, cfg.haar_url or HAAR_URL_DEFAULT))
        if not missing:
            return
        mb = QtWidgets.QMessageBox(parent)
        mb.setWindowTitle("Models missing")
        mb.setText(f"Models folder:\n{cfg.models_dir}\n\nDownload defaults now?")
        mb.setStandardButtons(QtWidgets.QMessageBox.StandardButton.Yes | QtWidgets.QMessageBox.StandardButton.No)
        if mb.exec() == QtWidgets.QMessageBox.StandardButton.Yes:
            ModelManager._download_many(parent, missing)

    @staticmethod
    def fetch_defaults(parent: QtWidgets.QWidget, cfg: AppSettings):
        p = ModelManager.paths(cfg)
        todo = []
        if not p.yolo.exists():
            todo.append(("YOLOv8n ONNX", p.yolo, cfg.yolo_url or YOLO_URL_DEFAULT))
        if not p.haar.exists():
            todo.append(("Haar face cascade", p.haar, cfg.haar_url or HAAR_URL_DEFAULT))
        if not todo:
            QtWidgets.QMessageBox.information(parent, "Models", "All default models already present.")
            return
        ModelManager._download_many(parent, todo)

    @staticmethod
    def _download_many(parent: QtWidgets.QWidget, items: list[Tuple[str, Path, str]]):
        for label, path, url in items:
            ensure_dir(path.parent)
            prog = _DownloadDialog(parent, f"Downloading {label}…", url, path)
            ok = prog.exec_and_download()
            if not ok:
                QtWidgets.QMessageBox.warning(parent, "Download failed", f"Could not fetch {label}.\nURL: {url}")

class _DownloadDialog(QtWidgets.QDialog):
    def __init__(self, parent, title: str, url: str, path: Path):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.url = url
        self.path = path
        from PyQt6 import QtWidgets
        self.pb = QtWidgets.QProgressBar()
        self.lbl = QtWidgets.QLabel(url)
        self.btn = QtWidgets.QPushButton("Cancel")
        self.btn.clicked.connect(self.reject)
        lay = QtWidgets.QVBoxLayout(self)
        lay.addWidget(self.lbl); lay.addWidget(self.pb); lay.addWidget(self.btn)
        self._ok = False

    def exec_and_download(self) -> bool:
        from PyQt6 import QtWidgets
        try:
            with requests.get(self.url, stream=True, timeout=(5, 60)) as r:
                r.raise_for_status()
                total = int(r.headers.get("Content-Length", 0)) or None
                got = 0
                with self.path.open("wb") as fp:
                    for chunk in r.iter_content(chunk_size=65536):
                        if not chunk: continue
                        fp.write(chunk); got += len(chunk)
                        if total:
                            self.pb.setMaximum(total); self.pb.setValue(got)
                        QtWidgets.QApplication.processEvents()
            self._ok = True
        except Exception:
            self._ok = False
        self.accept()
        return self._ok
## `overlays.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\overlays.py
**Size**: 2344 bytes
**Modified**: 2025-11-06 23:17:11 +0000
**SHA256**: 4afa91730c898dd60dde701eef83dd6243bff224d15b63b05845754b3a32aa1a
``````python# overlays.py
# Overlay renderer with toggles and render order: boxes → labels → crosshair.
from __future__ import annotations
from dataclasses import dataclass
from typing import Tuple
from PyQt6 import QtGui, QtCore
from detectors import DetectionPacket, DetBox


@dataclass
class OverlayFlags:
    yolo: bool = True
    faces: bool = True
    pets: bool = True
    tracks: bool = True


def _pen(width: int, rgb: Tuple[int, int, int]) -> QtGui.QPen:
    p = QtGui.QPen(QtGui.QColor(*rgb))
    p.setWidth(width)
    return p


def _brush(rgb: Tuple[int, int, int], a: int) -> QtGui.QBrush:
    c = QtGui.QColor(*rgb)
    c.setAlpha(a)
    return QtGui.QBrush(c)


def draw_overlays(p: QtGui.QPainter, pkt: DetectionPacket, flags: OverlayFlags):
    if flags.yolo:
        for b in pkt.yolo:
            _draw_box(p, b, (0, 255, 0))
    if flags.faces:
        for b in pkt.faces:
            _draw_box(p, b, (0, 200, 255))
    if flags.pets:
        for b in pkt.pets:
            _draw_box(p, b, (255, 200, 0))

    if flags.yolo:
        for b in pkt.yolo:
            _draw_label(p, b.cls, b.score, b.xyxy, (0, 255, 0))
    if flags.faces:
        for b in pkt.faces:
            _draw_label(p, b.cls, b.score, b.xyxy, (0, 200, 255))
    if flags.pets:
        for b in pkt.pets:
            _draw_label(p, b.cls, b.score, b.xyxy, (255, 200, 0))

    # Crosshair at image center
    cx, cy = pkt.size[0] // 2, pkt.size[1] // 2
    p.setPen(_pen(1, (255, 255, 255)))
    p.drawLine(cx - 12, cy, cx + 12, cy)
    p.drawLine(cx, cy - 12, cx, cy + 12)


def _draw_box(p: QtGui.QPainter, b: DetBox, rgb):
    x1, y1, x2, y2 = b.xyxy
    rect = QtCore.QRectF(x1, y1, x2 - x1, y2 - y1)
    p.setPen(_pen(2, rgb))
    p.setBrush(QtCore.Qt.BrushStyle.NoBrush)
    p.drawRect(rect)


def _draw_label(p: QtGui.QPainter, name: str, score: float, xyxy, rgb):
    x1, y1, x2, y2 = xyxy
    text = f"{name} {score:.2f}"
    fm = QtGui.QFontMetrics(p.font())
    tw = fm.horizontalAdvance(text) + 6
    th = fm.height() + 4
    r = QtCore.QRectF(x1, y1 - th, tw, th)
    p.setPen(QtCore.Qt.PenStyle.NoPen)
    p.setBrush(_brush(rgb, 160))
    p.drawRect(r)
    p.setPen(_pen(1, (0, 0, 0)))
    p.drawText(r.adjusted(3, 0, -3, 0), QtCore.Qt.AlignmentFlag.AlignVCenter, text)
## `presence.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\presence.py
**Size**: 1689 bytes
**Modified**: 2025-11-07 00:06:33 +0000
**SHA256**: 8c11612563bfcf00fc021ae213c8a96bfbec1d3946ba3a22727478d19bde1b04
``````python# presence.py
# Uses YOLO or faces to generate events; prevents “no events” when YOLO is absent.
from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, Set
from detectors import DetectionPacket
from utils import ensure_dir

class PresenceBus:
    def __init__(self, cam_name: str, logs_dir: Path, ttl_ms: int = 2500):
        self.cam = cam_name
        self.logs_dir = Path(logs_dir)
        self.ttl = ttl_ms
        self.last_seen: Dict[str, int] = {}
        self.present: Set[str] = set()

    def update(self, pkt: DetectionPacket):
        now = pkt.ts_ms
        seen = set()
        # Prefer YOLO
        for b in pkt.yolo:
            if b.cls in ("person", "dog", "cat"):
                seen.add(b.cls)
        # Fallback: any face counts as a person presence
        if not seen and pkt.faces:
            seen.add("person")

        # update timestamps
        for k in seen:
            self.last_seen[k] = now

        # exit events
        for k in list(self.present):
            if now - self.last_seen.get(k, 0) > self.ttl:
                self.present.remove(k)
                self._write({"ts": now, "camera": self.cam, "event": "exit", "type": k})

        # enter events
        for k in seen:
            if k not in self.present:
                self.present.add(k)
                self._write({"ts": now, "camera": self.cam, "event": "enter", "type": k})

    def _write(self, rec: Dict):
        ensure_dir(self.logs_dir)
        f = self.logs_dir / f"{self.cam}.jsonl"
        with f.open("a", encoding="utf-8") as fp:
            fp.write(json.dumps(rec) + "\n")
## `ptz.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\ptz.py
**Size**: 251 bytes
**Modified**: 2025-11-06 23:17:30 +0000
**SHA256**: a029e548d69cd3d88155ee0d1caf7aa22dcea05156f54a94565d758ed895b34d
``````python# ptz.py
# PTZ client stub.
from __future__ import annotations
from settings import CameraSettings

class PTZClient:
    def __init__(self, cam: CameraSettings):
        self.cam = cam

    def nudge(self, dx: int, dy: int):
        return
## `recorder.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\recorder.py
**Size**: 1598 bytes
**Modified**: 2025-11-06 23:16:59 +0000
**SHA256**: 1ad88c24892a16a55ef377084ee670cff1fd1ecd62d3ff8eb99508b39c593c69
``````python# recorder.py
# Per-camera prebuffer recorder with flush-on-start.
from __future__ import annotations
from collections import deque
from pathlib import Path
import cv2 as cv
from typing import Optional
import numpy as np
from utils import timestamp_name, ensure_dir

class PrebufferRecorder:
    def __init__(self, cam_name: str, out_dir: Path, fps: int = 25, pre_ms: int = 3000):
        self.cam_name = cam_name
        self.out_dir = Path(out_dir)
        self.fps = fps
        self.pre_ms = pre_ms
        self.buf = deque()  # (ts_ms, bgr)
        self.writer: Optional[cv.VideoWriter] = None
        self.size = None

    def on_frame(self, bgr: np.ndarray, ts_ms: int):
        self.size = (bgr.shape[1], bgr.shape[0])
        self.buf.append((ts_ms, bgr.copy()))
        # Trim buffer
        while self.buf and ts_ms - self.buf[0][0] > self.pre_ms:
            self.buf.popleft()
        if self.writer is not None:
            self.writer.write(bgr)

    def start(self):
        if self.writer is not None:
            return
        ensure_dir(self.out_dir)
        fname = f"{self.cam_name}_{timestamp_name()}.avi"
        path = str(self.out_dir / fname)
        fourcc = cv.VideoWriter_fourcc(*"MJPG")
        self.writer = cv.VideoWriter(path, fourcc, self.fps, self.size)
        for _, b in self.buf:
            self.writer.write(b)

    def stop(self):
        if self.writer is None:
            return
        self.writer.release()
        self.writer = None

    def close(self):
        self.stop()
        self.buf.clear()
## `refactor/ai_viewer/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\__init__.py
**Size**: 20 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: bd2bb7253368725b542a933b07fae240fa726465b550779548acb0c2c6679956
``````python# ai_viewer package
## `refactor/ai_viewer/ai/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\__init__.py
**Size**: 10 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 527c668d5ae4f9e34e46c8ad88ce3c5b4a41eb27ec684ace6a4f11621fc42edf
``````python# package
## `refactor/ai_viewer/ai/detectors/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\detectors\__init__.py
**Size**: 37 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: d1f3931c3245e751d1b160958c97a17f72527f8fc41c399e9022cd46f361f583
``````python# package
__all__ = ["YOLODetector"]
## `refactor/ai_viewer/ai/detectors/yolo.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\detectors\yolo.py
**Size**: 2760 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 5d27d2177f452b6abbddaa129b927130f40eb8c2130ba7f61e25654f8355bf38
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class YOLODetector:
    """Lightweight YOLO (ONNX) wrapper for COCO classes.
    Uses OpenCV DNN and letterbox preprocessing.
    """
    COCO = [
        'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light',
        'fire hydrant','stop sign','parking meter','bench','bird','cat','dog','horse','sheep','cow','elephant',
        'bear','zebra','giraffe','backpack','umbrella','handbag','tie','suitcase','frisbee','skis','snowboard',
        'sports ball','kite','baseball bat','baseball glove','skateboard','surfboard','tennis racket','bottle',
        'wine glass','cup','fork','knife','spoon','bowl','banana','apple','sandwich','orange','broccoli',
        'carrot','hot dog','pizza','donut','cake','chair','couch','potted plant','bed','dining table','toilet',
        'tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven','toaster','sink','refrigerator',
        'book','clock','vase','scissors','teddy bear','hair drier','toothbrush'
    ]
    def __init__(self, model_path='ai/models/yolo.onnx', input_size=640, conf=0.35, iou=0.45):
        self.net = None
        self.size = input_size
        self.conf = conf
        self.iou = iou
        if os.path.exists(model_path):
            try:
                self.net = cv2.dnn.readNetFromONNX(model_path)
                self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
                self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            except Exception as e:
                print('[YOLO] load failed:', e)

## `refactor/ai_viewer/ai/recognition/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\recognition\__init__.py
**Size**: 41 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: ed734539c933a3a34bb9e541e1c4e6b33521f993798ff7fa693338129d736a44
``````python# package
__all__ = ["FaceDB", "PetsDB"]
## `refactor/ai_viewer/ai/recognition/face_db.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\recognition\face_db.py
**Size**: 2020 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: b367e6421a6114fa7ebb065523127dd75d8159d8f26f3cb3680988f4f7162c0a
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class FaceDB:
    """Face recognition store.
    - Trains LBPH if contrib available; falls back to ORB matching.
    - Persists samples as images on disk under ai/data/faces/<name>.
    """
    def __init__(self, base='ai/data/faces'):
        self.base = base
        os.makedirs(self.base, exist_ok=True)
        self.size = (160,160)
        self.model = None
        try:
            self.model = cv2.face.LBPHFaceRecognizer_create(radius=1, neighbors=8, grid_x=8, grid_y=8)
        except Exception:
            self.model = None
        self.labels=[]
        self.orb = cv2.ORB_create(nfeatures=1000)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db_descs={}
        self.cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

## `refactor/ai_viewer/ai/recognition/pets_db.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ai\recognition\pets_db.py
**Size**: 1700 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: d10c458f63d26b5a805f3d5d42b7ef9b1b895f37382c58a1888673880179b2b7
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class PetsDB:
    """Pet recognition store (dogs/cats) using ORB descriptors.
    - Persists samples as grayscale images under ai/data/pets/{dogs,cats}/<name>.
    """
    def __init__(self, base='ai/data/pets'):
        self.base=base; os.makedirs(os.path.join(base,'dogs'),exist_ok=True); os.makedirs(os.path.join(base,'cats'),exist_ok=True)
        self.orb=cv2.ORB_create(nfeatures=1000); self.bf=cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        self.db={'dogs':{},'cats':{}}

## `refactor/ai_viewer/core/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\__init__.py
**Size**: 76 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 70f8d242b6a8f542db5033d778d462e9609cfd4d5d7b23c8853cfbf6089a18c7
``````python# package
__all__ = ["CameraConfig", "CameraStreamThread", "SimpleTracker"]
## `refactor/ai_viewer/core/config.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\config.py
**Size**: 1421 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 8c80b427dd7de971e81acdab4074924191376fb2f529114b1c4be167a5162a82
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class CameraConfig:
    name: str
    host: str                 # ip[:port] for port 80
    user: Optional[str] = None
    password: Optional[str] = None
    token: Optional[str] = None  # Base64 of user:pass

## `refactor/ai_viewer/core/stream.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\stream.py
**Size**: 1330 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: e58383c6d94d46a0701e665731c7882be89ee50d0ae30f7d4d2bdc3a88f3a588
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class CameraStreamThread(QtCore.QThread):
    frameReady = QtCore.Signal(np.ndarray, float)  # (bgr_frame, timestamp)

## `refactor/ai_viewer/core/tracking.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\core\tracking.py
**Size**: 1513 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 6a32426fb44dd6b178ee556aeed2eb9cfe454b1daf4929a758ecc307590df105
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class SimpleTracker:
    """Very simple IOU-based tracker with TTL for object permanence.
    Tracks: list of dicts with bbox, cls, last_ts.
    """
    def __init__(self, ttl: float = 1.0, iou_thresh: float = 0.3):
        self.ttl = ttl
        self.iou_thresh = iou_thresh
        self.tracks = []

## `refactor/ai_viewer/tools/dupes.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\tools\dupes.py
**Size**: 3486 bytes
**Modified**: 2025-09-01 00:09:02 +0100
**SHA256**: dd5746ceb3a99a54a34531a4516e26cf30aeaed8216a18a2bd670856dfe67385
``````python"""Utility tools for the Qt MDI app.
 - Image ingestion from a source directory into face/pet stores
 - Simple near-duplicate culling via average-hash (aHash)
"""
from __future__ import annotations
import os
import time
from typing import Tuple
import numpy as np
import cv2


def ingest_images(src_dir: str, dest_dir: str, size: Tuple[int,int], gray: bool=False) -> int:
    os.makedirs(dest_dir, exist_ok=True)
    count=0
    for root,_,files in os.walk(src_dir):
        for fn in files:
            if not fn.lower().endswith(('.jpg','.jpeg','.png')):
                continue
            try:
                img=cv2.imread(os.path.join(root,fn))
                if img is None: continue
                if gray:
                    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
                img=cv2.resize(img, size)
                now=time.time(); ms=int((now-int(now))*1000); ds=time.strftime('%Y%m%d_%H%M%S', time.localtime(now))
                out=os.path.join(dest_dir, f'{ds}_{ms:03d}.jpg')
                cv2.imwrite(out, img)
                count+=1
            except Exception:
                pass
    return count


def cull_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4) -> int:
    """Remove near-duplicate images in a folder using aHash similarity.
    Returns number of removed files.
    """
    if not os.path.isdir(target):
        return 0
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; removed=0
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for fp in files:
        h=ahash(fp)
        if h is None: continue
        dup=False
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                try:
                    os.remove(fp); removed+=1
                except Exception:
                    pass
                dup=True
                break
        if not dup:
            hashes.append(h)
    return removed


def find_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4):
    """Analyze a directory for near-duplicates by aHash.
    Returns (files:list[str], remove_indices:set[int]) where remove_indices
    contains indices in files suggested for deletion.
    """
    if not os.path.isdir(target):
        return [], set()
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; remove=set()
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for idx, fp in enumerate(files):
        h=ahash(fp)
        if h is None: continue
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                remove.add(idx)
                break
        else:
            hashes.append(h)
    return files, remove
## `refactor/ai_viewer/tools/gallery.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\tools\gallery.py
**Size**: 2563 bytes
**Modified**: 2025-09-01 00:09:02 +0100
**SHA256**: ed4c0bcf1225b6ccd0d3c7cd68b72431a3de9b44b730b25278901cce8f198aaf
``````python"""Qt thumbnail gallery dialog for selecting and deleting images."""
from __future__ import annotations
import os
from PySide6 import QtCore, QtGui, QtWidgets


class GalleryDialog(QtWidgets.QDialog):
    def __init__(self, dir_path: str, title: str, parent=None):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.resize(900, 600)
        self.dir_path = dir_path
        v = QtWidgets.QVBoxLayout(self)
        self.list = QtWidgets.QListWidget()
        self.list.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.list.setIconSize(QtCore.QSize(160, 120))
        self.list.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.list.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        v.addWidget(self.list, 1)
        btns = QtWidgets.QDialogButtonBox()
        self.btn_del = btns.addButton('Delete Selected', QtWidgets.QDialogButtonBox.ButtonRole.ActionRole)
        self.btn_close = btns.addButton(QtWidgets.QDialogButtonBox.StandardButton.Close)
        v.addWidget(btns)
        self.btn_del.clicked.connect(self.do_delete)
        self.btn_close.clicked.connect(self.accept)
        self.populate()

    def populate(self):
        self.list.clear()
        if not os.path.isdir(self.dir_path):
            return
        files = [os.path.join(self.dir_path,f) for f in os.listdir(self.dir_path)
                 if f.lower().endswith(('.jpg','.jpeg','.png'))]
        files.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        for fp in files:
            item = QtWidgets.QListWidgetItem(os.path.basename(fp))
            try:
                pix = QtGui.QPixmap(fp)
                if not pix.isNull():
                    item.setIcon(QtGui.QIcon(pix.scaled(160,120, QtCore.Qt.KeepAspectRatio, QtCore.Qt.SmoothTransformation)))
            except Exception:
                pass
            item.setData(QtCore.Qt.ItemDataRole.UserRole, fp)
            self.list.addItem(item)

    def do_delete(self):
        items = self.list.selectedItems()
        if not items:
            return
        if QtWidgets.QMessageBox.question(self, 'Delete', f'Delete {len(items)} images?') != QtWidgets.QMessageBox.Yes:
            return
        cnt=0
        for it in items:
            fp = it.data(QtCore.Qt.ItemDataRole.UserRole)
            try:
                os.remove(fp)
                cnt+=1
            except Exception:
                pass
        self.populate()
        QtWidgets.QMessageBox.information(self, 'Manage', f'Deleted {cnt} files')

## `refactor/ai_viewer/ui/__init__.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ui\__init__.py
**Size**: 141 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 5da5f6efc2ba7bb8642e5b42a8312221dbffab2d1dc1e25b4aec16c1b0cb10b8
``````python# package
__all__ = ["AddCameraDialog", "CameraWidget", "CollectionDialog", "CullDialog", "DetectionThread", "MainWindow", "SettingsDialog"]
## `refactor/ai_viewer/ui/camera_widget.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ui\camera_widget.py
**Size**: 3104 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: da11cd263950e3b6d678e68d84fba65c2c86dfec988d748df6563fd494a57b6d
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class CollectionDialog(QtWidgets.QDialog):
    stopClicked = QtCore.Signal()
    def __init__(self, parent=None, title: str = 'Collection'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(False)
        self.setWindowFlag(QtCore.Qt.WindowType.WindowStaysOnTopHint, True)
        self.resize(360, 120)
        v = QtWidgets.QVBoxLayout(self)
        self.lbl = QtWidgets.QLabel('Starting...')
        self.lbl.setWordWrap(True)
        v.addWidget(self.lbl, 1)
        btns = QtWidgets.QDialogButtonBox()
        self.btn_stop = btns.addButton('Stop', QtWidgets.QDialogButtonBox.ActionRole)
        self.btn_close = btns.addButton(QtWidgets.QDialogButtonBox.Close)
        v.addWidget(btns)
        self.btn_stop.clicked.connect(lambda: self.stopClicked.emit())
        self.btn_close.clicked.connect(self.accept)
    def set_status(self, text: str):
        try:
            self.lbl.setText(text)
        except Exception:
            pass

class CameraWidget(QtWidgets.QWidget):
    closed = QtCore.Signal(dict)
    eventLogged = QtCore.Signal(str)
    def __init__(self, cfg: CameraConfig, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.setWindowTitle(f"{cfg.name} [{cfg.host}]")
        self.label = QtWidgets.QLabel('Connecting…')
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setMinimumSize(320, 240)
        self.label.setStyleSheet('background:#000; color:#9cf;')

class DetectionThread(QtCore.QThread):
    resultsReady = QtCore.Signal(list, list)  # dets, faces
    def __init__(self, widget: CameraWidget, interval_ms: int = 200):
        super().__init__(widget)
        self.w = widget
        self.interval = interval_ms
        self._stop = threading.Event()
        self._skip_cycles = 0
        self.max_skip_cycles = 1  # skip this many cycles if tracks are active

## `refactor/ai_viewer/ui/windows.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\ai_viewer\ui\windows.py
**Size**: 7148 bytes
**Modified**: 2025-09-01 00:09:01 +0100
**SHA256**: 81fb94986bf50f786583c28d25cf99199e10e398ed012f96a5ff828416a432d5
``````python#!/usr/bin/env python3
"""
ESP32-CAM MDI Viewer (Qt)

Multi-document interface (MDI) master application to manage multiple
ESP32-CAM feeds as independent, floating, resizable windows inside a
single main window. Includes a standard toolbar with camera management
and recording controls. Supports basic MJPEG streaming with optional
Basic-Auth or token, plus pre-buffered video capture per camera.

Dependencies (install on your PC):
  - pip install PySide6 requests opencv-python numpy

Notes:
  - This is a first pass skeleton designed to get the MDI scaffolding,
    multi-camera streaming, and pre-buffered recording in place.
  - Face/pet recognition and the advanced UI from cam_ai.py can be
    integrated in phased steps by adding overlays and per-camera tool
    panels.
"""

from __future__ import annotations
import os
import sys
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import Optional, Deque, Tuple

import requests
import numpy as np
import cv2

from PySide6 import QtCore, QtGui, QtWidgets
import sys as _sys
_sys.path.append(os.path.dirname(__file__))  # allow local module imports
from gallery import GalleryDialog
import tools



class AddCameraDialog(QtWidgets.QDialog):
    def __init__(self, parent=None, initial: Optional[CameraConfig]=None, title: str='Add Camera'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(True)
        form = QtWidgets.QFormLayout(self)
        self.ed_name = QtWidgets.QLineEdit(initial.name if initial else 'Camera')
        self.ed_host = QtWidgets.QLineEdit(initial.host if initial else '192.168.1.100')
        self.ed_user = QtWidgets.QLineEdit(initial.user or '' if initial else '')
        self.ed_pass = QtWidgets.QLineEdit(initial.password or '' if initial else '')
        self.ed_pass.setEchoMode(QtWidgets.QLineEdit.Password)
        self.ed_token = QtWidgets.QLineEdit(initial.token or '' if initial else '')
        form.addRow('Name', self.ed_name)
        form.addRow('Host (ip[:port])', self.ed_host)
        form.addRow('User', self.ed_user)
        form.addRow('Password', self.ed_pass)
        form.addRow('Token (Base64 user:pass)', self.ed_token)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept)
        btns.rejected.connect(self.reject)
        form.addRow(btns)

class SettingsDialog(QtWidgets.QDialog):
    """App settings: detector interval, aHash grid size + Hamming, PTZ aim timing, deadzone."""
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle('Settings')
        self.resize(420, 240)
        form = QtWidgets.QFormLayout(self)
        self.s_det_interval = QtWidgets.QSpinBox(); self.s_det_interval.setRange(50, 2000); self.s_det_interval.setSingleStep(50); self.s_det_interval.setSuffix(' ms')
        self.s_hash_size = QtWidgets.QSpinBox(); self.s_hash_size.setRange(4, 16)
        self.s_hamming = QtWidgets.QSpinBox(); self.s_hamming.setRange(0, 32)
        self.s_ptz_interval = QtWidgets.QSpinBox(); self.s_ptz_interval.setRange(100, 2000); self.s_ptz_interval.setSingleStep(50); self.s_ptz_interval.setSuffix(' ms')
        self.s_deadzone = QtWidgets.QSpinBox(); self.s_deadzone.setRange(2, 20); self.s_deadzone.setSuffix(' %')
        form.addRow('Detector interval', self.s_det_interval)
        form.addRow('aHash grid size', self.s_hash_size)
        form.addRow('Hamming threshold', self.s_hamming)
        form.addRow('PTZ aim interval', self.s_ptz_interval)
        form.addRow('PTZ deadzone', self.s_deadzone)
        btns = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept); btns.rejected.connect(self.reject)
        form.addRow(btns)

class CullDialog(QtWidgets.QDialog):
    """Preview duplicates with highlight before deletion.
    Includes a tolerance slider to adjust the near-duplicate matching threshold.
    Lower values are stricter; higher tolerate more difference.
    """
    def __init__(self, dir_path: str, files: list[str], remove: set[int], title: str, parent=None):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.resize(1000, 700)
        self.dir_path = dir_path
        self.files = files or []
        self.remove = set(remove or set())
        # Matching parameters (aHash on 8x8 grid → Hamming distance 0..64).
        # Practical duplicate tolerance range ~0..16; start at 4 by default.
        self.hash_size = 8
        self.thresh = 4
        v = QtWidgets.QVBoxLayout(self)
        self.view = QtWidgets.QListWidget()
        self.view.setViewMode(QtWidgets.QListView.ViewMode.IconMode)
        self.view.setIconSize(QtCore.QSize(160, 120))
        self.view.setResizeMode(QtWidgets.QListView.ResizeMode.Adjust)
        self.view.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
        v.addWidget(self.view, 1)
        # Controls
        h = QtWidgets.QHBoxLayout()
        # Tolerance slider block
        tol_box = QtWidgets.QHBoxLayout()
        self.lbl_tol = QtWidgets.QLabel('Tolerance:')
        self.sld_tol = QtWidgets.QSlider(QtCore.Qt.Orientation.Horizontal)
        self.sld_tol.setMinimum(0)
        self.sld_tol.setMaximum(16)
        self.sld_tol.setValue(self.thresh)
        self.sld_tol.setTickInterval(1)
        self.sld_tol.setSingleStep(1)
        self.lbl_tol_val = QtWidgets.QLabel(str(self.thresh))
        tol_box.addWidget(self.lbl_tol)
        tol_box.addWidget(self.sld_tol)
        tol_box.addWidget(self.lbl_tol_val)
        tol_box_w = QtWidgets.QWidget(); tol_box_w.setLayout(tol_box)
        h.addWidget(tol_box_w, 2)
        # Info + actions
        self.lbl_info = QtWidgets.QLabel()
        h.addWidget(self.lbl_info, 1)
        self.btn_confirm = QtWidgets.QPushButton('Confirm Delete')
        self.btn_cancel = QtWidgets.QPushButton('Cancel')
        h.addWidget(self.btn_confirm)
        h.addWidget(self.btn_cancel)
        v.addLayout(h)
        self.btn_confirm.clicked.connect(self.on_confirm)
        self.btn_cancel.clicked.connect(self.reject)
        self.sld_tol.valueChanged.connect(self.on_tol_changed)
        self.populate()

class MainWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle('ESP32-CAM MDI')
        self.resize(1200, 800)
        self.mdi = QtWidgets.QMdiArea()
        self.setCentralWidget(self.mdi)
        # Events sidebar (shows events for active camera)
        self.eventsDock = QtWidgets.QDockWidget('Events', self)
        self.eventsView = QtWidgets.QListWidget()
        self.eventsDock.setWidget(self.eventsView)
        self.addDockWidget(QtCore.Qt.RightDockWidgetArea, self.eventsDock)
        self._active_cam_ref = None
        try:
            self.mdi.subWindowActivated.connect(self.on_sub_activated)
        except Exception:
            pass


def main():
    app = QtWidgets.QApplication(sys.argv)
    mw = MainWindow()
    mw.show()
    sys.exit(app.exec())
## `refactor/run_ai_viewer.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\refactor\run_ai_viewer.py
**Size**: 103 bytes
**Modified**: 2025-09-01 00:17:07 +0100
**SHA256**: 11addfebb0dd30e9a9f30d0f362b8a4a2c09ae4af43e9800d3c52c6eb03fe648
``````python#!/usr/bin/env python3
from ai_viewer.ui.windows import main

if __name__ == "__main__":
    main()## `settings.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\settings.py
**Size**: 3700 bytes
**Modified**: 2025-11-06 23:30:13 +0000
**SHA256**: dc930a86e71a204d5f28abe3fc762b69886c53575edc819fcc26b5d2cd39ed1b
``````python# settings.py
# Base-path aware settings. Paths are anchored to the AI folder (this file's parent).
from __future__ import annotations
from dataclasses import dataclass, field, asdict
from typing import List, Optional
from pathlib import Path
import json
import os

# --- Project base directory (…/ESP32_CAM_AI/AI) ---
BASE_DIR = Path(__file__).resolve().parent

SETTINGS_FILE = BASE_DIR / "config" / "app_settings.json"

def _abs_under_base(p: Path) -> Path:
    # Make absolute under BASE_DIR if relative
    if not p.is_absolute():
        p = BASE_DIR / p
    return p

@dataclass
class CameraSettings:
    name: str
    stream_url: str
    user: Optional[str] = None
    password: Optional[str] = None
    token: Optional[str] = None

    @classmethod
    def from_ip(cls, name: str, host: str, user: Optional[str] = None,
                password: Optional[str] = None, token: Optional[str] = None):
        host = host.strip()
        base = f"http://{host}:81/stream"
        if token:
            sep = "&" if "?" in base else "?"
            base = f"{base}{sep}token={token}"
        return cls(name=name, stream_url=base, user=user, password=password, token=token)

    def effective_url(self) -> str:
        return self.stream_url

@dataclass
class AppSettings:
    models_dir: Path = Path("models")
    output_dir: Path = Path("recordings")
    logs_dir: Path = Path("logs")
    detect_interval_ms: int = 100
    thresh_yolo: float = 0.35
    prebuffer_ms: int = 3000
    yolo_url: Optional[str] = None
    haar_url: Optional[str] = None
    cameras: List[CameraSettings] = field(default_factory=list)

def load_settings() -> AppSettings:
    if SETTINGS_FILE.exists():
        with SETTINGS_FILE.open("r", encoding="utf-8") as fp:
            raw = json.load(fp)
        cams = [CameraSettings(**c) for c in raw.get("cameras", [])]
        cfg = AppSettings(
            models_dir=Path(raw.get("models_dir", "models")),
            output_dir=Path(raw.get("output_dir", "recordings")),
            logs_dir=Path(raw.get("logs_dir", "logs")),
            detect_interval_ms=int(raw.get("detect_interval_ms", 100)),
            thresh_yolo=float(raw.get("thresh_yolo", 0.35)),
            prebuffer_ms=int(raw.get("prebuffer_ms", 3000)),
            yolo_url=raw.get("yolo_url"),
            haar_url=raw.get("haar_url"),
            cameras=cams
        )
    else:
        cfg = AppSettings()

    # Anchor to BASE_DIR
    cfg.models_dir = _abs_under_base(cfg.models_dir)
    cfg.output_dir = _abs_under_base(cfg.output_dir)
    cfg.logs_dir   = _abs_under_base(cfg.logs_dir)
    return cfg

def save_settings(cfg: AppSettings):
    SETTINGS_FILE.parent.mkdir(parents=True, exist_ok=True)
    data = {
        # store as relative-to-base where possible for portability
        "models_dir": str(Path(cfg.models_dir).resolve().relative_to(BASE_DIR) if str(cfg.models_dir).startswith(str(BASE_DIR)) else str(cfg.models_dir)),
        "output_dir": str(Path(cfg.output_dir).resolve().relative_to(BASE_DIR) if str(cfg.output_dir).startswith(str(BASE_DIR)) else str(cfg.output_dir)),
        "logs_dir":   str(Path(cfg.logs_dir).resolve().relative_to(BASE_DIR) if str(cfg.logs_dir).startswith(str(BASE_DIR)) else str(cfg.logs_dir)),
        "detect_interval_ms": cfg.detect_interval_ms,
        "thresh_yolo": cfg.thresh_yolo,
        "prebuffer_ms": cfg.prebuffer_ms,
        "yolo_url": cfg.yolo_url,
        "haar_url": cfg.haar_url,
        "cameras": [asdict(c) for c in cfg.cameras],
    }
    with SETTINGS_FILE.open("w", encoding="utf-8") as fp:
        json.dump(data, fp, indent=2)
## `src/app/main.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\app\main.py
**Size**: 277 bytes
**Modified**: 2025-10-23 18:14:11 +0100
**SHA256**: d2ffb5966caa86a3a6f98413d3b34abc01ff6cb21cb18eab04ce67c8a4e435f7
``````python# app/main.py

import sys
from PySide6.QtWidgets import QApplication
from ui.main_window import MainWindow

def main() -> None:
    app = QApplication(sys.argv)
    win = MainWindow()
    win.show()
    sys.exit(app.exec())

if __name__ == "__main__":
    main()
## `src/camera/detection.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\camera\detection.py
**Size**: 524 bytes
**Modified**: 2025-08-31 22:27:27 +0100
**SHA256**: e795dbcb175265732d159cbc36342c44f9031a236c7ecdf53ae24386d0767b6a
``````python# camera/detection.py

from PySide6 import QtCore
import threading
import time
from typing import List, Tuple

class DetectionThread(QtCore.QThread):
    resultsReady = QtCore.Signal(list, list)  # dets, faces

    def __init__(self, widget: 'CameraWidget', interval_ms: int = 200, parent=None):
        super().__init__(parent)
        self.w = widget
        self.interval = interval_ms
        self._stop = threading.Event()
        self.max_skip_cycles = 1

    # … (same implementation as before) …## `src/camera/stream.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\camera\stream.py
**Size**: 787 bytes
**Modified**: 2025-08-31 22:26:17 +0100
**SHA256**: f1cca6c7fbc7ca45ae31f8286ee852baf66b3bfd98161e76bf71dfccb0cb57f0
``````python# camera/stream.py

import requests
import numpy as np
import cv2
from collections import deque
from typing import Deque, Tuple
from PySide6 import QtCore
import os
import time
import threading

class CameraStreamThread(QtCore.QThread):
    frameReady = QtCore.Signal(np.ndarray, float)  # (frame, ts)

    def __init__(self, cfg: CameraConfig, prebuffer_seconds: float = 5.0, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.prebuffer: Deque[Tuple[np.ndarray, float]] = deque(
            maxlen=int(prebuffer_seconds * 20))
        self._stop = threading.Event()
        self._session = None
        self._resp = None
        self._buf = bytearray()

    # … (same implementation as before, only moved into its own file) …## `src/ui/dialogs.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\ui\dialogs.py
**Size**: 1750 bytes
**Modified**: 2025-08-31 22:28:02 +0100
**SHA256**: 1a2657e57dbc90eae2474486a75e8782cd0341d44848ad8176ef0ed27ca78224
``````python# ui/dialogs.py

from PySide6 import QtWidgets
from ..core.config import CameraConfig

class AddCameraDialog(QtWidgets.QDialog):
    def __init__(self, parent=None, initial: CameraConfig | None = None, title: str = 'Add Camera'):
        super().__init__(parent)
        self.setWindowTitle(title)
        self.setModal(True)
        form = QtWidgets.QFormLayout(self)

        self.ed_name   = QtWidgets.QLineEdit(initial.name if initial else 'Camera')
        self.ed_host   = QtWidgets.QLineEdit(initial.host if initial else '192.168.1.100')
        self.ed_user   = QtWidgets.QLineEdit(initial.user or '')
        self.ed_pass   = QtWidgets.QLineEdit(initial.password or '')
        self.ed_pass.setEchoMode(QtWidgets.QLineEdit.Password)
        self.ed_token  = QtWidgets.QLineEdit(initial.token or '')

        form.addRow('Name',   self.ed_name)
        form.addRow('Host',   self.ed_host)
        form.addRow('User',   self.ed_user)
        form.addRow('Pass',   self.ed_pass)
        form.addRow('Token',  self.ed_token)

        btns = QtWidgets.QDialogButtonBox(
            QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel)
        btns.accepted.connect(self.accept)
        btns.rejected.connect(self.reject)
        form.addRow(btns)

    def get_config(self) -> CameraConfig | None:
        if self.exec() == QtWidgets.QDialog.Accepted:
            return CameraConfig(
                name=self.ed_name.text().strip() or 'Camera',
                host=self.ed_host.text().strip(),
                user=self.ed_user.text().strip() or None,
                password=self.ed_pass.text(),
                token=self.ed_token.text().strip() or None
            )
        return None## `src/ui/widgets.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\src\ui\widgets.py
**Size**: 4345 bytes
**Modified**: 2025-08-31 22:25:06 +0100
**SHA256**: b8766fc67b3d30facea44bca78951575f272233906d5af34c68deaf37fedd132
``````python# ui/widgets.py

from PySide6 import QtCore, QtGui, QtWidgets
import cv2
import numpy as np
import os
import time
from ..ai.yolo import YOLODetector
from ..ai.facedb import FaceDB
from ..ai.petsdb import PetsDB
from ..ai.tracker import SimpleTracker
from ..camera.detection import DetectionThread
from ..camera.stream import CameraStreamThread
from ..core.config import CameraConfig
from ..core.settings_dialog import SettingsDialog

class CameraWidget(QtWidgets.QWidget):
    """Widget that displays a single ESP32‑CAM feed and handles all per‑camera UI."""
    closed = QtCore.Signal(dict)          # emitted on close
    eventLogged = QtCore.Signal(str)      # emits a log line

    def __init__(self, cfg: CameraConfig, parent=None):
        super().__init__(parent)
        self.cfg = cfg
        self.setWindowTitle(f"{cfg.name} [{cfg.host}]")
        self._setup_ui()
        self._init_ai_components()
        self._init_threads()
        self._recording = False
        self._writer = None
        self._aim_timer = QtCore.QTimer(self)
        self._aim_timer.setInterval(300)          # PTZ aim interval
        self._aim_timer.timeout.connect(self._aim_at_target)

    # ------------------------------------------------------------------
    #  UI set‑up (toolbar, labels, etc.)
    # ------------------------------------------------------------------
    def _setup_ui(self):
        self.lbl = QtWidgets.QLabel('Connecting…')
        self.lbl.setAlignment(QtCore.Qt.AlignCenter)
        self.lbl.setMinimumSize(320, 240)
        self.lbl.setStyleSheet('background:#000; color:#9cf;')

        toolbar = QtWidgets.QToolBar()
        toolbar.setMovable(False)
        toolbar.addAction('Start', self.start_stream)
        toolbar.addAction('Stop',  self.stop_stream)
        toolbar.addSeparator()
        toolbar.addAction('Start Rec', self.start_recording)
        toolbar.addAction('Stop Rec',  self.stop_recording)

        # AI toggles
        self.chk_yolo = QtWidgets.QCheckBox('YOLO')
        self.chk_face = QtWidgets.QCheckBox('Face')
        self.chk_dogid = QtWidgets.QCheckBox('Dog ID')
        ai_btn = QtWidgets.QToolButton()
        ai_btn.setText('AI')
        ai_btn.setPopupMode(QtWidgets.QToolButton.InstantPopup)
        ai_menu = QtWidgets.QMenu(ai_btn)
        ai_menu.addAction('YOLO', self.chk_yolo.toggle)
        ai_menu.addAction('Face', self.chk_face.toggle)
        ai_menu.addAction('Dog ID', self.chk_dogid.toggle)
        ai_btn.setMenu(ai_menu)

        self.lbl_status = QtWidgets.QLabel('Ready')
        self.lbl_status.setStyleSheet('color:#246; padding:2px 4px;')

        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(toolbar)
        layout.addWidget(self.lbl, 1)
        layout.setContentsMargins(4,4,4,4)

    # ------------------------------------------------------------------
    #  AI / DB helpers
    # ------------------------------------------------------------------
    def _init_ai_components(self):
        self.yolo   = YOLODetector()
        self.facedb = FaceDB()
        self.facedb.load()
        self.pets   = PetsDB()
        self.pets.load()
        self.tracker = SimpleTracker(ttl=1.0)

    # ------------------------------------------------------------------
    #  Threads
    # ------------------------------------------------------------------
    def _init_threads(self):
        self.stream_thread = CameraStreamThread(self.cfg)
        self.stream_thread.frameReady.connect(self._on_frame)
        self.stream_thread.start()

        self.det_thr = DetectionThread(self)
        self.det_thr.resultsReady.connect(self._on_results)
        self.det_thr.start()

    # ------------------------------------------------------------------
    #  Public API (mostly UI actions)
    # ------------------------------------------------------------------
    def start_stream(self):
        if not self.stream_thread.isRunning():
            self.stream_thread._stop.clear()
            self.stream_thread.start()

    def stop_stream(self):
        if self.stream_thread.isRunning():
            self.stream_thread.stop()
            self.stream_thread.wait(700)   # graceful shutdown

    # … (recording, enrollment, collection, etc.) …## `stream.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\stream.py
**Size**: 5443 bytes
**Modified**: 2025-11-06 23:17:15 +0000
**SHA256**: dbae9ee2ff04f4c0f100ea67dab8aa818763de5e2ba75d5d9f9ac6a360116418
``````python# stream.py
# Unified capture with robust timeouts, retries, and MJPEG fallback.
from __future__ import annotations
import threading
import queue
import time
from typing import Optional, Tuple
from urllib.parse import urlparse, urlunparse

import cv2 as cv
import numpy as np
import requests

from settings import CameraSettings
from utils import monotonic_ms


class StreamCapture:
    def __init__(self, cam: CameraSettings):
        self.cam = cam
        self._stop = threading.Event()
        self._q: "queue.Queue[Tuple[bool, Optional[np.ndarray], int]]" = queue.Queue(maxsize=2)
        self._t: Optional[threading.Thread] = None
        self.last_backend = "init"

    def start(self):
        self._stop.clear()
        self._t = threading.Thread(target=self._run, daemon=True)
        self._t.start()

    def stop(self):
        self._stop.set()
        if self._t:
            self._t.join(timeout=1.0)

    def read(self) -> Tuple[bool, Optional[np.ndarray], int]:
        try:
            ok, frame, ts = self._q.get(timeout=0.25)
            return ok, frame, ts
        except queue.Empty:
            return False, None, 0

    # ---------- internals ----------
    def _run(self):
        url = self.cam.effective_url()
        parsed = urlparse(url)
        while not self._stop.is_set():
            try:
                if parsed.scheme in ("rtsp",):
                    ok = self._run_opencv(url)
                    if not ok:
                        self._fail_once("cv-no-rtsp")
                elif parsed.scheme in ("http", "https"):
                    # Try OpenCV first; if fails, fallback to MJPEG
                    ok = self._run_opencv(url)
                    if not ok:
                        ok = self._run_mjpeg(url)
                        if not ok:
                            self._fail_once("mjpeg-fail")
                else:
                    self._fail_once("bad-url")
                # short backoff before retry
                self._sleep_with_cancel(1.0)
            except Exception:
                self._fail_once("exception")
                self._sleep_with_cancel(1.0)

    def _run_opencv(self, url: str) -> bool:
        self.last_backend = "cv-ffmpeg"
        # Basic auth inline if provided
        u = url
        if self.cam.user and self.cam.password:
            p = urlparse(url)
            netloc = f"{self.cam.user}:{self.cam.password}@{p.hostname or ''}"
            if p.port:
                netloc += f":{p.port}"
            u = urlunparse((p.scheme, netloc, p.path, p.params, p.query, p.fragment))
        cap = cv.VideoCapture(u, cv.CAP_FFMPEG)
        ok, frame = cap.read()
        if not ok or frame is None:
            cap.release()
            return False
        self._offer(True, frame, monotonic_ms())
        while not self._stop.is_set():
            ok, frame = cap.read()
            if not ok or frame is None:
                break
            self._offer(True, frame, monotonic_ms())
        cap.release()
        return True

    def _run_mjpeg(self, url: str) -> bool:
        self.last_backend = "http-mjpeg"
        headers = {
            "Connection": "keep-alive",
            "Accept": "multipart/x-mixed-replace, image/jpeg, */*",
            "User-Agent": "ESP32-CAM-AI-Viewer/1.0",
        }
        auth_obj = None
        if self.cam.user and self.cam.password:
            auth_obj = requests.auth.HTTPBasicAuth(self.cam.user, self.cam.password)
        try:
            with requests.get(url, stream=True, auth=auth_obj, timeout=(5, 15), headers=headers) as r:
                r.raise_for_status()
                buf = bytearray()
                for chunk in r.iter_content(chunk_size=4096):
                    if self._stop.is_set():
                        return True
                    if not chunk:
                        continue
                    buf.extend(chunk)
                    # crude JPEG scan
                    while True:
                        start = buf.find(b"\xff\xd8")
                        end = buf.find(b"\xff\xd9")
                        if start != -1 and end != -1 and end > start:
                            jpg = bytes(buf[start:end+2])
                            del buf[:end+2]
                            frame = cv.imdecode(np.frombuffer(jpg, dtype=np.uint8), cv.IMREAD_COLOR)
                            if frame is not None:
                                self._offer(True, frame, monotonic_ms())
                            continue
                        break
            return True
        except requests.exceptions.ReadTimeout:
            self._fail_once("timeout")
            return False
        except Exception:
            self._fail_once("http-error")
            return False

    def _offer(self, ok: bool, frame: Optional[np.ndarray], ts_ms: int):
        if self._q.full():
            try:
                self._q.get_nowait()
            except queue.Empty:
                pass
        self._q.put((ok, frame, ts_ms))

    def _fail_once(self, tag: str):
        self.last_backend = f"disconnected:{tag}"
        self._offer(False, None, 0)

    def _sleep_with_cancel(self, sec: float):
        t0 = time.time()
        while not self._stop.is_set() and time.time() - t0 < sec:
            time.sleep(0.05)
## `tools.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\tools.py
**Size**: 3486 bytes
**Modified**: 2025-08-29 00:37:45 +0100
**SHA256**: dd5746ceb3a99a54a34531a4516e26cf30aeaed8216a18a2bd670856dfe67385
``````python"""Utility tools for the Qt MDI app.
 - Image ingestion from a source directory into face/pet stores
 - Simple near-duplicate culling via average-hash (aHash)
"""
from __future__ import annotations
import os
import time
from typing import Tuple
import numpy as np
import cv2


def ingest_images(src_dir: str, dest_dir: str, size: Tuple[int,int], gray: bool=False) -> int:
    os.makedirs(dest_dir, exist_ok=True)
    count=0
    for root,_,files in os.walk(src_dir):
        for fn in files:
            if not fn.lower().endswith(('.jpg','.jpeg','.png')):
                continue
            try:
                img=cv2.imread(os.path.join(root,fn))
                if img is None: continue
                if gray:
                    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
                img=cv2.resize(img, size)
                now=time.time(); ms=int((now-int(now))*1000); ds=time.strftime('%Y%m%d_%H%M%S', time.localtime(now))
                out=os.path.join(dest_dir, f'{ds}_{ms:03d}.jpg')
                cv2.imwrite(out, img)
                count+=1
            except Exception:
                pass
    return count


def cull_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4) -> int:
    """Remove near-duplicate images in a folder using aHash similarity.
    Returns number of removed files.
    """
    if not os.path.isdir(target):
        return 0
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; removed=0
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for fp in files:
        h=ahash(fp)
        if h is None: continue
        dup=False
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                try:
                    os.remove(fp); removed+=1
                except Exception:
                    pass
                dup=True
                break
        if not dup:
            hashes.append(h)
    return removed


def find_similar_in_dir(target: str, hash_size: int = 8, hamming_thresh: int = 4):
    """Analyze a directory for near-duplicates by aHash.
    Returns (files:list[str], remove_indices:set[int]) where remove_indices
    contains indices in files suggested for deletion.
    """
    if not os.path.isdir(target):
        return [], set()
    files=[os.path.join(target,f) for f in os.listdir(target) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    files.sort(key=lambda p: os.path.getmtime(p))
    hashes=[]; remove=set()
    def ahash(path):
        try:
            img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: return None
            img=cv2.resize(img,(hash_size,hash_size))
            avg=img.mean(); bits=(img>avg).astype(np.uint8)
            return bits
        except Exception:
            return None
    for idx, fp in enumerate(files):
        h=ahash(fp)
        if h is None: continue
        for hh in hashes:
            dist = int((h^hh).sum())
            if dist <= hamming_thresh:
                remove.add(idx)
                break
        else:
            hashes.append(h)
    return files, remove
## `utils.py`
**Absolute path**: C:\Users\stellaris\Documents\PlatformIO\Projects\ESP32_CAM_AI\AI\utils.py
**Size**: 1141 bytes
**Modified**: 2025-11-06 23:31:58 +0000
**SHA256**: aac37d4cf748a394770a76f613a0970ed3871c6af8125c7c7eb3defb5bc6f7cd
``````python# utils.py
# open_folder_or_warn uses absolute, existing paths; no change except BASE_DIR import removed.
from __future__ import annotations
import time
from pathlib import Path
import numpy as np, cv2 as cv
from PyQt6 import QtGui, QtWidgets, QtCore

def monotonic_ms() -> int:
    return int(time.monotonic() * 1000)

def timestamp_name() -> str:
    return time.strftime("%Y%m%d_%H%M%S")

def ensure_dir(p: Path):
    Path(p).mkdir(parents=True, exist_ok=True)

def qimage_from_bgr(bgr: np.ndarray) -> QtGui.QImage:
    h, w = bgr.shape[:2]
    rgb = cv.cvtColor(bgr, cv.COLOR_BGR2RGB)
    bpl = int(rgb.strides[0])
    return QtGui.QImage(rgb.data, w, h, bpl, QtGui.QImage.Format.Format_RGB888).copy()

def open_folder_or_warn(parent: QtWidgets.QWidget, path: Path):
    try:
        ensure_dir(path)
        ok = QtGui.QDesktopServices.openUrl(QtCore.QUrl.fromLocalFile(str(Path(path).resolve())))
        if not ok:
            raise RuntimeError("OS refused to open path")
    except Exception as e:
        QtWidgets.QMessageBox.warning(parent, "Open folder failed", f"Could not open:\n{path}\n\n{e}")
